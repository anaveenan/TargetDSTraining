{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning At Scale\n",
    "\n",
    "Data Analytics and Machine Learning at Scale \n",
    "\n",
    "---\n",
    "__Name:__  *Dr. James G. Shanahan*   \n",
    "__Email:__  *James.Shanahan  @ gmail.com   \n",
    "__Quiz:__  Debugging strategies in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please first choose which Spark cluster backs this notebook to get your SC/sqlContext\n",
    "\n",
    "* Back this notebook by Spark that is running on your local machine in a Container world\n",
    "* Back this notebook by Spark that is running an EMR Cluster (note one has to read and write data from/to S3 to run Spark jobs on EMR)\n",
    "* Back this notebook by Spark that is rnning on your local machine natively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you wish to launch a Spark cluster on your local machine in a Container world and back this notebook by that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f08e8082610>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f08c1ffc190>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "print sc\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you wish to back this notebook by an EMR cluster that is already up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "# First, we initialize the Spark environment\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you wish to launch a Spark cluster on your local machine in NATIVE model and back this notebook by that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x1050241d0>\n",
      "<pyspark.sql.context.SQLContext object at 0x100575210>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "hello hi hi hallo\n",
    "bonjour hola hi ciao\n",
    "nihao konnichiwa ola\n",
    "hola nihao hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hi hi hallo\r\n",
      "bonjour hola hi ciao\r\n",
      "nihao konnichiwa ola\r\n",
      "hola nihao hello"
     ]
    }
   ],
   "source": [
    "cat wordcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES on Inputs to Spark\n",
    "\n",
    "http://spark.apache.org/docs/latest/programming-guide.html\n",
    "All of Sparkâ€™s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile(\"/my/directory\"), textFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").\n",
    "\n",
    "The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 64MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize('wordcount.txt')  #distributes the string\n",
    "rdd.first()\n",
    "#rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('wordcount.txt')  #create an RDD\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'hello hi hi hallo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging in Spark  \n",
    "\n",
    "* ### PART 1: Write Mapper/reduce functions as standalone code and debug on a test record (key-value pair)\n",
    "* ### PART 2: n a multi operation call: break it down and debug step by step on a small test data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PART 1: debug each closure independently with small unit tests\n",
    "Where a closure can be (e.g., mapper/reducer/filter function first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is ia an example of  mapper function (referred to as closure in Spark as this function and \n",
    "# its state will be serialized and shipped to each worker)\n",
    "\n",
    "def mySplitFunction(string):\n",
    "    string.split()\n",
    "mySplitFunction(\"hello hi hi hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "# debug this function to return the first token in a string record\n",
    "# for some reason we get back the first character and not the first string\n",
    "def mySplitFunction(string):\n",
    "    toks = string.split()[0]\n",
    "    return toks[0]\n",
    "\n",
    "#fake out my mapper function and debug\n",
    "print mySplitFunction(\"hello hi hi hallo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "## debug this function to return the first token in a string record\n",
    "# for some reason we get back the first character and not the first string\n",
    "\n",
    "\n",
    "\n",
    "# solution \n",
    "def mySplitFunction(string):\n",
    "    toks = string.split()[0]\n",
    "    return toks\n",
    "\n",
    "#fake out my mapper function and debug\n",
    "print mySplitFunction(\"hello hi hi hallo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2:  In a multi operation call: break it down and debug step by step on a small test data set\n",
    "### Call one operation at a time and take a couple of results (e.g., take(1) and examine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'hello', u'hi', u'hi']\n"
     ]
    }
   ],
   "source": [
    "# output the tokens from each record (one to MANY transformation)\n",
    "\n",
    "def mySplitFunction(string):\n",
    "    string.split()\n",
    "    \n",
    "logFileNAME = 'wordcount.txt'\n",
    "text_file = sc.textFile(logFileNAME)\n",
    "\n",
    "#debug flatmap\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).take(3)\n",
    "print counts\n",
    "\n",
    "#              .map(lambda word: (word, 1)) \\\n",
    "#              .reduceByKey(lambda a, b: a + b)\n",
    "# wordCounts = counts.collect()\n",
    "# for v in counts.collect():\n",
    "#     print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'hello', 1), (u'hi', 1), (u'hi', 1)]\n"
     ]
    }
   ],
   "source": [
    "# output the tokens and corresponding count from each record (one to one map function)\n",
    "\n",
    "def mySplitFunction(string):\n",
    "    string.split()\n",
    "    \n",
    "logFileNAME = 'wordcount.txt'\n",
    "text_file = sc.textFile(logFileNAME)\n",
    "\n",
    "#debug flatmap\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .take(3)\n",
    "print counts\n",
    "\n",
    "#              .reduceByKey(lambda a, b: a + b)\n",
    "# wordCounts = counts.collect()\n",
    "# for v in counts.collect():\n",
    "#     print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'ciao', 1)\n",
      "(u'bonjour', 1)\n",
      "(u'nihao', 2)\n",
      "(u'hola', 2)\n",
      "(u'konnichiwa', 1)\n",
      "(u'hallo', 1)\n",
      "(u'hi', 3)\n",
      "(u'hello', 2)\n",
      "(u'ola', 1)\n"
     ]
    }
   ],
   "source": [
    "# complete word count\n",
    "#\n",
    "Count words in file/directory\n",
    "logFileNAME = 'wordcount.txt'\n",
    "text_file = sc.textFile(logFileNAME)\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "wordCounts = counts.collect()\n",
    "for v in counts.collect():\n",
    "    print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'ciao', 1), (u'bonjour', 1), (u'nihao', 2), (u'hola', 2), (u'konnichiwa', 1), (u'hallo', 1), (u'hi', 3), (u'hello', 2), (u'ola', 1)]\n"
     ]
    }
   ],
   "source": [
    "print wordCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__sortByKey([ascending], [numTasks])__\t\n",
    "\n",
    "When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hallo', 1),\n",
       " (u'konnichiwa', 1),\n",
       " (u'ola', 1),\n",
       " (u'ciao', 1),\n",
       " (u'bonjour', 1),\n",
       " (u'nihao', 2),\n",
       " (u'hello', 2),\n",
       " (u'hola', 2),\n",
       " (u'hi', 3)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hi', 3)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Last 1\n",
    "wordCounts[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hallo', 1), (u'konnichiwa', 1), (u'ola', 1), (u'ciao', 1), (u'bonjour', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first  5\n",
    "wordCounts[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
