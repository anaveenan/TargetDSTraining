{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning At Scale\n",
    "\n",
    "Data Analytics and Machine Learning at Scale \n",
    "\n",
    "---\n",
    "__Name:__  *Dr. James G. Shanahan*   \n",
    "__Email:__  *James.Shanahan  @ gmail.com   \n",
    "__Quiz:__  Secondary Sorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Sort in Spark (Total Sort)\n",
    "\n",
    "__ This notebook provides examples of Secondary Sorts in Spark__\n",
    "\n",
    "* Roll your own [See below]\n",
    "* Via the repartitionAndSortWithinPartitions transformation  See below and [pySpark Manual](http://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "* DataFrames [explore by yourself]\n",
    "---\n",
    "__ See how findthe maximum value of a RDD __\n",
    "* Find the maximum value of a RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please first choose which Spark cluster backs this notebook to get your SC/sqlContext\n",
    "\n",
    "* Back this notebook by Spark that is running on your local machine in a Container world\n",
    "* Back this notebook by Spark that is running an EMR Cluster (note one has to read and write data from/to S3 to run Spark jobs on EMR)\n",
    "* Back this notebook by Spark that is rnning on your local machine natively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you wish to launch a Spark cluster on your local machine in a Container world and back this notebook by that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f94b006a750>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f94a25eecd0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "print sc\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you wish to back this notebook by an EMR cluster that is already up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "# First, we initialize the Spark environment\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you wish to launch a Spark cluster on your local machine in NATIVE model and back this notebook by that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x1050241d0>\n",
      "<pyspark.sql.context.SQLContext object at 0x100575210>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting text.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile text.txt\n",
    "group1 bar 1\n",
    "group1 zoo 26\n",
    "group1 noo 2\n",
    "group1 foo 1\n",
    "group3 labs 1\n",
    "group2 quxx 1\n",
    "group2 axxx 1\n",
    "group2 st#ff 1\n",
    "group3 #funky 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def myOwnHash(x):\n",
    "    return hash(x[0])\n",
    "def readData(line):\n",
    "    x = line.split(\" \")\n",
    "    return x[:2],x[2:]\n",
    "text_file = sc.textFile('text.txt')\n",
    "rdd = text_file.map(readData)\n",
    "rdd_partitioned = rdd.partitionBy(4,myOwnHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_partitioned.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glom in Spark RDD\n",
    "glom() Return an RDD created by coalescing all elements within each partition into an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[([u'group1', u'bar'], [u'1']),\n",
       "  ([u'group1', u'zoo'], [u'26']),\n",
       "  ([u'group1', u'noo'], [u'2']),\n",
       "  ([u'group1', u'foo'], [u'1']),\n",
       "  ([u'group3', u'labs'], [u'1'])],\n",
       " [([u'group2', u'quxx'], [u'1']),\n",
       "  ([u'group2', u'axxx'], [u'1']),\n",
       "  ([u'group2', u'st#ff'], [u'1']),\n",
       "  ([u'group3', u'#funky'], [u'1'])]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[([u'group1', u'bar'], [u'1']),\n",
       "  ([u'group1', u'zoo'], [u'26']),\n",
       "  ([u'group1', u'noo'], [u'2']),\n",
       "  ([u'group1', u'foo'], [u'1'])],\n",
       " [],\n",
       " [([u'group3', u'labs'], [u'1']), ([u'group3', u'#funky'], [u'1'])],\n",
       " [([u'group2', u'quxx'], [u'1']),\n",
       "  ([u'group2', u'axxx'], [u'1']),\n",
       "  ([u'group2', u'st#ff'], [u'1'])]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE within each partition (aka group) the records are sorted is sorted in \n",
    "#increasing order of the second part of the key\n",
    "rdd_partitioned.glom().collect()\n",
    "\n",
    "# group 1 is in parition 1\n",
    "# group 2 is in parition 4\n",
    "# Group 3 is in partition 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([u'group1', u'bar'], [u'1']),\n",
       " ([u'group1', u'zoo'], [u'26']),\n",
       " ([u'group1', u'noo'], [u'2']),\n",
       " ([u'group1', u'foo'], [u'1']),\n",
       " ([u'group3', u'labs'], [u'1']),\n",
       " ([u'group3', u'#funky'], [u'1']),\n",
       " ([u'group2', u'quxx'], [u'1']),\n",
       " ([u'group2', u'axxx'], [u'1']),\n",
       " ([u'group2', u'st#ff'], [u'1'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE within each partition (aka group) the records are sorted  in \n",
    "#increasing order of the second part of the key\n",
    "rdd_partitioned.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Secondary Sort via the repartitionAndSortWithinPartitions transformation\n",
    "Another important capability to be aware of is the repartitionAndSortWithinPartitions transformation. It’s a transformation that sounds arcane, but seems to come up in all sorts of strange situations. This transformation pushes sorting down into the shuffle machinery, where large amounts of data can be spilled efficiently and sorting can be combined with other operations.\n",
    "For example, Apache Hive on Spark uses this transformation inside its join implementation. It also acts as a vital building block in the secondary sort pattern, in which you want to both group records by key and then, when iterating over the values that correspond to a key, have them show up in a particular order. This issue comes up in algorithms that need to group events by user and then analyze the events for each user based on the order they occurred in time. Taking advantage of repartitionAndSortWithinPartitions to do secondary sort currently requires a bit of legwork on the part of the user, but SPARK-3655 will simplify things vastly.\n",
    "In [ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 10), (0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartitionAndSortWithinPartitions(numPartitions=None, partitionFunc=<function portable_hash at 0x7f2bec385230>, ascending=True, keyfunc=<function <lambda> at 0x7f2bec3839b0>)\n",
    "# Repartition the RDD according to the given partitioner and, within each resulting partition, \n",
    "# sort records by their keys.\n",
    "\n",
    "rdd = sc.parallelize([(0, 10), (0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    "rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)\n",
    "rdd2.glom().collect()  #print the output\n",
    "#[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5), (0, 8), (2, 6), (1, 3), (3, 8), (3, 8)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Another example of secondary sort  via repartitionAndSortWithinPartitions\n",
    "### k1,k2,v1,v2 : partition by k1 and sort by k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ss.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ss.txt\n",
    "1,3,a,b\n",
    "2,5,a,c\n",
    "1,4,a,f\n",
    "3,4,d,c\n",
    "2,1,f,a\n",
    "1,1,e,r\n",
    "2,4,o,1\n",
    "3,2,d,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 3], [u'a', u'b']), ([2, 5], [u'a', u'c']), ([1, 4], [u'a', u'f']), ([3, 4], [u'd', u'c']), ([2, 1], [u'f', u'a']), ([1, 1], [u'e', u'r']), ([2, 4], [u'o', u'1']), ([3, 2], [u'd', u'c'])]\n"
     ]
    }
   ],
   "source": [
    "def read_data(line):\n",
    "    d = line.split(',')\n",
    "    return [int(d[0]),int(d[1])],[d[2],d[3]]\n",
    "dataRDD = sc.textFile(\"ss.txt\").map(read_data).cache()\n",
    "print dataRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 3], [u'a', u'b']), ([2, 5], [u'a', u'c']), ([1, 4], [u'a', u'f']), ([3, 4], [u'd', u'c']), ([2, 1], [u'f', u'a']), ([1, 1], [u'e', u'r']), ([2, 4], [u'o', u'1']), ([3, 2], [u'd', u'c'])]\n"
     ]
    }
   ],
   "source": [
    "ssdata = dataRDD.repartitionAndSortWithinPartitions(numPartitions=3,\n",
    "                                                    partitionFunc= lambda x: x[0]%3,keyfunc=lambda x: x[1])\n",
    "print dataRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check data by partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[([3, 2], [u'd', u'c']), ([3, 4], [u'd', u'c'])],\n",
       " [([1, 1], [u'e', u'r']), ([1, 3], [u'a', u'b']), ([1, 4], [u'a', u'f'])],\n",
       " [([2, 1], [u'f', u'a']), ([2, 4], [u'o', u'1']), ([2, 5], [u'a', u'c'])]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssdata.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the maximum value of a RDD.\n",
    "glom() Return an RDD created by coalescing all elements within each partition into an array.\n",
    "\n",
    "For example, to get the maximum value of a RDD.\n",
    "\n",
    "val maxValue = dataRDD.reduce(_ max _)\n",
    "\n",
    "There will be lot of shuffles between partitions for comparison.\n",
    "Rather than comparing all the values,\n",
    "1. Find the maximum in each partition\n",
    "2. Compare maximum value between partitions to get the final max value.\n",
    "\n",
    "val maxValue = dataRDD.glom().map((row: Array[Double]) => value.max).reduce(_ max _)\n",
    "\n",
    "\n",
    "Reference:\n",
    "http://blog.madhukaraphatak.com/glom-in-spark/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
