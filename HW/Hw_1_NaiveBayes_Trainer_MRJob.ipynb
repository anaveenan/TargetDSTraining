{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a single Jupyter note, please include questions, and question numbers in the questions and in the responses.\n",
    "Submit your homework notebook via the following form:\n",
    "\n",
    "   + http://goo.gl/forms/er3OFr5eCMWDngB72\n",
    "   \n",
    "=== REFERENCES\n",
    "As reference chapter please read chapter 13 from the IRBook  (Introduction to Information Retrieval\n",
    "By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze, © 2008 Cambridge University Press\n",
    "   + Website: http://informationretrieval.org/\n",
    "\n",
    "Multinomial Naive Bayes with Laplace smoothing\n",
    "   + http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf\n",
    "\n",
    "MRjob Reference (any question you may have on MRJob from : \n",
    "   + http://mrjob.readthedocs.io/en/latest/\n",
    "\n",
    "Writing An Hadoop MapReduce Program In Python (WordCount)\n",
    "   + http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n",
    "\n",
    "=== SERVER with MRJob installed\n",
    "\n",
    "http://ec2-52-201-222-181.compute-1.amazonaws.com:8000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === HW 1 ASSIGNMENTS using Mrjob or Hadoop Streaming and Python (classification using Multinomial Naive Bayes)==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0: \n",
    "\n",
    "Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n",
    "What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer HW1.0.0\n",
    "\n",
    "#### Big data is a used to describe data that is so large or so complex that traditional applications are inadequate.  A more general definition for big data is high volume, high velocity, and/or, high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery, and process optimization.  An example is the collection of guest sales, demographic attributes, and social media content that can be used to predict guest behavior or customer life time value. \n",
    "\n",
    "#### A race condition is when the final value in parallel computing is compromised when sychronization fails across the nodes.  An example would be when two nodes read the value of X and one overwrites the other, versus when one node updates the value on to disk, and then the second node updates the value to disk.\n",
    "#### MapReduce is structured parallel processing, while Hadoop is a distributed file system.  In other words, Hadoop focuses on the storing of data, while it waits for instructions to access the data.  MapReduce informs Hadoop how a specific task will be accomplished.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1: \n",
    "Here is an example of functional programming in basic python in terms of mappers and reducers (by way of example):\n",
    "\n",
    "\n",
    "#EXAMPLE Mapper functions in Python\n",
    "def fahrenheit(T):\n",
    "    return ((float(9)/5)*T + 32)\n",
    "\n",
    "def celsius(T):\n",
    "    return (float(5)/9)*(T-32)\n",
    " \n",
    "temperatures = (36.5, 37, 37.5, 38, 39)\n",
    "F = map(fahrenheit, temperatures)\n",
    "#returns  97.7  98.6  99.5 100.4 102.2\n",
    "C = map(celsius, F)\n",
    "\n",
    "#EXAMPLE Reducer function in Python\n",
    "import functools\n",
    "functools.reduce(lambda x,y: x+y, [47,11,42,13])\n",
    "#returns 113\n",
    "\n",
    "import functools as reduce\n",
    "print \"Average temp is %fF\" % (reduce(lambda x,y: x+y, F)/len(F) )\n",
    "#returns Average temp is 99.68F\n",
    "\n",
    "\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example of functional programming in raw python code and show the code running. E.g., in raw python find the average length of a string in and of strings using a python \"map-reduce\" (functional programming) job (similar in style to the above). Alternatively, you can do this in python Hadoop Streaming.   \n",
    "\n",
    "strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]\n",
    ".......\n",
    "\n",
    " \n",
    "import functools as reduce\n",
    "temperatures = (36.5, 37, 37.5, 38, 39)\n",
    "F = map(fahrenheit, temperatures)\n",
    "print \"Average temp is %fF\" % (reduce(lambda x,y: x+y, F)/len(F) )\n",
    "#returns Average temp is 99.68F\n",
    "\n",
    "map(sqr, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\z001c9v\\\\Documents\\\\Target DS Training 2016\\\\NaiveBayes'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Answer to HW1.0.1 - simple Python ###\n",
    "\n",
    "strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]\n",
    "str_len = []\n",
    "def mapper(strings):\n",
    "    for word in strings:\n",
    "        print(word)\n",
    "        length = len(word)\n",
    "        print(length)\n",
    "        str_len.append(length)\n",
    "    return str_len \n",
    "print str_len\n",
    "\n",
    "def reducer(str_len):\n",
    "    denominator = 0\n",
    "    numerator = 0\n",
    "    for i in str_len:\n",
    "        denominator += 1\n",
    "        numerator += i\n",
    "    print ('denominator:' , denominator) \n",
    "    print ('numerator:' , numerator)\n",
    "    average = numerator/denominator\n",
    "    return average\n",
    "\n",
    " \n",
    "strings_length= mapper(strings)\n",
    "print(strings_length)\n",
    "average = reducer(strings_length)\n",
    "print('string average: ', str(average) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to HW1.0.1 -  MRJob Hadoop Streaming ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"] > wordstring.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile nb_hw1_0_1.py \n",
    "from mrjob.job import MRJob\n",
    "\n",
    "#strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]\n",
    "class avgtxt(MRJob):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super(avgtxt,self).__init__(*args,**kwargs)\n",
    "        self.l = 0\n",
    "        self.denominator = 0\n",
    "        self.numerator = 0\n",
    "        self.counter = 0\n",
    "    #print('foo')\n",
    "    def mapper(self, _ , line) :\n",
    "        print(\"line:\" , line)\n",
    "        for word in line.split(','):\n",
    "            print('word:' , word)\n",
    "            self.l = len(word)\n",
    "            print('length:' , self.l)\n",
    "            print(type(self.l))\n",
    "            self.denominator += 1\n",
    "            print(self.denominator)\n",
    "            self.numerator += self.l\n",
    "            print(self.numerator)\n",
    "        yield ('divide' , [self.numerator , self.denominator])\n",
    "\n",
    "#'''\n",
    "    def reducer(self, key , values):\n",
    "        print('key_values', key , str(values))\n",
    "        print(list(values))\n",
    "        print(type(key))\n",
    "        print(type(values))\n",
    "        print(values[0][0])\n",
    "        '''\n",
    "        numerator = float(values[0])\n",
    "        print(numberator)\n",
    "        denominator = float(values[1])\n",
    "        print(denominator)\n",
    "        avgtxt_len = numerator/denominator\n",
    "        print(avgtxt_len)\n",
    "        '''    \n",
    "#        for word in key:\n",
    "\n",
    "            \n",
    "        #sum_num = sum(float(self.l)\n",
    "        #print ('numerator' , str(sum_num))\n",
    "        #sum_den = sum(float(values))\n",
    "        #print ('denominator' , str(sum_den))\n",
    "        #avgtxt = sum_num / sum_den\n",
    "        #yield ('Avg Text Length' , str(avgtxt))\n",
    "#'''        \n",
    "if __name__ == '__main__':\n",
    "    avgtxt.run()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python nb_hw1_0_1.py wordstring.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1 Cross fold validation (*)\n",
    "\n",
    "What is cross validation (in partiticular 10-fold cross validation)?\n",
    "\n",
    "\n",
    "* for more background see slides from lecture 2:\n",
    "   + https://www.dropbox.com/s/3ch0yb4oxyxsuo9/Lecture-02-ML-Intro-Bias-Variance.pdf?dl=0\n",
    "\n",
    "and \n",
    "\n",
    "   + https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer HW1.1 Cross fold validation\n",
    "\n",
    "#### Cross validation is an iterative process where the data are partition into k equal sized subsamples.  A single k is used as validation, while the k-1 sample of data are used for the training of the model.  The results of each k fold validation can be averaged together to form a single point estimate.  In a 10 fold validation, there would be 10 sub samples over 10 iterations, with each sub sample acting as the validation data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multinomial naive Bayes classifier\n",
    "===== SPAM Dataset \n",
    "In the remainder of this assignment you will produce a spam filter\n",
    "that is backed by a multinomial naive Bayes classifier  (see http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html).\n",
    "\n",
    "For the sake of this assignment we will focus on the basic construction \n",
    "of the parallelized classifier, and not consider its validation or calibration,\n",
    "and so you will have the classifier operate on its own training data (unlike a \n",
    "field application where one would use non-overlapping subsets for training, validation and testing).\n",
    "\n",
    "The data you will use is a curated subset of the Enron email corpus\n",
    "(whose details you may find in the file enronemail_README.txt  in the directory surrounding these instructions).\n",
    "\n",
    " --NOTE: please use the subject field and the body field for all your Naive Bayes modeling. \n",
    "\n",
    "NOTE: This SPAM/HAM dataset for HW1 contains 100 records from the Enron SPAM/HAM corpus. Please limit your study to this unless otherwise instructed. There are about 93,000 emails in the original SPAM/HAM corpus. There are several versions of the SPAM/HAM corpus. Other Enron-Spam datasets are available from http://www.aueb.gr/users/ion/data/enron-spam/index.html and http://www.aueb.gr/users/ion/publications.html in both raw and pre-processed form. \n",
    "\n",
    "Doing some exploratory data analysis you will see (with this very small dataset) the following:\n",
    "> wc -l enronemail_1h.txt  #100 email records\n",
    "     100 enronemail_1h.txt\n",
    "> cut -f2 -d$'\\t' enronemail_1h.txt|wc  #extract second field which is SPAM flag\n",
    "     101     394    3999\n",
    "JAMES-SHANAHANs-Desktop-Pro-2:HW1-Questions jshanahan$ cut -f2 -d$'\\t' enronemail_1h.txt|head\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "1\n",
    "\n",
    "\n",
    "\n",
    "> head -n 100 enronemail_1h.txt|tail -1|less #an example SPAM email record\n",
    "018.2001-07-13.SA_and_HP       1        [ilug] we need your assistance to invest in your country        dear sir/madam,  i am well confident of your capability to assist me in  a transaction for mutual benefit of both parties, ie  (me and you) i am also believing that you will not  expose or betray the trust and confidence i am about  to establish with you. i have decided to contact you  with greatest delight and personal respect.  well, i am victor sankoh, son to mr. foday  sankoh  who was arrested by the ecomog peace keeping force  months ago in my country sierra leone. …."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2:  WORDCOUNT\n",
    "Using the Enron dataset and Hadoop MapReduce streaming (or MRJob), write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "       8    \n",
    "       #NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer HW1.2 Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### View Data ###\n",
    "!wc -l enronemail_1h.txt  #100 email records\n",
    "!head -n 100 enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## HW1.2 WordCount Python Development\n",
    "import re , string \n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "f = open('enronemail_1h.txt', 'r')\n",
    "lines = f.readline\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "Corpus = {}\n",
    "counter = 0\n",
    "while counter < 100:\n",
    "    for i in f.readlines():\n",
    "        #print('original:')\n",
    "        #print(i)\n",
    "        #print(type(i))\n",
    "        #cleantxt = re.sub('\\W+',' ',i)\n",
    "        #print('clean text:')\n",
    "        #print(cleantxt)\n",
    "        words = i.split('\\t')\n",
    "        #words.sort()\n",
    "        #print('words: ')\n",
    "        #print(words)\n",
    "        for j in words[2:]:\n",
    "            cleantxt = re.sub('\\W+',' ',j)\n",
    "            splitclean = cleantxt.split()\n",
    "            #filtered_words = [word for word in splitclean if word not in stopwords.words('english')]\n",
    "            for k in splitclean:\n",
    "                #print( k )\n",
    "                k = regex.sub(' ', k.lower())\n",
    "                cleantxt2 = re.sub('\\W+',' ',k)\n",
    "                #print(cleantxt2)\n",
    "                if cleantxt2 in Corpus:\n",
    "                    Corpus[cleantxt2] += 1\n",
    "                else :\n",
    "                    Corpus[cleantxt2] = 1\n",
    "\n",
    "        #print(cleantxt)\n",
    "        #print cleantxt\n",
    "        #wordsclean = cleantxt.split()\n",
    "        #print('clean words:')\n",
    "        #print(wordsclean)\n",
    "        #print('\\n')\n",
    "        counter += 1\n",
    "\n",
    "print('Corpus:')        \n",
    "print(Corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.2 WordCount MRJOB Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile nb_hw1_2.py\n",
    "from mrjob.job import MRJob\n",
    "import re , string\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "Corpus = dict()\n",
    "class WCMrJob(MRJob):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WCMrJob, self).__init__(*args, **kwargs)\n",
    "        self.Corpus = dict()\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(WCMrJob, self).configure_options()\n",
    "        self.add_passthrough_option(\"--match_word\", type='str', default=\"\")\n",
    "\n",
    "    def mapper(self, _ , line):\n",
    "        \n",
    "        words = line.split('\\t')\n",
    "        #print('words:')\n",
    "        #print(words)\n",
    "        for j in words[2:]:\n",
    "            j = regex.sub(' ', j.lower())\n",
    "            cleantxt = re.sub('\\W+',' ',j)\n",
    "            splitclean = cleantxt.split()\n",
    "            #print('cleantxt:')\n",
    "            #print(cleantxt)\n",
    "            #print('splitclean:')\n",
    "            #print(splitclean)\n",
    "            for i in splitclean:\n",
    "                yield(i , 1)\n",
    "                #print(i, 1)\n",
    "            \n",
    "    #def reducer(self, key , values):\n",
    "        \n",
    "        #self.Corpus[key] = sum(values)\n",
    "        #print self.Corpus\n",
    "        #yield (key , sum(values))\n",
    "        #print self.Corpus[self.options.match_word]\n",
    "        #Corpus[key] = sum(values)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        #yield (key , sum(values))\n",
    "        if key == self.options.match_word:\n",
    "            yield (key , sum(values))    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WCMrJob.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python nb_hw1_2.py enronemail_1h.txt --match_word assistance\n",
    "# !python nb_hw1_2.py head 3 enronemail_1h.txt --match_word assistance\n",
    "#!cat enronemail_1h.txt | python nb_hw1_2.py --match_word assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2.1  \n",
    "Using Hadoop MapReduce (or MRJob) and your wordcount job (from HW1.2) determine the top-10 occurring tokens (most frequent tokens) using a single reducer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer HW1.2.1 top ten wordcount words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile nb_hw1_2_1.py\n",
    "from mrjob.job import MRJob\n",
    "import re , string\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "Corpus = dict()\n",
    "class WCMrJob(MRJob):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WCMrJob, self).__init__(*args, **kwargs)\n",
    "        self.Corpus = dict()\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(WCMrJob, self).configure_options()\n",
    "        self.add_passthrough_option(\"--match_word\", type='str', default=\"\")\n",
    "\n",
    "    def mapper(self, _ , line):\n",
    "        \n",
    "        words = line.split('\\t')\n",
    "        #print('words:')\n",
    "        #print(words)\n",
    "        for j in words[2:]:\n",
    "            j = regex.sub(' ', j.lower())            \n",
    "            cleantxt = re.sub('\\W+',' ',j)\n",
    "            splitclean = cleantxt.split()\n",
    "            #print('cleantxt:')\n",
    "            #print(cleantxt)\n",
    "            #print('splitclean:')\n",
    "            #print(splitclean)\n",
    "            for i in splitclean:\n",
    "                yield(i , 1)\n",
    "                #print(i, 1)\n",
    "            \n",
    "    def combiner(self, key , value):\n",
    "        yield ( key , sum(value) )\n",
    "\n",
    "    def reducer(self, key, value):\n",
    "        self.Corpus[key] = value\n",
    "        #for v , k in value:\n",
    "        yield ( key , sum(value) )\n",
    "        #print( self.Corpus[key])\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    WCMrJob.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the file from output\n",
    "from nb_hw1_2_1 import WCMrJob\n",
    "\n",
    "inputData = 'enronemail_1h.txt'\n",
    "\n",
    "mr_job = WCMrJob(args=[inputData])\n",
    "results={}\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    \n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "        results[key] = value\n",
    "    \n",
    "    with open('nb_hw1_2_1counts.txt' , 'w') as f:\n",
    "        for k in results.keys():\n",
    "            f.writelines( k + \"\\t\" + str(results[k]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the sort from the output file.\n",
    "from collections import Counter\n",
    " \n",
    "prevresultscounts = {}\n",
    "resultdictcounts = [s.split('\\n')[0].split('\\t') for s in open(\"nb_hw1_2_1counts.txt\").readlines()]\n",
    "for word, statsStr in resultdictcounts:\n",
    "    prevresultscounts[word] =  map(int, statsStr.split(\",\"))\n",
    " \n",
    "    \n",
    "Counter(prevresultscounts).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3: Multinomial NAIVE BAYES with NO Smoothing using a single reducer\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce (or MRJob), write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). **Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:**\n",
    "\n",
    "   **the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM** \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM **(when concatenated)** is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. **Count up how many times you need to process a zero probabilty for each class and report.** \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. **Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set.** Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "NOTE: please assume one reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer HW1.3: Multinomial NAIVE BAYES with NO Smoothing using a single reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"enronemail_1h.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "# Randomly sort data to avoid any bias from the order in original data\n",
    "random.shuffle(data)\n",
    "\n",
    "num_lines = len(data)\n",
    "num_lines_70pct = int(.7*num_lines)\n",
    "num_lines_85pct = int(.85*num_lines)\n",
    "\n",
    "print(num_lines)\n",
    "print(num_lines_70pct)\n",
    "print(num_lines_85pct)\n",
    "\n",
    "train_data = data[:num_lines_70pct]\n",
    "validation_data = data[num_lines_70pct:num_lines_85pct]\n",
    "testing_data = data[num_lines_85pct:]\n",
    "\n",
    "with open(\"enron_training_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(train_data))\n",
    "with open(\"enron_validation_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(validation_data))\n",
    "with open(\"enron_testing_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Trainier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile MRNaiveBayesTrainer.py\n",
    "\n",
    "\"\"\"An implementation of a multinomial Naive Bayes learner as an MRJob.\n",
    "   This is meant as an example of why mapper_final is useful.\n",
    "   \n",
    "   This learning algorithm implementation can be further optimised. HOW?\n",
    "   \n",
    "   Use a cool pattern to do this!\n",
    "\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRNaiveBayesTrainer(MRJob):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRNaiveBayesTrainer, self).__init__(*args, **kwargs)\n",
    "        self.modelStats = {}\n",
    "\n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(MRNaiveBayesTrainer, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Don't actually yield anything for each line. Instead, collect them\n",
    "        # and yield the sums when all lines have been processed. The results\n",
    "        # will be collected by the reducer.\n",
    "        docID, docClass,text = line.split(\"\\t\",2)\n",
    "        text = re.sub('\\W+',' ',text)\n",
    "        words = text.split()\n",
    "        \n",
    "        if docID != \"D5\":  #skip doc d5 in chinese dataset\n",
    "            if docClass == \"1\":\n",
    "                yield(\"TomsPriors\", \"0,1\")\n",
    "                for word in words:\n",
    "                    yield(word, \"0,1\")\n",
    "            else:\n",
    "                yield(\"TomsPriors\", \"1,0\")\n",
    "                for word in words:\n",
    "                    yield(word, \"1,0\")\n",
    "        \n",
    "\n",
    "    def reducer(self, word, values):\n",
    "        #aggregate counts for Pr(Word|Class)\n",
    "        #yield(\"number of values for \"+word, str(values))\n",
    "        w0Total=0\n",
    "        w1Total=0\n",
    "        for value in values:\n",
    "            w0, w1 =  value.split(\",\")\n",
    "            w0Total += float(w0)\n",
    "            w1Total += float(w1)  \n",
    "        self.modelStats[word] =  [w0Total, w1Total]\n",
    "\n",
    "        #yield(\"JIMI \"+word, [w0Total, w1Total])\n",
    "    def reducer_final(self):       \n",
    "        class0Total = 0\n",
    "        class1Total = 0\n",
    "        for k in self.modelStats.keys():\n",
    "            if k != \"TomsPriors\":\n",
    "                class0Total += self.modelStats[k][0]\n",
    "                class1Total += self.modelStats[k][1]\n",
    "        vocabularySize = len(self.modelStats.keys()) -1  #ignore TomsPriors\n",
    "        #some yields to see some model internal parameters\n",
    "        #yield (\"defaultPrior 0 class\", class0Total+vocabularySize)\n",
    "        #yield (\"defaultPrior 1 class\", class1Total+vocabularySize)\n",
    "        #yield (\"count 0 class\", class0Total)\n",
    "        #yield (\"count 1 class\", class1Total)\n",
    "        #yield (\"vocabularySize\", vocabularySize)\n",
    "        \n",
    "        #calculate priors \n",
    "        classCount0, classCount1 = self.modelStats.get(\"TomsPriors\")\n",
    "        del self.modelStats[\"TomsPriors\"]\n",
    "        total = classCount0 + classCount1\n",
    "        yield(\"TomsPriors\", ','.join(str(j) for j in [classCount0, classCount1, classCount0/total, classCount1/total])) \n",
    "        for k in self.modelStats.keys():\n",
    "            yield(k, ','.join(str(j) for j in [self.modelStats[k][0],\n",
    "                      self.modelStats[k][1],\n",
    "                      (self.modelStats[k][0] + 1) /(class0Total + vocabularySize), #remove 1,vocabulariSize to remove smoothing.\n",
    "                      (self.modelStats[k][1] +1)/(class1Total+vocabularySize)]))        \n",
    " \n",
    "# The if __name__ == \"__main__\": \n",
    "# ... trick exists in Python so that our Python files \n",
    "# can act as either reusable modules, or as standalone programs.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRNaiveBayesTrainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is Command line:\n",
    "!python MRNaiveBayesTrainer.py head 3 enronemail_1h.txt\n",
    "#!python MRNaiveBayesTrainer.py --jobconf mapred.reduce.tasks=1 chineseExample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is MRJOB:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random\n",
    "from MRNaiveBayesTrainer import MRNaiveBayesTrainer \n",
    "\n",
    "# STEP 1: Train a mulitnomial Naive Bayes      \n",
    "#trainingData = 'NaiveBayes/chineseExample.txt'\n",
    "trainingData = 'enronemail_1h.txt'\n",
    "\n",
    "# create an instance of the Trainer class\n",
    "# and initiatialize it\n",
    "mr_job = MRNaiveBayesTrainer(args=[trainingData])\n",
    "modelStats={}\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access to the output reducer/reducer_final of \n",
    "    # the last step in MRNaiveBayesTrainer\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "        modelStats[key] = value            \n",
    "    # Store model locally\n",
    "    with open('model1.txt', 'w') as f:\n",
    "        for k in modelStats.keys():\n",
    "            f.writelines( k + \"\\t\"+ str(modelStats[k]) +\"\\n\")\n",
    "print modelStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Classifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile nb_hw1_3_MRNaiveBayesClassifier.py\n",
    "\n",
    "\"\"\"An implementation of a multinomial Naive Bayes CLASSIFIER as an MRJob.\n",
    "\"\"\"\n",
    "\n",
    "# STILL under constuction \n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import os , re \n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "class MRNaiveBayesClassifier(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init, \n",
    "                       mapper=self.mapper,\n",
    "                       combiner = self.combiner,\n",
    "                       reducer=self.reducer,\n",
    "                       reducer_final=self.reducer_final   )\n",
    "               ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRNaiveBayesClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "    #load model from file; it has been sent from the master node to each worker node\n",
    "    def mapper_init(self):\n",
    "        #print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        self.modelStats = {}\n",
    "        recordStrs = [s.split('\\n')[0].split('\\t') for s in open(\"C:\\Users\\z001c9v\\Documents\\Target DS Training 2016\\NaiveBayes\\model1.txt\").readlines()] # \"model\"\n",
    "        for word, statsStr in recordStrs:\n",
    "            self.modelStats[word] = map(float, statsStr.split(\",\"))\n",
    "        #print model to stdout\n",
    "        for word in self.modelStats.keys():\n",
    "            print \"Model\", word, \"-->\", self.modelStats[word]\n",
    "\n",
    "        print ('\\n')\n",
    "    def mapper(self, _, line):\n",
    "        ClassSpam = {}\n",
    "        #print(line)\n",
    "        # Don't actually yield anything for each line. Instead, collect them\n",
    "        # and yield the sums when all lines have been processed. The results\n",
    "        # will be collected by the reducer.\n",
    "        docID, docClass,text = line.split(\"\\t\",2) \n",
    "        text = re.sub('\\W+',' ',text)\n",
    "        words = text.split()\n",
    "        print(docID, [docClass,text])\n",
    "        c0, c1, prClass0, prClass1 = map(float, self.modelStats[\"TomsPriors\"])\n",
    "        print \"c0 = %04.3f , c1 = %04.3f , prClass0=%04.3f, prClass1=%04.3f\" % (c0 , c1 , prClass0, prClass1)\n",
    "        # Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "        # Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "        PrClass0GivenDoc = prClass0  \n",
    "        PrClass1GivenDoc = prClass1\n",
    "        '''\n",
    "        # Levels version : \n",
    "        for word in words:\n",
    "            #print word, modelStats[word]  #Pr(word|class = 0) all stats\n",
    "            print 'Pr(%s|class = 0) = %04.3f' %(word, self.modelStats[word][2])  #Pr(word|class = 0)\n",
    "            print 'Pr(%s|class = 1) = %04.3f' %(word, self.modelStats[word][3])  #Pr(word|class = 1)\n",
    "            PrClass0GivenDoc *= self.modelStats[word][2]\n",
    "            PrClass1GivenDoc *= self.modelStats[word][3]            \n",
    "            if (PrClass1GivenDoc > PrClass0GivenDoc) :\n",
    "                ClassSpam[docID] = 1\n",
    "            else :\n",
    "                ClassSpam[docID] = 0\n",
    "            print PrClass1GivenDoc , PrClass0GivenDoc \n",
    "        print 'Docid: %s , ClassSpam: %s' %(docID, ClassSpam[docID])\n",
    "        \n",
    "        '''\n",
    "        #Log version :\n",
    "        for word in words:\n",
    "            #print word, modelStats[word]  #Pr(word|class = 0) all stats\n",
    "            print 'Pr(%s|class = 0) = %04.3f' %(word, self.modelStats[word][2])  #Pr(word|class = 0)\n",
    "            print 'Pr(%s|class = 1) = %04.3f' %(word, self.modelStats[word][3])  #Pr(word|class = 1)\n",
    "            PrClass0GivenDoc += log(self.modelStats[word][2])\n",
    "            PrClass1GivenDoc += log(self.modelStats[word][3])\n",
    "  \n",
    "            if (PrClass1GivenDoc - PrClass0GivenDoc) > 0 :\n",
    "                ClassSpam[docID] = 1\n",
    "            else :\n",
    "                ClassSpam[docID] = 0\n",
    "        diff = PrClass1GivenDoc - PrClass0GivenDoc\n",
    "        print \"Pr(Class=0| Doc) = %04.5f, log(Pr(Class=0| Doc=D5)) = %f\" % (exp(PrClass0GivenDoc), PrClass0GivenDoc)\n",
    "        print \"Pr(Class=1| Doc) = %04.5f, log(Pr(Class=1| Doc=D5)) = %f\" % (exp(PrClass1GivenDoc), PrClass1GivenDoc) \n",
    "        print 'Difference logs : %f' %(diff)\n",
    "        print 'Docid: %s , docClass: %s , ClassSpam: %s' %(docID, docClass , ClassSpam[docID])\n",
    "        if docClass == str(ClassSpam[docID]) : \n",
    "            #yield ('classificationCorrect' , 1)\n",
    "            print 'classificationCorrect' , 1\n",
    "        else :\n",
    "            #yield ('classificationInCorrect' , 1)\n",
    "            print 'classificationInCorrect' , 1\n",
    "        #yield (classificationCorrect, 1) for correct, yield (classificationCorrect, 1) for incorrect\n",
    "        #yield(None, \"fake output\")\n",
    "        #Class priors (Pr(Class =0) and Pr(Class =1))\n",
    "        \n",
    "    #def combiner(self, word, values):\n",
    "        #yield (word, values)\n",
    "        #yield (word , sum(values))\n",
    "        #print word , sum(values)\n",
    "'''\n",
    "    def reducer(self, word, values):\n",
    "        #denominator = len(word)\n",
    "        #correct = sum(values)\n",
    "        #yield (\"ErrorRate\", \"ErrorRate\")\n",
    "        #yield (\"Accuracy\", \"Accuracy\")\n",
    "\n",
    "\n",
    "    # QUESTION is this really necessary?\n",
    "    def reducer_final(self):\n",
    "        yield (\"ErrorRate\", \"ErrorRate\")\n",
    "        yield (\"Accuracy\", \"Accuracy\")\n",
    "'''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #MRNaiveBayesTrainer.run()\n",
    "    MRNaiveBayesClassifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.4:  Multinomial Naive Bayes with Smoothing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4.0: \n",
    "Repeat HW1.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for HW1.3 versus HW1.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1.4.1 Jelinek-Mercer (JM) smoothing* \n",
    "\n",
    "With different smoothing methods, p(wk|ci) (i.e., the word class conditionals) will be computed\n",
    "differently. We consider Jelinek-Mercer (JM) smoothing as an alternative to Laplace  Let c(w, ci) denote\n",
    "the frequency of word w in category ci,  p(w|C) be the maximum likelihood estimation of word w in \n",
    "collection C (relative frequency) and let |C for classi| denote the length of the classi. Then:\n",
    "\n",
    "1) Jelinek-Mercer (JM) smoothing:\n",
    "pλ (w|ci) = (1 − λ) * c(w, ci)/|C for classi|   +  λ p(w|C)\n",
    "\n",
    "where λ = 0.3  \n",
    "λ a hyperparameter for the Jelinek-Mercer (JM) smoothing that gives more (or less) \n",
    "credit to the background model p(w|C) or to the foreground model \n",
    "\n",
    "Repleat HW 1.4.0 with Jelinek-Mercer (JM) smoothing. Present a nice table comparing results where each \n",
    "row is the approach taken (Multinomial Naive Bayes with Laplace+1, or Multinomial Naive Bayes with JM) and the column is error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4.2 Split data in to training, validation and testing data subsets\n",
    "\n",
    "Split the data using MRJob into three subsets in the following proportions (70% for training, 15% for valdiation, and 15% for testing). Train Multinomial Naive Bayes classifiers using Laplace plus-one smoothing and using  Jelinek-Mercer (JM) smoothing where you consider different hyperparameter values for λ. Please consider λ in {0.0, 0.1, 0.3, 0.5, 0.7, 1}. Present  a table compare the  results of the different approaches: each  row is the approach taken (e.g., Multinomial Naive Bayes with Laplace+1, or Multinomial Naive Bayes with  with JM= 0.3 for λ =0.3) and a column for  error rate on the training, validation and test data sets. Present a graph also (in python) consisting of three curves (where the x-axis represents the approach taken and the y-axis represents the error rate). Dont forget to put a good title on your graph!\n",
    "\n",
    "Looking the validation curve select the best model. How does it perform on the unseen test set? Comment.\n",
    "\n",
    "\n",
    "* REFERENCES \n",
    "   + http://www.ntu.edu.sg/home/gaocong/papers/wpp095-yuan.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.5: Remove words with frequency of less than three (3) in the training set\n",
    "Repeat HW1.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset. Report the error and the change in error. HINT: ignore tokens with a frequency of less than three (3). Think of this as a preprocessing step. How many new mapreduce jobs do you need to solve thus homework? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.6.0: Multinomial Naive Bayes using SciKit-Learn\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.4.2 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the  misclassification error rates (train, validation, testing)\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1.6.1: OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "\n",
    "-  Bernoulli Naive Bayes using SciKit-Learn\n",
    "—  Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.6 and report the misclassification error \n",
    "-  Discuss the performance differences in terms of misclassification error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "\n",
    "Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.7: OPTIONAL Preprocess the Entire Spam Dataset\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW (https://www.dropbox.com/sh/hemnvr0422nr36g/AAAPoK-aYxkFGxGjzaeRNEwSa?dl=0) is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "--- Line 1 contains the subject\n",
    "--- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python (or MrJob) write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.8: OPTIONAL Build and evaluate a NB classifier on the  Entire Spam Dataset\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    " -- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW1.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    " -- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.8.1: OPTIONAL\n",
    "—  Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW1.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "-  Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================\n",
    "END OF HOMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
