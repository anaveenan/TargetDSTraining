{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is 56A0-1A81\n",
      "\n",
      " Directory of C:\\Users\\z001c9v\\Documents\\Target DS Training 2016\\TargetDSTraining\\HW4\n",
      "\n",
      "08/15/2016  12:34 PM    <DIR>          .\n",
      "08/15/2016  12:34 PM    <DIR>          ..\n",
      "08/15/2016  12:17 PM    <DIR>          .ipynb_checkpoints\n",
      "08/15/2016  12:33 PM            18,029 DAMLAS-HW04-Template-2016-08-05.ipynb\n",
      "08/12/2016  10:13 AM               470 dataset.txt\n",
      "08/15/2016  12:34 PM            67,696 Decision_Trees.ipynb\n",
      "08/10/2016  11:33 PM            28,629 titanic_test.csv\n",
      "08/10/2016  11:33 PM            61,194 titanic_train.csv\n",
      "               5 File(s)        176,018 bytes\n",
      "               3 Dir(s)  355,418,136,576 bytes free\n",
      "('TMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('COMPUTERNAME', '9XK2R72')\n",
      "('PROCESSOR_LEVEL', '6')\n",
      "('LIB', ';C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('USERDOMAIN', 'DHC')\n",
      "('SPARK_HOME', 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\')\n",
      "('PSMODULEPATH', 'C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\')\n",
      "('COMMONPROGRAMFILES', 'C:\\\\Program Files\\\\Common Files')\n",
      "('PROCESSOR_IDENTIFIER', 'Intel64 Family 6 Model 94 Stepping 3, GenuineIntel')\n",
      "('VBOX_MSI_INSTALL_PATH', 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\')\n",
      "('PROGRAMFILES', 'C:\\\\Program Files')\n",
      "('PROCESSOR_REVISION', '5e03')\n",
      "('DATACONNECTORLIBPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('SYSTEMROOT', 'C:\\\\WINDOWS')\n",
      "('CLICOLOR', '1')\n",
      "('PROGRAMFILES(X86)', 'C:\\\\Program Files (x86)')\n",
      "('COMSPEC', 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe')\n",
      "('TERM', 'xterm-color')\n",
      "('DOCKER_TOOLBOX_INSTALL_PATH', 'C:\\\\Apps\\\\Docker Toolbox')\n",
      "('WINDOWS_TRACING_LOGFILE', 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log')\n",
      "('DB2INSTANCE', 'DB2')\n",
      "('PROCESSOR_ARCHITECTURE', 'AMD64')\n",
      "('ALLUSERSPROFILE', 'C:\\\\ProgramData')\n",
      "('SCCMNORDCDOWNLOAD', '1')\n",
      "('LOCALAPPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local')\n",
      "('HOMEPATH', '\\\\')\n",
      "('USERDOMAIN_ROAMINGPROFILE', 'DHC')\n",
      "('JAVA_HOME', 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\')\n",
      "('JPY_INTERRUPT_EVENT', '1244')\n",
      "('PROGRAMW6432', 'C:\\\\Program Files')\n",
      "('USERNAME', 'Z001C9V')\n",
      "('COPLIB', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('LOGONSERVER', '\\\\\\\\TETTSDCC10P')\n",
      "('PROMPT', '$P$G')\n",
      "('WINDOWS_TRACING_FLAGS', '3')\n",
      "('JPY_PARENT_PID', '644')\n",
      "('PROGRAMDATA', 'C:\\\\ProgramData')\n",
      "('PYTHONPATH', 'C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;')\n",
      "('CLASSPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\tdgeospatial.jar;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\jmsam.jar;.;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2java.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\sqlj.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc_license_cu.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\bin;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\common.jar')\n",
      "('PATH', 'C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\Apps\\\\R\\\\Rtools\\\\bin;C:\\\\Apps\\\\R\\\\Rtools\\\\gcc-4.6.3\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Teradata Performance Monitor Object 15.00\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\msg;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\ODBC Driver for Teradata\\\\Lib;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files\\\\SAS94\\\\x86\\\\Secure\\\\ccme4;C:\\\\Program Files\\\\SAS94\\\\Secure\\\\ccme4;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\tdwallet\\\\nt-i386\\\\;C:\\\\Program Files (x86)\\\\Microsoft Application Virtualization Client;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin\\\\;C:\\\\WINDOWS\\\\SysWOW64\\\\;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\SysWOW64\\\\Wbem;C:\\\\WINDOWS\\\\SysWOW64\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program Files (x86)\\\\AdminStudio\\\\9.5\\\\Common;C:\\\\PROGRA~1\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Program Files\\\\ibm\\\\gsk8\\\\lib64;C:\\\\Program Files (x86)\\\\ibm\\\\gsk8\\\\lib;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Apps\\\\Anaconda2;C:\\\\Apps\\\\Anaconda2\\\\Scripts;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda3;C:\\\\Apps\\\\Anaconda3\\\\Scripts;C:\\\\Apps\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;;C:\\\\Apps\\\\Docker Toolbox;C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\;C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2;')\n",
      "('HOMESHARE', '\\\\\\\\corp.target.com\\\\dfsroot\\\\HomeDirs\\\\HQ02\\\\59808964')\n",
      "('GIT_PAGER', 'cat')\n",
      "('UATDATA', 'C:\\\\WINDOWS\\\\CCM\\\\UATData\\\\D9F8C395-CAB8-491d-B8AC-179A1FE1BE77')\n",
      "('PATHEXT', '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC')\n",
      "('INCLUDE', 'C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\INCLUDE;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('SESSIONNAME', 'Console')\n",
      "('FP_NO_HOST_CHECK', 'NO')\n",
      "('WINDIR', 'C:\\\\WINDOWS')\n",
      "('TEMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('HOMEDRIVE', 'H:')\n",
      "('OS', 'Windows_NT')\n",
      "('SYSTEMDRIVE', 'C:')\n",
      "('PUBLIC', 'C:\\\\Users\\\\Public')\n",
      "('NUMBER_OF_PROCESSORS', '8')\n",
      "('APPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Roaming')\n",
      "('USERDNSDOMAIN', 'CORP.TARGET.COM')\n",
      "('IBMDB2', 'C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN')\n",
      "('PAGER', 'cat')\n",
      "('COMMONPROGRAMW6432', 'C:\\\\Program Files\\\\Common Files')\n",
      "('HADOOP_HOME', 'C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2')\n",
      "('COMMONPROGRAMFILES(X86)', 'C:\\\\Program Files (x86)\\\\Common Files')\n",
      "('IPY_INTERRUPT_EVENT', '1244')\n",
      "('USERPROFILE', 'C:\\\\Users\\\\z001c9v')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.11 :: Anaconda 4.0.0 (64-bit)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a014ed8fe6a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!dir\n",
    "%pwd\n",
    "!python --version\n",
    "import os\n",
    "\n",
    "for key, value in os.environ.items():\n",
    "    print(key, value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark Initialization: DOCKER \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda2\\;C:\\Apps\\Anaconda2\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;\n",
      "<pyspark.context.SparkContext object at 0x0000000003F2EDD8>\n",
      "<pyspark.sql.context.SQLContext object at 0x00000000065A2CC0>\n"
     ]
    }
   ],
   "source": [
    "# Initiate Spark on Local:  ** Do not run this on Docker **\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python\\\\lib\\\\py4j-0.10.1-src.zip')) #Note, this needs to be in PYTHONPATH env variable.\n",
    "\n",
    "\n",
    "print(os.environ['PYTHONPATH'])\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"MyApp\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS - Machine Learning At Scale\n",
    "## Assignment - HW4\n",
    "Data Analytics and Machine Learning at Scale\n",
    "Target, Minneapolis\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ DAMLAS (Section *Your Section Goes Here*, e.g., Summer 2016)     \n",
    "__Email:__  *Your Target Email User Goes Here*@Target.com     \n",
    "__Week:__   04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW 4 Problems](#3)   \n",
    "    4.0.  [Final Project description](#4.0)   \n",
    "    4.1.  [Build a decision to predict whether you can play tennis or no](#4.1)   \n",
    "    4.2.  [Regression Tree (OPTIONAL Homework)](#4.2)    \n",
    "    4.3.  [Predict survival on the Titanic](#4.3)    \n",
    "    4.4.  [Heritage Healthcare Prize (Predict # Days in Hospital next year)](#4.4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "* Homework submissions are due by Thursday, 08/18/2016 at 11AM (CT).\n",
    "\n",
    "\n",
    "* Prepare a single Jupyter notebook (not a requirment), please include questions, and question numbers in the questions and in the responses.\n",
    "Submit your homework notebook via the following form:\n",
    "\n",
    "   + [Submission Link - Google Form](http://goo.gl/forms/er3OFr5eCMWDngB72)\n",
    "\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "\n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* [Lecture Slides on Decision Trees and Ensembles](https://www.dropbox.com/s/lm4vuocqoq6mq7k/Lecture-13-Decision-Trees-PLanet.pdf?dl=0)\n",
    "* [Notebook on learning classification decision trees](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/p99ro9v9z81wtwh/Decision_Trees.ipynb)\n",
    "\n",
    "* Chapter 17 on decision Trees,   https://www.dropbox.com/s/5ca98ah5chqlcmn/Data_Science_from_Scratch%20%281%29.pdf?dl=0   [Please do not share this PDF]\n",
    "* Karau, Holden, Konwinski, Andy, Wendell, Patrick, & Zaharia, Matei. (2015). Learning Spark: Lightning-fast big data analysis. Sebastopol, CA: O’Reilly Publishers.\n",
    "* Hastie, Trevor, Tibshirani, Robert, & Friedman, Jerome. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). Stanford, CA: Springer Science+Business Media. __(Download for free [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf))__\n",
    "* Ryza, Sandy, Laserson, Uri, Owen, Sean, & Wills, Josh. (2015). Advanced analytics with Spark: Patterns for learning from data at scale. Sebastopol, CA: O’Reilly Publishers.\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW4  <a name=\"4\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"4.0\"></a>\n",
    "## HW4.0 Final Project description\n",
    "\n",
    "Please prepare your project description using the following format\n",
    "* 200 words abstract\n",
    "* data source and description\n",
    "* pipeline of steps (in a block diagram)\n",
    "* Metrics for success\n",
    "\n",
    "PLEASE NOTE: We will probably have project team sizes of 3 people plus/minus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"4.1\"></a>\n",
    "## HW4.1 Build a decision to predict whether you can play tennis or not\n",
    "\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Decision Trees\n",
    "\n",
    "Write a program in Python (or in Spark; the Spark part is optional) to implement the ID3 decision tree algorithm. Have a look at this [notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/p99ro9v9z81wtwh/Decision_Trees.ipynb)  for ideas. You should build a tree to predict PlayTennis, based on the other attributes (but, do not use the Day attribute in your tree.). You should read in a space delimited dataset in a file called dataset.txt and output to the screen your decision tree and the training set accuracy in some readable format. For example, here is the tennis dataset. The first line will contain the names of the fields:\n",
    "\n",
    "<PRE>\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no\n",
    "</PRE>\n",
    "\n",
    "The last column is the classification attribute, and will always contain contain the values yes or no.\n",
    "\n",
    "For output, you can choose how to draw the tree so long as it is clear what the tree is. You might find it easier if you turn the decision tree on its side, and use indentation to show levels of the tree as it grows from the left. For example:\n",
    "\n",
    "<PRE>\n",
    "outlook = sunny\n",
    "|  humidity = high: no\n",
    "|  humidity = normal: yes\n",
    "outlook = overcast: yes\n",
    "outlook = rainy\n",
    "|  windy = TRUE: no\n",
    "|  windy = FALSE: yes\n",
    "\n",
    "</PRE>\n",
    "\n",
    "You don't need to make your tree output look exactly like above: feel free to print out something similarly readable if you think it is easier to code.\n",
    "\n",
    "You may find Python dictionaries especially useful here, as they will give you a quick an easy way to help manage counting the number of times you see a particular attribute.\n",
    "\n",
    "Here are some FAQs that I've gotten in the past regarding this assignment, and some I might get if I don't answer them now.\n",
    "\n",
    "__Should my code work for other datasets besides the tennis dataset?__ \n",
    "Yes. We will give your program a different dataset to try it out with. You may assume that our dataset is correct and well-formatted, but you should not make assumptions regrading number of rows, number of columns, or values that will appear within. The last column will also be the classification, and will always contain yes or no values.\n",
    "\n",
    "__Is it possible that some value, like \"normal,\" could appear in more than one column?__\n",
    "Yes. In addition to the column \"humidity\", we might have had another column called \"skycolor\" which could have values \"normal,\" \"weird,\" and \"bizarre.\"\n",
    "\n",
    "__Could \"yes\" and \"no\" appear as possible values in columns other than the classification column?__\n",
    "Yes. In addition to the classification column \"playtennis,\" we might have had another column called \"seasonalweather\" which would contain \"yes\" and \"no.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "import math, random\n",
    "\n",
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count/total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):        \n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)    \n",
    "    return sum(data_entropy(subset)*len(subset)/total_count for subset in subsets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \n",
    "    ({'outlook':'sunny', 'temperature':'hot', 'humidity':'high', 'wind':'FALSE'}, False),\n",
    "    ({'outlook':'sunny', 'temperature':'hot', 'humidity':'high', 'wind':'TRUE'}, False),\n",
    "    ({'outlook':'overcast', 'temperature':'hot', 'humidity':'high', 'wind':'FALSE'}, True),\n",
    "    ({'outlook':'rainy', 'temperature':'mild', 'humidity':'high', 'wind':'FALSE'}, True),\n",
    "    ({'outlook':'rainy', 'temperature':'cool', 'humidity':'normal', 'wind':'FALSE'}, True),\n",
    "    ({'outlook':'rainy', 'temperature':'cool', 'humidity':'normal', 'wind':'TRUE'}, False),\n",
    "    ({'outlook':'overcast', 'temperature':'cool', 'humidity':'normal', 'wind':'TRUE'}, True),\n",
    "    ({'outlook':'sunny', 'temperature':'mild', 'humidity':'high', 'wind':'FALSE'}, False),\n",
    "    ({'outlook':'sunny', 'temperature':'cool', 'humidity':'normal', 'wind':'FALSE'}, True),\n",
    "    ({'outlook':'rainy', 'temperature':'mild', 'humidity':'normal', 'wind':'FALSE'}, True),\n",
    "    ({'outlook':'sunny', 'temperature':'mild', 'humidity':'normal', 'wind':'TRUE'}, True),\n",
    "    ({'outlook':'overcast', 'temperature':'mild', 'humidity':'high', 'wind':'TRUE'}, True),\n",
    "    ({'outlook':'overcast', 'temperature':'hot', 'humidity':'normal', 'wind':'FALSE'}, True),\n",
    "    ({'outlook':'rainy', 'temperature':'mild', 'humidity':'high', 'wind':'TRUE'}, False)    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item \n",
    "    is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups\n",
    "    \n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
    "    each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"        \n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook 0.693536138896\n",
      "temperature 0.911063393012\n",
      "humidity 0.788450457308\n",
      "wind 0.892158928262\n"
     ]
    }
   ],
   "source": [
    "for key in['outlook','temperature','humidity','wind']:\n",
    "    print key, partition_entropy_by(inputs, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " temperature 0.4\n",
      "humidity 0.0\n",
      "wind 0.950977500433\n"
     ]
    }
   ],
   "source": [
    "sunny_inputs = [(input, label) for input, label in inputs if input[\"outlook\"] == \"sunny\"]\n",
    "\n",
    "for key in ['temperature','humidity','wind']:\n",
    "    print key, partition_entropy_by(sunny_inputs, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "    \n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "   \n",
    "    # otherwise find the correct subtree\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute)  # None if input is missing attribute\n",
    "\n",
    "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
    "        subtree_key = None              # we'll use the None subtree\n",
    "    \n",
    "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
    "    return classify(subtree, input)     # and use it to classify the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the tree\n",
      "('outlook', {'rainy': ('wind', {'FALSE': True, None: True, 'TRUE': False}), 'overcast': True, 'sunny': ('humidity', {'high': False, None: False, 'normal': True}), None: True})\n"
     ]
    }
   ],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "\n",
    "    # if this is our first pass, \n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0:                  # if only Falses are left\n",
    "        return False                    # return a \"False\" leaf\n",
    "        \n",
    "    if num_falses == 0:                 # if only Trues are left\n",
    "        return True                     # return a \"True\" leaf\n",
    "\n",
    "    if not split_candidates:            # if no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "                            \n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "        key=partial(partition_entropy_by, inputs))\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates \n",
    "                      if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.iteritems() }\n",
    "\n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "\n",
    "    return (best_attribute, subtrees)\n",
    "\n",
    "print \"building the tree\"\n",
    "tree = build_tree_id3(inputs)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rainy / mild / normal / TRUE:  False\n",
      "overcast / mild / normal / TRUE:  True\n"
     ]
    }
   ],
   "source": [
    "print ( \"rainy / mild / normal / TRUE: \" ) , classify(tree, \n",
    "     {\"outlook\" : \"rainy\", \n",
    "      \"temperature\" : \"mild\", \n",
    "      \"humidity\" : \"normal\", \n",
    "      \"wind\" : \"TRUE\"})  \n",
    " \n",
    "print ( \"overcast / mild / normal / TRUE: \" ) , classify(tree, \n",
    "     {\"outlook\" : \"overcast\", \n",
    "      \"temperature\" : \"mild\", \n",
    "      \"humidity\" : \"normal\", \n",
    "      \"wind\" : \"TRUE\"})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intern:  True\n",
      "Senior:  True\n"
     ]
    }
   ],
   "source": [
    "print \"Intern: \", classify(tree, { \"level\" : \"Intern\" } )\n",
    "print \"Senior: \", classify(tree, { \"level\" : \"Senior\" } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW4.1.1 What is the classification accuracy of the tree on the training data?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.2  Is it possible to produce some set of correct training examples that will get the algorihtm\n",
    "to include the attribute Temperature in the learned tree, even though the true target concept is\n",
    "independent of Temperature? if no, explain. If yes, give such a set. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.3  Now, build a tree using only examples D1–D7. What is the classification accuracy for the\n",
    "training set? what is the accuracy for the test set (examples D8–D14)? explain why you think these\n",
    "are the results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.4 In this case, and others, there are only a few labelled examples available for training (that\n",
    "is, no additional data is available for testing or validation). Suggest a concrete pruning strategy, that\n",
    "can be readily embedded in the algorithm, to avoid over fitting. Explain why you think this strategy\n",
    "should work.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"4.2\"></a>\n",
    " ## HW4.2 Regression Tree (OPTIONAL Homework) \n",
    " \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Implement a decision tree algorithm for regression for two input continous variables and one categorical input variable on a single core computer using Python. \n",
    "\n",
    "- Use the IRIS dataset to evaluate your code, where the input variables are: Petal.Length Petal.Width  Species  and the target or output variable is  Sepal.Length. \n",
    "- Use the same dataset to train and test your implementation. \n",
    "- Stop expanding nodes once you have less than ten (10) examples (along with the usual stopping criteria). \n",
    "- Report the mean squared error for your implementation and contrast that with the MSE from scikit-learn's implementation on this dataset (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"4.3\"></a>\n",
    "## HW4.3 Predict survival on the Titanic using Python (Logistic regression, SVMs, Random Forests)\n",
    "\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, you need to review (and edit the code) in this [notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/kmbgrkhh73931lo/Titanic-EDA-LogisticRegression.ipynb) to do analysis of what sorts of people were likely to survive. In particular, please look at how the tools of machine learning are used to predict which passengers survived the tragedy. Please share any useful graphs/analysis you come up with via the group email. For example, pick the top two most important variables and plot the separating hyperplane in this 2D space that is generated using an SVM (or logistic regression model or plot both; are they similar?) that is learnt using those two features only. Comment on your observations. Please feel free to come up other graphs/analysis (e.g., clustering the passengers). \n",
    "\n",
    "For more details see:\n",
    "\n",
    "* https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<type 'str'>\n",
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "<type 'list'>\n",
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "('t_train:', <type 'list'>)\n",
      "('t_train:', ['1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S', '2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C', '3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S', '4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S', '5,0,3,\"Allen, Mr. William Henry\",male,35,0,0,373450,8.05,,S'])\n",
      "('t_train_rdd:', <class 'pyspark.rdd.RDD'>)\n",
      "('t_train_2:', <class 'pyspark.rdd.PipelinedRDD'>)\n",
      "('t_train_3:', <class 'pyspark.rdd.PipelinedRDD'>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input training data\n",
    "#train_data_csv = '/home/jovyan/work/root/Documents/Target DS Training 2016/TargetDSTraining/HW4/titanic_train.csv'\n",
    "t_train = sc.textFile(\"titanic_train.csv\")\n",
    "print (type(t_train))\n",
    "\n",
    "t_train_utf = t_train.map(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "col_names = t_train_utf.collect()[0]\n",
    "print (type(col_names))\n",
    "print (col_names)\n",
    "col_names_dt = col_names.split(',')\n",
    "print (type(col_names_dt))\n",
    "col_names_dt.remove('PassengerId')\n",
    "col_names_dt.remove('Name')\n",
    "print (col_names_dt)\n",
    "                                  \n",
    "\n",
    "\n",
    "t_train = t_train_utf.cache()\n",
    "#del t_train[0]\n",
    "t_data = t_train.collect()[1:]      # indexing the RDD removes Rows!!!!\n",
    "print ('t_train:' , type(t_train.collect()[1:]))\n",
    "print ('t_train:' , t_data[:5])\n",
    "t_train_rdd = sc.parallelize(t_data)\n",
    "print ('t_train_rdd:' , type(t_train_rdd))\n",
    "t_train_rdd.take(5)\n",
    "\n",
    "t_train_2 = t_train_rdd.map(lambda line: line.split(','))\n",
    "print ('t_train_2:' ,type(t_train_2))\n",
    "t_train_2.take(5)\n",
    "\n",
    "t_train_3 = t_train_2.map(lambda line: line.pop(0))\n",
    "print ('t_train_3:' ,type(t_train_3))\n",
    "t_train_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 9, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-31-32b35ebc4de3>\", line 2, in <lambda>\nNameError: global name 'Row' is not defined\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-31-32b35ebc4de3>\", line 2, in <lambda>\nNameError: global name 'Row' is not defined\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-32b35ebc4de3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Start by parsing the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt_train_rdd_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mt_train_rdd_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#print t_train_rdd_1.collect()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \"\"\"\n\u001b[1;32m-> 1328\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    942\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 9, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-31-32b35ebc4de3>\", line 2, in <lambda>\nNameError: global name 'Row' is not defined\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-31-32b35ebc4de3>\", line 2, in <lambda>\nNameError: global name 'Row' is not defined\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Start by parsing the training data\n",
    "t_train_rdd_1 = t_train.map(lambda x: Row(x.split(',')))\n",
    "t_train_rdd_1.first()\n",
    "#print t_train_rdd_1.collect()\n",
    "                            \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "parts = t_train.map(lambda l: l.split(\",\"))\n",
    "print (type(people))\n",
    "#people = t_train.map(lambda p: Row() for c in parts).collect() #, age=int(p[1])))\n",
    "print (type(people))\n",
    "\n",
    "#df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
    "df = sqlContext.createDataFrame(t_train, [\"PassengerId\",\"Survived\",\"Pclass\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\"])\n",
    "df.collect()\n",
    "#for i in people:\n",
    " #   print i\n",
    "\n",
    "#sqlContext.createDataFrame(t_train , ('PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-a6d11e51f4d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sqlContext.createDataFrame()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#.printSchema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mPerson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"age\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPerson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Row' is not defined"
     ]
    }
   ],
   "source": [
    "#sqlContext.createDataFrame()\n",
    "#.printSchema\n",
    "Person = Row(\"name\", \"age\")\n",
    "print type(Person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the data should be RDD of LabeledPoint",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-1cf518de90db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m model = DecisionTree.trainClassifier(t_train, numClasses=2, categoricalFeaturesInfo={},\n\u001b[1;32m----> 7\u001b[1;33m                                      impurity='gini', maxDepth=5, maxBins=32)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\mllib\\tree.py\u001b[0m in \u001b[0;36mtrainClassifier\u001b[1;34m(cls, data, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \"\"\"\n\u001b[0;32m    215\u001b[0m         return cls._train(data, \"classification\", numClasses, categoricalFeaturesInfo,\n\u001b[1;32m--> 216\u001b[1;33m                           impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\mllib\\tree.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(cls, data, type, numClasses, features, impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\u001b[0m\n\u001b[0;32m    139\u001b[0m                minInstancesPerNode=1, minInfoGain=0.0):\n\u001b[0;32m    140\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"the data should be RDD of LabeledPoint\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         model = callMLlibFunc(\"trainDecisionTreeModel\", data, type, numClasses, features,\n\u001b[0;32m    143\u001b[0m                               impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\n",
      "\u001b[1;31mAssertionError\u001b[0m: the data should be RDD of LabeledPoint"
     ]
    }
   ],
   "source": [
    "# Using mllib :\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from time import time\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "model = DecisionTree.trainClassifier(t_train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "t0 = time()\n",
    "model = DecisionTree.trainClassifier(t_train_rdd_1, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "t1 = time()\n",
    "\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"4.4\"></a>\n",
    " ## HW4.4 Heritage Healthcare Prize (Predict # Days in Hospital next year)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "1. Introduction \n",
    "Back to Table of Contents\n",
    "\n",
    "The Heritage Health Prize (HHP) was a data science challenge sponsored by The Heritage Provider Network. It took place from April 4, 2011 to April 4, 2013. For information on the winning entries, please see here.\n",
    "\n",
    "Please see the following notebooks for more background and candidate solutions\n",
    "\n",
    "\n",
    "- Spark Map-Reduce + MMLlib solution (with optional extensions) See [Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/v52cxipe7yftf97/HeritageHealthPrizeUnitTestNotebook_Spark-Map-Reduce.ipynb)\n",
    "\n",
    "- Spark SQL + MLLib solution (with optional extensions): [Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/s2wxg6g982oho5m/HeritageHealthPrizeUnitTestNotebook_SQL_FINAL.ipynb)\n",
    "\n",
    "\n",
    "Please look at section 7 in both notebooks complete any one or more the suggested next steps. E.g.,\n",
    "\n",
    "* Please complete the EDA extensions using inspiration from the Titanic Notebook from above.\n",
    "* __Complete Section 3.B: EDA-0. Gather information to see what transformations may need to be done on the data.__\n",
    "Answer questions about each raw DataFrame. In general, is the data in good shape? For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what values does DaysInHospital take on? Are they all integers? What values does ClaimsTruncated take on? Are they all integers? In the Claims DataFrame (df_claims), how many different ProviderIDs are there? How many different PrimaryConditionGroups are there? What are their values? What values can the CharlesonIndex take on? Are they integers? In the Drug Count DataFrame (df_drug_count), what values can DrugCount take on? Are they all integers? Given this information, what transformations are needed?\n",
    "\n",
    "* __Complete Section 3.D: EDA-1. Create tables and graphs to display information about the transformed DataFrames. __\n",
    "For inspiration, see the Titanic notebook discussed above. Answer questions about each DataFrame. For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what is the minimum, maximum, mean, and standard deviation of DaysInHospital? In the Claims DataFrame, group by MemberID and Year and count the number of records. What is the minimum, maximum, mean, and standard deviation of the count? Do the same for the Drug Count and Lab Count DataFrames, etc.\n",
    "\n",
    "\n",
    "* __ Please generate ensemble of DT model using 100 trees with 8 nodes and report the Loss __\n",
    "Try additional models. See possibilities here (e.g. Decision Tree Regressor, Gradient-Boosted Trees Regressor, Random Forest Regressor). See an example here. Tune their hyperparameters. Try different feature selections. Try a two-step model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
