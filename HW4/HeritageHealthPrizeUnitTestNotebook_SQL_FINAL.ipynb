{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning at Scale\n",
    "Dr. James G. Shanahan and Marguerite Oneto, with assistance from Kasane Utsumi   \n",
    "&copy; copyright July 2016\n",
    "\n",
    "\n",
    "# The Heritage Health Prize Challenge Using Spark SQL\n",
    "\n",
    "### Improve Healthcare, Win \\$3,000,000.  Identify patients who will be admitted to a hospital within the next year using historical claims data.\n",
    "\n",
    "This notebook uses [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html), [DataFrames](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame), and [Pipelines](https://spark.apache.org/docs/latest/ml-guide.html) to build a basic pipeline for modeling outcomes using the Heritage Health Prize dataset.\n",
    "\n",
    "The notebook is organized as follows:  \n",
    "\n",
    "1.  **Introduction**\n",
    "2.  **Set Up Spark**\n",
    "3.  **ETL**\n",
    "4.  **Feature Engineering**\n",
    "5.  **Modeling, Evaluation, and Tuning**\n",
    "6.  **Results**\n",
    "7.  **Next Steps**\n",
    "8.  **Resources**\n",
    "9.  **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [Introduction](#1)   \n",
    "2.  [Set Up Spark](#2)\n",
    "    1.  [Local Set-Up](#2.A)\n",
    "    2.  [Docker Set-Up](#2.B)\n",
    "    3.  [Cluster Set-Up](#2.C)\n",
    "3.  [ETL](#3)\n",
    "    1.  [Extract](#3.A)\n",
    "    2.  [EDA-0](#3.B)\n",
    "    3.  [Transform and Load](#3.C)\n",
    "        1.  [Target Variables](#3.C.a)\n",
    "        2.  [Claims Data](#3.C.b)\n",
    "        3.  [Drug Data](#3.C.c)\n",
    "        4.  [Lab Data](#3.C.d)\n",
    "        5.  [Members Data](#3.C.e)\n",
    "    4.  [EDA-1](#3.D)\n",
    "4.  [Feature Engineering](#4)\n",
    "    1.  [Aggregation](#4.A)\n",
    "        1.  [Claims Data](#4.A.a)\n",
    "        2.  [Drug Data](#4.A.b)\n",
    "        3.  [Lab Data](#4.A.c)\n",
    "    2.  [One Hot Encoding: Members Data](#4.B)\n",
    "    3.  [Create Final DataFrame For Modeling](#4.C)\n",
    "        1.  [Merge Data](#4.C.a)\n",
    "        2.  [Handle Missing Data](#4.C.b)\n",
    "        3.  [Drop Unnecessary Variables](#4.C.c)\n",
    "        4.  [External Storage Read/Write of Final DataFrame](#4.C.d)\n",
    "        5.  [EDA-2](#4.C.e)\n",
    "5.  [Modeling, Evaluation, and Tuning](#5)\n",
    "    1.  [Create Training, Validation, and Test DataFrames](#5.A)\n",
    "    2.  [Define Evaluation Metric](#5.B)\n",
    "    3.  [Build and Evaluate Baseline Model](#5.C)\n",
    "    4.  [Feature Selection: Principal Component Analysis (PCA)](#5.D)\n",
    "    5.  [Hyperparameter Tuning](#5.E)\n",
    "6.  [Results](#6)\n",
    "7.  [Next Steps](#7)\n",
    "8.  [Resources](#8)\n",
    "9.  [Appendix](#9)\n",
    "    1.  [Runtimes](#9.A)\n",
    "    2.  [Example of Training a Non-Linear Model Using Spark Pipelines](#9.B)\n",
    "    3.  [User-Defined Functions in Spark Pipelines](#9.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Introduction <a name=\"1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "[The Heritage Health Prize (HHP)](https://www.heritagehealthprize.com/c/hhp) was a data science challenge sponsored by [The Heritage Provider Network](http://www.heritageprovidernetwork.com).  It took place from April 4, 2011 to April 4, 2013.  For information on the winning entries, please see [here](http://www.heritagehealthprize.com/c/hhp/details/milestone-winners).\n",
    "\n",
    "In this notebook, we follow a traditional data science project process to address the HHP challenge of predicting future hospital stays using past patient treatment information.  We extract, transform, and load the data (**ETL**).We conduct **feature engineering**.  Along the way, we do some exploratory data analysis (**EDA**).  We then **create models, evaluate their performance, and fine tune their parameters**.  We write up our **results**.  We explain what additional research we would like to do in a list of **next steps**.  We list our **resources**.  And finally, we create an **appendix** for tangential information related to our project.\n",
    "\n",
    "These are the Python modules needed to run this notebook:\n",
    "\n",
    "1.  pyspark\n",
    "2.  pandas\n",
    "3.  os\n",
    "4.  sys\n",
    "5.  math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Set Up Spark <a name=\"2\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A Local Set-Up <a name=\"2.A\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Use this code to set up a spark context to run on your local computer.  Set the SPARK_HOME environment variable to the Spark directory being used on your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Replace the path below with your particular local Spark directory\n",
    "# SPARK_HOME='/usr/local/Cellar/apache-spark/spark-1.6.2-bin-hadoop2.6/'\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "# if not spark_home:\n",
    "#     raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "# sys.path.insert(0, os.path.join(spark_home,'python'))\n",
    "# sys.path.insert(0, os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "# execfile(os.path.join(spark_home,'python/pyspark/shell.py'))\n",
    "\n",
    "# from pyspark.sql import SQLContext\n",
    "# sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B Docker Set-Up <a name=\"2.B\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Use this code to set up a Spark Context and a SQL Context to run on a Docker virtual machine on your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# import pyspark\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# # We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# # In this case, it is local multicore execution with \"local[*]\"\n",
    "# app_name = \"hhp\"\n",
    "# master = \"local[*]\"\n",
    "# conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "# sc = pyspark.SparkContext(conf=conf)\n",
    "# sql_context = SQLContext(sc)\n",
    "# print sc\n",
    "# print sql_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C Cluster Set-Up <a name=\"2.C\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Use this code to set up a SQL Context when the cluster already has a Spark Context running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x7effae3df790>\n"
     ]
    }
   ],
   "source": [
    "sql_context = SQLContext(sc)\n",
    "print sql_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.  ETL <a name=\"3\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    " A copy of the HHP dataset can be downloaded [here](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AADp4D50rGM61hpaSThZnqF3a/HHP_release3?dl=0).  It consists of the following files:\n",
    "1.  [Claims.csv](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AADAkd0WulfgFthMibmIBtJoa/HHP_release3/Claims.csv?dl=0)  (Claims Data - features)\n",
    "2.  [DaysInHospital_Y2.csv](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AAD0_gwrKFo657caVjV3mjZVa/HHP_release3/DaysInHospital_Y2.csv?dl=0)  (Target Data for Year 2 - labels)\n",
    "3.  [DaysInHospital_Y3.csv](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AACmJkf7txiQiPZ_jd5cqsf3a/HHP_release3/DaysInHospital_Y3.csv?dl=0)  (Target Data for Year 3 - labels)\n",
    "4.  [DrugCount.csv](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AAA_9IxBOsoBSqyhlu7x8z2qa/HHP_release3/DrugCount.csv?dl=0)  (Drug Count Data - features)\n",
    "5.  [LabCount.csv](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AABxVuWLtBLqdYr6pyJCPsfga/HHP_release3/LabCount.csv?dl=0)  (Lab Count Data - features)\n",
    "6.  [Members.csv](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AACgVHEh2yUxzavQlurF8hfla/HHP_release3/Members.csv?dl=0)  (Members Data -features)\n",
    "7.  [Target.csv](https://www.dropbox.com/sh/upt0j2q44ncrn1m/AAD7M6yWTsRtToS7uKmbWrA7a/HHP_release3/Target.csv?dl=0)  (Target Data for Year 4 - labels)\n",
    "\n",
    "The Extract-Transform-Load process will have three steps:  \n",
    "1.  Extract  \n",
    "2.  EDA-0  \n",
    "3.  Transform and Load  \n",
    "\n",
    "**NOTE: The data dictionary for the HHP data can be found [here](https://www.heritagehealthprize.com/c/hhp/data).**\n",
    "\n",
    "<img src=\"https://dl.dropbox.com/s/d5u2et0xahnt2ct/HHP_DataDictionary.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "First, set the directory where the data can be found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the directory containing the raw data files\n",
    "DATA_DIR = os.path.join('data/HHP_release3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A  Extract <a name=\"3.A\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Spark 1.6 does not have native support to load csv files directly into SQL DataFrames. We tried [Databrick's CSV Data Source package](https://github.com/databricks/spark-csv), but that did not work. Thus, we implemented a work-around by first reading the csv file into a pandas dataframe and then reading the pandas dataframe into a Spark SQL DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for target_Y2 DataFrame with 76038 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|24027423|              0|             0|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for target_Y3 DataFrame with 71435 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|90963501|              0|             0|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for target_Y4 DataFrame with 70942 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ClaimsTruncated: string (nullable = true)\n",
      " |-- DaysInHospital: string (nullable = true)\n",
      "\n",
      "+--------+---------------+--------------+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|\n",
      "+--------+---------------+--------------+\n",
      "|20820036|              0|           NaN|\n",
      "+--------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for claims DataFrame with 2668990 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- ProviderID: string (nullable = true)\n",
      " |-- Vendor: string (nullable = true)\n",
      " |-- PCP: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Specialty: string (nullable = true)\n",
      " |-- PlaceSvc: string (nullable = true)\n",
      " |-- PayDelay: string (nullable = true)\n",
      " |-- LengthOfStay: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- PrimaryConditionGroup: string (nullable = true)\n",
      " |-- CharlsonIndex: string (nullable = true)\n",
      " |-- ProcedureGroup: string (nullable = true)\n",
      " |-- SupLOS: string (nullable = true)\n",
      "\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "|MemberID|ProviderID|Vendor|  PCP|Year|Specialty|PlaceSvc|PayDelay|LengthOfStay|       DSFS|PrimaryConditionGroup|CharlsonIndex|ProcedureGroup|SupLOS|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "|42286978|   8013252|172193|37796|  Y1|  Surgery|  Office|      28|         NaN|8- 9 months|              NEUMENT|            0|           MED|     0|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for drug_count DataFrame with 818241 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- DrugCount: string (nullable = true)\n",
      "\n",
      "+--------+----+-----------+---------+\n",
      "|MemberID|Year|       DSFS|DrugCount|\n",
      "+--------+----+-----------+---------+\n",
      "|48925661|  Y2|9-10 months|       7+|\n",
      "+--------+----+-----------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for lab_count DataFrame with 361484 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- DSFS: string (nullable = true)\n",
      " |-- LabCount: string (nullable = true)\n",
      "\n",
      "+--------+----+-----------+--------+\n",
      "|MemberID|Year|       DSFS|LabCount|\n",
      "+--------+----+-----------+--------+\n",
      "|69258001|  Y3|2- 3 months|       1|\n",
      "+--------+----+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "Schema for members DataFrame with 113000 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- AgeAtFirstClaim: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      "\n",
      "+--------+---------------+---+\n",
      "|MemberID|AgeAtFirstClaim|Sex|\n",
      "+--------+---------------+---+\n",
      "|14723353|          70-79|  M|\n",
      "+--------+---------------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def load_csv_file(filename):\n",
    "    # HACK - Loading csv directly into a dataframe was supposed to work by using a Databricks\n",
    "    # package (https://github.com/databricks/spark-csv), but it didn't.\n",
    "    # So in this section, we are going the Pandas -> DataFrame route.  Could also go RDD -> DataFrame route.\n",
    "    \n",
    "    # Set our file path\n",
    "    input_path = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    # Read into Pandas dataframe.  We assume the file contains a header row.\n",
    "    # Note:  We read in all fields as string.\n",
    "    df_pandas = pd.read_csv(input_path, dtype='str')  \n",
    "    \n",
    "    # Read Pandas dataframe into SQL DataFrame.\n",
    "    # We must define the schema so that datatype is not automatically inferred and everything remains string.\n",
    "    schema_string = df_pandas.columns.values.tolist()\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schema_string]\n",
    "    schema = StructType(fields)\n",
    "    \n",
    "    return sql_context.createDataFrame(df_pandas, schema)\n",
    "\n",
    "# Read in Year 2 Target Variables\n",
    "df_target_Y2 = load_csv_file('DaysInHospital_Y2.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y2 DataFrame with %d rows:\" %(df_target_Y2.count())\n",
    "df_target_Y2.printSchema()\n",
    "df_target_Y2.show(1)\n",
    "\n",
    "# Read in Year 3 Target Variables\n",
    "df_target_Y3 = load_csv_file('DaysInHospital_Y3.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y3 DataFrame with %d rows:\" %(df_target_Y3.count())\n",
    "df_target_Y3.printSchema()\n",
    "df_target_Y3.show(1)\n",
    "\n",
    "# Read in Year 4 Target Variables\n",
    "df_target_Y4 = load_csv_file('Target.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for target_Y4 DataFrame with %d rows:\" %(df_target_Y4.count())\n",
    "df_target_Y4.printSchema()\n",
    "df_target_Y4.show(1)\n",
    "\n",
    "# Read in Claims Data\n",
    "df_claims = load_csv_file('Claims.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for claims DataFrame with %d rows:\" %(df_claims.count())\n",
    "df_claims.printSchema()\n",
    "df_claims.show(1)\n",
    "\n",
    "# Read in Drug Data\n",
    "df_drug_count = load_csv_file('DrugCount.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for drug_count DataFrame with %d rows:\" %(df_drug_count.count())\n",
    "df_drug_count.printSchema()\n",
    "df_drug_count.show(1)\n",
    "\n",
    "# Read in Lab Data\n",
    "df_lab_count = load_csv_file('LabCount.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for lab_count DataFrame with %d rows:\" %(df_lab_count.count())\n",
    "df_lab_count.printSchema()\n",
    "df_lab_count.show(1)\n",
    "\n",
    "# Read in Members Data\n",
    "df_members = load_csv_file('Members.csv').cache()\n",
    "# Check the data\n",
    "print \"Schema for members DataFrame with %d rows:\" %(df_members.count())\n",
    "df_members.printSchema()\n",
    "df_members.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B  EDA-0 <a name=\"3.B\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "It is a best practice to do some EDA on each table to see what transformations we need to complete to get the data in shape for feature engineering.  Given Spark DataFrames, there are two ways to explore the data:   \n",
    "\n",
    "1.  Run SQL queries on temporary tables created from the DataFrames using 'sql_context.sql'  OR  \n",
    "2.  Run queries directly on the DataFrames (for a list of DataFrame operations, see [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)).\n",
    "\n",
    "Below are examples of each method.  Also see [Section 7: Next Steps](#7.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 1: SQL queries on temporary tables\n",
    "# In order to run SQL queries on DataFrames, we must first register them as temporary tables\n",
    "\n",
    "# df_target_Y2.registerTempTable(\"target_Y2_tt\")\n",
    "# df_target_Y3.registerTempTable(\"target_Y3_tt\")\n",
    "# df_target_Y4.registerTempTable(\"target_Y4_tt\")\n",
    "df_claims.registerTempTable(\"claims_tt\")\n",
    "# df_drug_count.registerTempTable(\"drug_count_tt\")\n",
    "# df_lab_count.registerTempTable(\"lab_count_tt\")\n",
    "# df_members.registerTempTable(\"members_tt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the Claims Data where the LengthOfStay is suppressed:\n",
      "+-----+\n",
      "|  _c0|\n",
      "+-----+\n",
      "|11332|\n",
      "+-----+\n",
      "\n",
      "Possible Values for PrimaryConditionGroup:\n",
      "+---------------------+\n",
      "|PrimaryConditionGroup|\n",
      "+---------------------+\n",
      "|                  NaN|\n",
      "|              NEUMENT|\n",
      "|               RESPR4|\n",
      "|               INFEC4|\n",
      "|               SEPSIS|\n",
      "|               STROKE|\n",
      "|              PERVALV|\n",
      "|               CANCRA|\n",
      "|               CANCRB|\n",
      "|              LIVERDZ|\n",
      "|              SEIZURE|\n",
      "|               CANCRM|\n",
      "|               PRGNCY|\n",
      "|               SKNAUT|\n",
      "|               HEART2|\n",
      "|               HEART4|\n",
      "|                  CHF|\n",
      "|               RENAL1|\n",
      "|               RENAL2|\n",
      "|               RENAL3|\n",
      "|                ROAMI|\n",
      "|               MISCL1|\n",
      "|               MISCL5|\n",
      "|             ARTHSPIN|\n",
      "|               PNCRDZ|\n",
      "|              PERINTL|\n",
      "|                  AMI|\n",
      "|              APPCHOL|\n",
      "|               CATAST|\n",
      "|               TRAUMA|\n",
      "|                HIPFX|\n",
      "|                PNEUM|\n",
      "|                  UTI|\n",
      "|               MSC2a3|\n",
      "|               HEMTOL|\n",
      "|               METAB1|\n",
      "|               METAB3|\n",
      "|                 COPD|\n",
      "|             GIOBSENT|\n",
      "|              GIBLEED|\n",
      "|              FLaELEC|\n",
      "|               GYNEC1|\n",
      "|              FXDISLC|\n",
      "|              ODaBNCA|\n",
      "|              MISCHRT|\n",
      "|               GYNECA|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then we can run some queries on the tt tables\n",
    "# Example: Count the number of samples in the Claims Data where the LengthOfStay is suppressed\n",
    "df_sup_los_1 = sql_context.sql(\"SELECT COUNT(MemberID) FROM claims_tt WHERE SupLOS = 1\")\n",
    "print 'Number of samples in the Claims Data where the LengthOfStay is suppressed:'\n",
    "df_sup_los_1.show(1)\n",
    "df_sup_los_1.unpersist()\n",
    "# Example:  Get all possible values of the variable PrimaryConditionGroup\n",
    "df_pcg = sql_context.sql(\"SELECT DISTINCT(PrimaryConditionGroup) FROM claims_tt\")\n",
    "print 'Possible Values for PrimaryConditionGroup:'\n",
    "df_pcg.show(df_pcg.count())\n",
    "\n",
    "# Free up memory\n",
    "sql_context.dropTempTable(\"claims_tt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:  EDA on each temp table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MemberID', 'ProviderID', 'Vendor', 'PCP', 'Year', 'Specialty', 'PlaceSvc', 'PayDelay', 'LengthOfStay', 'DSFS', 'PrimaryConditionGroup', 'CharlsonIndex', 'ProcedureGroup', 'SupLOS']\n",
      "+----+------+\n",
      "|Year| count|\n",
      "+----+------+\n",
      "|  Y1|865689|\n",
      "|  Y2|898872|\n",
      "|  Y3|904429|\n",
      "+----+------+\n",
      "\n",
      "+--------------------+------+\n",
      "|           Specialty| count|\n",
      "+--------------------+------+\n",
      "|                 NaN|  8405|\n",
      "|Obstetrics and Gy...| 36594|\n",
      "|               Other| 92687|\n",
      "|  Diagnostic Imaging|207297|\n",
      "|          Pediatrics| 84862|\n",
      "|      Rehabilitation| 57554|\n",
      "|            Internal|672059|\n",
      "|           Emergency|126130|\n",
      "|             Surgery|208217|\n",
      "|          Laboratory|653188|\n",
      "|      Anesthesiology| 33435|\n",
      "|    General Practice|473655|\n",
      "|           Pathology| 14907|\n",
      "+--------------------+------+\n",
      "\n",
      "+-------------------+-------+\n",
      "|           PlaceSvc|  count|\n",
      "+-------------------+-------+\n",
      "|                NaN|   7632|\n",
      "|        Urgent Care| 199528|\n",
      "|              Other|  11700|\n",
      "|    Independent Lab| 657750|\n",
      "|          Ambulance|  34766|\n",
      "|Outpatient Hospital| 121528|\n",
      "| Inpatient Hospital|  85776|\n",
      "|             Office|1542007|\n",
      "|               Home|   8303|\n",
      "+-------------------+-------+\n",
      "\n",
      "+------------+-------+\n",
      "|LengthOfStay|  count|\n",
      "+------------+-------+\n",
      "|         NaN|2597392|\n",
      "|      5 days|    510|\n",
      "|  1- 2 weeks|   1143|\n",
      "|      4 days|   1473|\n",
      "|  2- 4 weeks|    961|\n",
      "|  4- 8 weeks|    903|\n",
      "|      3 days|   3246|\n",
      "|   26+ weeks|      2|\n",
      "|      2 days|   6485|\n",
      "|      6 days|    179|\n",
      "|       1 day|  56696|\n",
      "+------------+-------+\n",
      "\n",
      "+------------+------+\n",
      "|        DSFS| count|\n",
      "+------------+------+\n",
      "|         NaN| 52770|\n",
      "| 8- 9 months|171878|\n",
      "|10-11 months|116328|\n",
      "| 7- 8 months|175191|\n",
      "| 6- 7 months|180662|\n",
      "|  0- 1 month|707721|\n",
      "| 5- 6 months|192000|\n",
      "| 4- 5 months|189001|\n",
      "| 9-10 months|151527|\n",
      "| 3- 4 months|212214|\n",
      "| 2- 3 months|225216|\n",
      "| 1- 2 months|247343|\n",
      "|11-12 months| 47139|\n",
      "+------------+------+\n",
      "\n",
      "+---------------------+------+\n",
      "|PrimaryConditionGroup| count|\n",
      "+---------------------+------+\n",
      "|                  NaN| 11410|\n",
      "|              NEUMENT|171605|\n",
      "|               RESPR4|138062|\n",
      "|               INFEC4| 83552|\n",
      "|               SEPSIS|   497|\n",
      "|               STROKE|  8416|\n",
      "|              PERVALV|  3518|\n",
      "|               CANCRA|  5587|\n",
      "|               CANCRB| 42895|\n",
      "|              LIVERDZ|  2747|\n",
      "|              SEIZURE| 20501|\n",
      "|               CANCRM|  1096|\n",
      "|               PRGNCY| 32004|\n",
      "|               SKNAUT|107976|\n",
      "|               HEART2| 54207|\n",
      "|               HEART4| 28733|\n",
      "|                  CHF| 13316|\n",
      "|               RENAL1|   602|\n",
      "|               RENAL2| 10922|\n",
      "|               RENAL3| 52214|\n",
      "+---------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-------+\n",
      "|CharlsonIndex|  count|\n",
      "+-------------+-------+\n",
      "|           5+|   5989|\n",
      "|          3-4|  49479|\n",
      "|            0|1356995|\n",
      "|          1-2|1256527|\n",
      "+-------------+-------+\n",
      "\n",
      "+--------------+-------+\n",
      "|ProcedureGroup|  count|\n",
      "+--------------+-------+\n",
      "|           NaN|   3675|\n",
      "|           SGS|   9406|\n",
      "|           SAS|   5745|\n",
      "|           SNS|   7796|\n",
      "|           SUS|   5612|\n",
      "|           RAD| 265272|\n",
      "|           SIS|  56461|\n",
      "|           SCS| 274805|\n",
      "|          ANES|  17061|\n",
      "|          SMCD|   3376|\n",
      "|            PL| 492919|\n",
      "|           SDS|  60678|\n",
      "|            EM|1048210|\n",
      "|           SRS|   7905|\n",
      "|           MED| 372101|\n",
      "|          SEOA|   8420|\n",
      "|            SO|    371|\n",
      "|           SMS|  29177|\n",
      "+--------------+-------+\n",
      "\n",
      "+------+-------+\n",
      "|SupLOS|  count|\n",
      "+------+-------+\n",
      "|     0|2657658|\n",
      "|     1|  11332|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Direct queries of DataFrames\n",
    "# DataFrame Operations:  https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "# Example: Get counts of values in each column of Claims data\n",
    "print df_claims.columns\n",
    "columns_for_analysis = ['Year', 'Specialty', 'PlaceSvc', 'LengthOfStay', 'DSFS', \n",
    "                        'PrimaryConditionGroup', 'CharlsonIndex', 'ProcedureGroup', 'SupLOS']\n",
    "for column in columns_for_analysis:\n",
    "    df_claims.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:  EDA on each DataFrame above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C  Transform and Load <a name=\"3.C\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We use **Spark SQL Pipelines** to transform the data based on our findings in EDA-0.  Mostly, we are converting strings to numeric values.  We follow these steps:  \n",
    "\n",
    "1.  Create the SQL statements to transform the data.  \n",
    "2.  Create a SQL transformer for each statement.  \n",
    "3.  Define a pipeline that runs all the SQL statements in order.  \n",
    "4.  Fit/define a pipeline model.  \n",
    "5.  Run the pipeline model to transform the data.  \n",
    "\n",
    "The SQL statements used in this section are based on this [blog](http://anotherdataminingblog.blogspot.com/2011/10/code-for-respectable-hhp-model.html).\n",
    "\n",
    "**NOTE: The SQLTransformer in pyspark.ml cannot execute all SQL commands.  This is because DataFrames are based on RDDs, and RDDs are immutable data structures.  Updating elements in place is not possible.  For example, instead of modifying an existing column with \"UPDATE ... SET\" as in traditional SQL, we must create a new column.  Refer to this [StackOverflow entry](http://stackoverflow.com/questions/29109916/updating-a-dataframe-column-in-spark).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.C.a  Target Variables <a name=\"3.C.a\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "**NOTE: When we build our models below using pipelines, Spark expects that our target variable will have the name 'label'.  We creat 'label' here in the transformation step as LN(DaysInHospital + 1.0).  This is in line with the log root-mean-squared-error (log RMSE) loss function used by HHP to measure model performance.  See [here](https://www.heritagehealthprize.com/c/hhp/details/evaluation) for the original HHP explanation of their evaluation metric and [here](#5.B) in this notebook for a summary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_target_Y2_transformed has 76038 rows:\n",
      "+--------+---------------+--------------+-----+----+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|label|Year|\n",
      "+--------+---------------+--------------+-----+----+\n",
      "|24027423|              0|           0.0|  0.0|  Y1|\n",
      "|98324177|              0|           0.0|  0.0|  Y1|\n",
      "+--------+---------------+--------------+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "df_target_Y3_transformed has 71435 rows:\n",
      "+--------+---------------+--------------+-----+----+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|label|Year|\n",
      "+--------+---------------+--------------+-----+----+\n",
      "|90963501|              0|           0.0|  0.0|  Y2|\n",
      "|85160905|              0|           0.0|  0.0|  Y2|\n",
      "+--------+---------------+--------------+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "df_target_Y4_transformed has 70942 rows:\n",
      "+--------+---------------+--------------+-----+----+\n",
      "|MemberID|ClaimsTruncated|DaysInHospital|label|Year|\n",
      "+--------+---------------+--------------+-----+----+\n",
      "|20820036|              0|           NaN| null|  Y3|\n",
      "|14625274|              1|           NaN| null|  Y3|\n",
      "+--------+---------------+--------------+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, ClaimsTruncated: string, DaysInHospital: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Convert strings to numbers and add Year\n",
    "sql_0001_text = \"SELECT MemberID, \\\n",
    "                 CAST(ClaimsTruncated AS INT) AS ClaimsTruncated, \\\n",
    "                 CAST(DaysInHospital AS DOUBLE) AS DaysInHospital, \\\n",
    "                 LN(DaysInHospital + 1.0) AS label, \\\n",
    "                 'Y1' AS Year \\\n",
    "                 FROM __THIS__\"\n",
    "sql_0002_text = \"SELECT MemberID, \\\n",
    "                 CAST(ClaimsTruncated AS INT) AS ClaimsTruncated, \\\n",
    "                 CAST(DaysInHospital AS DOUBLE) AS DaysInHospital, \\\n",
    "                 LN(DaysInHospital + 1.0) AS label, \\\n",
    "                 'Y2' AS Year \\\n",
    "                 FROM __THIS__\"\n",
    "sql_0003_text = \"SELECT MemberID, \\\n",
    "                 CAST(ClaimsTruncated AS INT) AS ClaimsTruncated, \\\n",
    "                 CAST(DaysInHospital AS DOUBLE) AS DaysInHospital, \\\n",
    "                 LN(DaysInHospital + 1.0) AS label, \\\n",
    "                 'Y3' AS Year \\\n",
    "                 FROM __THIS__\"\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "sql_0002 = SQLTransformer(statement=(sql_0002_text))\n",
    "sql_0003 = SQLTransformer(statement=(sql_0003_text))\n",
    "\n",
    "# Step 3: Define the pipelines\n",
    "transform_target_Y2_pipeline = Pipeline(stages=[sql_0001])\n",
    "transform_target_Y3_pipeline = Pipeline(stages=[sql_0002])\n",
    "transform_target_Y4_pipeline = Pipeline(stages=[sql_0003])\n",
    "\n",
    "# Step 4: Fit the pipeline models\n",
    "transform_target_Y2_pipeline_model = transform_target_Y2_pipeline.fit(df_target_Y2)\n",
    "transform_target_Y3_pipeline_model = transform_target_Y3_pipeline.fit(df_target_Y3)\n",
    "transform_target_Y4_pipeline_model = transform_target_Y4_pipeline.fit(df_target_Y4)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_target_Y2_transformed = transform_target_Y2_pipeline_model.transform(df_target_Y2).cache()\n",
    "df_target_Y3_transformed = transform_target_Y3_pipeline_model.transform(df_target_Y3).cache()\n",
    "df_target_Y4_transformed = transform_target_Y4_pipeline_model.transform(df_target_Y4).cache()\n",
    "\n",
    "# Check the transformed data\n",
    "print 'df_target_Y2_transformed has %d rows:' %(df_target_Y2_transformed.count())\n",
    "print df_target_Y2_transformed.show(2)\n",
    "print 'df_target_Y3_transformed has %d rows:' %(df_target_Y3_transformed.count())\n",
    "print df_target_Y3_transformed.show(2)\n",
    "print 'df_target_Y4_transformed has %d rows:' %(df_target_Y4_transformed.count())\n",
    "print df_target_Y4_transformed.show(2)\n",
    "\n",
    "# Free up memory\n",
    "df_target_Y2.unpersist()\n",
    "df_target_Y3.unpersist()\n",
    "df_target_Y4.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C.b  Claims Data <a name=\"3.C.b\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_claims_transformed has 2668990 rows:\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+---------+-----+--------------+-------------+\n",
      "|MemberID|ProviderID|Vendor|  PCP|Year|Specialty|PlaceSvc|PayDelay|LengthOfStay|       DSFS|PrimaryConditionGroup|CharlsonIndex|ProcedureGroup|SupLOS|PayDelayI|DSFSI|CharlsonIndexI|LengthOfStayI|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+---------+-----+--------------+-------------+\n",
      "|42286978|   8013252|172193|37796|  Y1|  Surgery|  Office|      28|         NaN|8- 9 months|              NEUMENT|            0|           MED|     0|       28|    9|             0|         null|\n",
      "|97903248|   3316066|726296| 5300|  Y3| Internal|  Office|      50|         NaN|7- 8 months|              NEUMENT|          1-2|            EM|     0|       50|    8|             2|         null|\n",
      "+--------+----------+------+-----+----+---------+--------+--------+------------+-----------+---------------------+-------------+--------------+------+---------+-----+--------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, ProviderID: string, Vendor: string, PCP: string, Year: string, Specialty: string, PlaceSvc: string, PayDelay: string, LengthOfStay: string, DSFS: string, PrimaryConditionGroup: string, CharlsonIndex: string, ProcedureGroup: string, SupLOS: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Convert strings to numbers\n",
    "sql_0001_text = \"SELECT *, \\\n",
    "                 CASE WHEN PayDelay = '162+' THEN 163 ELSE (CAST(PayDelay AS INT)) END AS PayDelayI \\\n",
    "                 FROM __THIS__\"\n",
    "sql_0002_text = \"SELECT *, \\\n",
    "                 CASE \\\n",
    "                 WHEN DSFS = '0- 1 month' THEN 1 \\\n",
    "                 WHEN DSFS = '1- 2 months' THEN 2 \\\n",
    "                 WHEN DSFS = '2- 3 months' THEN 3 \\\n",
    "                 WHEN DSFS = '3- 4 months' THEN 4 \\\n",
    "                 WHEN DSFS = '4- 5 months' THEN 5 \\\n",
    "                 WHEN DSFS = '5- 6 months' THEN 6 \\\n",
    "                 WHEN DSFS = '6- 7 months' THEN 7 \\\n",
    "                 WHEN DSFS = '7- 8 months' THEN 8 \\\n",
    "                 WHEN DSFS = '8- 9 months' THEN 9 \\\n",
    "                 WHEN DSFS = '9-10 months' THEN 10 \\\n",
    "                 WHEN DSFS = '10-11 months' THEN 11 \\\n",
    "                 WHEN DSFS = '11-12 months' THEN 12 \\\n",
    "                 WHEN DSFS IS NULL THEN NULL \\\n",
    "                 END \\\n",
    "                 AS DSFSI \\\n",
    "                 FROM __THIS__\"\n",
    "sql_0003_text = \"SELECT *, \\\n",
    "                 CASE \\\n",
    "                 WHEN CharlsonIndex = '0' THEN 0 \\\n",
    "                 WHEN CharlsonIndex = '1-2' THEN 2 \\\n",
    "                 WHEN CharlsonIndex = '3-4' THEN 4 \\\n",
    "                 WHEN CharlsonIndex = '5+' THEN 6 \\\n",
    "                 END \\\n",
    "                 AS CharlsonIndexI \\\n",
    "                 FROM __THIS__\"\n",
    "sql_0004_text = \"SELECT *, \\\n",
    "                 CASE \\\n",
    "                 WHEN LengthOfStay = '1 day' THEN 1 \\\n",
    "                 WHEN LengthOfStay = '2 days' THEN 2 \\\n",
    "                 WHEN LengthOfStay = '3 days' THEN 3 \\\n",
    "                 WHEN LengthOfStay = '4 days' THEN 4 \\\n",
    "                 WHEN LengthOfStay = '5 days' THEN 5 \\\n",
    "                 WHEN LengthOfStay = '6 days' THEN 6 \\\n",
    "                 WHEN LengthOfStay = '1- 2 weeks' THEN 11 \\\n",
    "                 WHEN LengthOfStay = '2- 4 weeks' THEN 21 \\\n",
    "                 WHEN LengthOfStay = '4- 8 weeks' THEN 42 \\\n",
    "                 WHEN LengthOfStay = '26+ weeks' THEN 180 \\\n",
    "                 WHEN LengthOfStay IS NULL THEN null \\\n",
    "                 END \\\n",
    "                 AS LengthOfStayI \\\n",
    "                 FROM __THIS__\"\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "sql_0002 = SQLTransformer(statement=(sql_0002_text))\n",
    "sql_0003 = SQLTransformer(statement=(sql_0003_text))\n",
    "sql_0004 = SQLTransformer(statement=(sql_0004_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "transform_claims_pipeline = Pipeline(stages=[sql_0001, sql_0002, sql_0003, sql_0004])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "transform_claims_pipeline_model = transform_claims_pipeline.fit(df_claims)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_claims_transformed = transform_claims_pipeline_model.transform(df_claims).cache()\n",
    "\n",
    "# Check the transformed data\n",
    "print 'df_claims_transformed has %d rows:' %(df_claims_transformed.count())\n",
    "print df_claims_transformed.show(2)\n",
    "\n",
    "# Free up memory\n",
    "df_claims.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C.c  Drug Data <a name=\"3.C.c\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_drug_count_transformed has 818241 rows:\n",
      "+--------+----+-----------+---------+----------+\n",
      "|MemberID|Year|       DSFS|DrugCount|DrugCountI|\n",
      "+--------+----+-----------+---------+----------+\n",
      "|48925661|  Y2|9-10 months|       7+|         7|\n",
      "|90764620|  Y3|8- 9 months|        3|         3|\n",
      "+--------+----+-----------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, Year: string, DSFS: string, DrugCount: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Convert strings to numbers\n",
    "sql_0001_text = \"SELECT *, \\\n",
    "                 CASE WHEN DrugCount = '7+' THEN 7 ELSE (CAST(DrugCount AS INT)) END AS DrugCountI \\\n",
    "                 FROM __THIS__\"\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "transform_drugs_pipeline = Pipeline(stages=[sql_0001])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "transform_drugs_pipeline_model = transform_drugs_pipeline.fit(df_drug_count)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_drug_count_transformed = transform_drugs_pipeline_model.transform(df_drug_count).cache()\n",
    "\n",
    "# Check the transformed data\n",
    "print 'df_drug_count_transformed has %d rows:' %(df_drug_count_transformed.count())\n",
    "print df_drug_count_transformed.show(2)\n",
    "\n",
    "# Free up memory\n",
    "df_drug_count.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C.d  Lab Data <a name=\"3.C.d\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_lab_count_transformed has 361484 rows:\n",
      "+--------+----+-----------+--------+---------+\n",
      "|MemberID|Year|       DSFS|LabCount|LabCountI|\n",
      "+--------+----+-----------+--------+---------+\n",
      "|69258001|  Y3|2- 3 months|       1|        1|\n",
      "|10143167|  Y1| 0- 1 month|       2|        2|\n",
      "+--------+----+-----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, Year: string, DSFS: string, LabCount: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Convert strings to numbers\n",
    "sql_0001_text = \"SELECT *, \\\n",
    "                 CASE WHEN LabCount = '10+' THEN 10 ELSE (CAST(LabCount AS INT)) END AS LabCountI \\\n",
    "                 FROM __THIS__\"\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "transform_labs_pipeline = Pipeline(stages=[sql_0001])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "transform_labs_pipeline_model = transform_labs_pipeline.fit(df_lab_count)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_lab_count_transformed = transform_labs_pipeline_model.transform(df_lab_count).cache()\n",
    "\n",
    "# Check the transformed data\n",
    "print 'df_lab_count_transformed has %d rows:' %(df_lab_count_transformed.count())\n",
    "print df_lab_count_transformed.show(2)\n",
    "\n",
    "# Free up memory\n",
    "df_lab_count.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C.e  Members Data <a name=\"3.C.e\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are no transformations needed on the members data.  It will be one hot encoded in Section 4.B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.D  EDA-1 <a name=\"3.D\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "It is a best practice to do some EDA on each table to get a feel for the categorical variables, to see the distributions of the numeric variables, etc.  See [here](http://itl.nist.gov/div898/handbook/eda/eda.htm) for a comprehensive list of EDA techniques and the graphs [here](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/kmbgrkhh73931lo/Titanic-EDA-LogisticRegression.ipynb) for inspiration from the [Titanic Kaggle competition](https://www.kaggle.com/c/titanic).  Also see [Section 7: Next Steps](#7.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO: EDA on df_target_Y2_transformed, df_target_Y3_transformed, df_target_Y4_transformed, df_claims_transformed,\n",
    "#        df_drug_count_transformed, df_lab_count_transformed, df_members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Feature Engineering <a name=\"4\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We have three steps to our feature engineering for HHP:\n",
    "\n",
    "4.A  [Aggregation](#4.A)  \n",
    "4.B  [One Hot Encoding](#4.B)  \n",
    "4.C  [Create Final DataFrame for Modeling](#4.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A  Aggregation <a name=\"4.A\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In this section, we create one line of data per MemberID per year.\n",
    "\n",
    "In the Claims, DrugCount, and LabCount data, there may be multiple lines of data for each member in a particular year.  We must aggregate the variables for each member in each year.\n",
    "\n",
    "For integer variables, we aggregate them by simple statistics, as in the example for the DrugCount below.\n",
    "\n",
    "<img src=\"https://dl.dropbox.com/s/zug5jr2pntcygbg/Aggregate.png\" width=\"800\" height=\"800\"/>\n",
    "\n",
    "For categorical variables, we [one hot encode](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science) them (i.e. create [dummy/indicator variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)).  We create a new variable for each value of each categorical variable and put a 1 if the category of that variable occurs in that record and a 0 otherwise.  We then sum up how many times that category occurs for each patient each year, as in the example for the categorical variable Place of Service (PlaceSvc) below.\n",
    "\n",
    "<img src=\"https://dl.dropbox.com/s/j0jvegfbj84li59/OHE.png\" width=\"900\" height=\"900\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.A.a  Claims Data <a name=\"4.A.a\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In aggregating the Claims data, we count the total number of claims for a patient within a given year, we count how many distinct values each categorical variable takes on, we aggregate the numeric variables (min, max, average, range, etc.), and we one hot encode the  categorical variables.  We also indicate whether the LengthOfStay variable was suppressed (supLOS).\n",
    "\n",
    "**NOTE: This takes a while to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2016-08-04 19:13:11\n",
      "df_claims_aggregated has 218415 rows:\n",
      "End time: 2016-08-04 19:21:12\n",
      "Elapsed Time: 8.01 minutes\n",
      "Elapsed CPU Time: 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "start_time_agg = time.time()\n",
    "cpu_start_time_agg = time.clock()\n",
    "print 'Start time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Step 1:  Create the SQL statements\n",
    "# Aggregate variables across Year and MemberID\n",
    "sql_0001_text = \"SELECT MemberID AS MemberID_c, Year AS Year_c, \\\n",
    "                 \\\n",
    "                 COUNT(*) AS num_Claims, \\\n",
    "                 COUNT(DISTINCT ProviderID) AS num_Providers, \\\n",
    "                 COUNT(DISTINCT Vendor) AS num_Vendors, \\\n",
    "                 COUNT(DISTINCT PCP) AS num_PCPs, \\\n",
    "                 COUNT(DISTINCT PlaceSvc) AS num_PlaceSvcs, \\\n",
    "                 COUNT(DISTINCT Specialty) AS num_Specialities, \\\n",
    "                 COUNT(DISTINCT PrimaryConditionGroup) AS num_PrimaryConditionGroups, \\\n",
    "                 COUNT(DISTINCT ProcedureGroup) AS num_ProcedureGroups, \\\n",
    "                 \\\n",
    "                 MAX(PayDelayI) AS max_PayDelay, \\\n",
    "                 MIN(PayDelayI) AS min_PayDelay, \\\n",
    "                 AVG(PayDelayI) AS avg_PayDelay, \\\n",
    "                 \\\n",
    "                 MAX(LengthOfStayI) AS max_LOS, \\\n",
    "                 MIN(LengthOfStayI) AS min_LOS, \\\n",
    "                 AVG(LengthOfStayI) AS avg_LOS, \\\n",
    "                 \\\n",
    "                 SUM(CASE WHEN LengthOfStay IS NULL AND SupLOS = 0 THEN 1 ELSE 0 END) AS LOS_TOT_UNKNOWN, \\\n",
    "                 SUM(CASE WHEN LengthOfStay IS NULL AND SupLOS = 1 THEN 1 ELSE 0 END) AS LOS_TOT_SUPRESSED, \\\n",
    "                 SUM(CASE WHEN LengthOfStay IS NOT NULL THEN 1 ELSE 0 END) AS LOS_TOT_KNOWN, \\\n",
    "                 \\\n",
    "                 MAX(DSFSI) AS max_dsfs, \\\n",
    "                 MIN(DSFSI) AS min_dsfs, \\\n",
    "                 MAX(DSFSI) - MIN(DSFSI) AS range_dsfs, \\\n",
    "                 AVG(DSFSI) AS avg_dsfs, \\\n",
    "                 \\\n",
    "                 MAX(CharlsonIndexI) AS max_CharlsonIndexI, \\\n",
    "                 MIN(CharlsonIndexI) AS min_CharlsonIndexI, \\\n",
    "                 AVG(CharlsonIndexI) AS avg_CharlsonIndexI, \\\n",
    "                 MAX(CharlsonIndexI) - MIN(CharlsonIndexI) AS range_CharlsonIndexI, \\\n",
    "                 \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'MSC2a3' THEN 1 ELSE 0 END) AS pcg1, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'METAB3' THEN 1 ELSE 0 END) AS pcg2, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'ARTHSPIN' THEN 1 ELSE 0 END) AS pcg3, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'NEUMENT' THEN 1 ELSE 0 END) AS pcg4, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'RESPR4' THEN 1 ELSE 0 END) AS pcg5, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'MISCHRT' THEN 1 ELSE 0 END) AS pcg6, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'SKNAUT' THEN 1 ELSE 0 END) AS pcg7, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'GIBLEED' THEN 1 ELSE 0 END) AS pcg8, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'INFEC4' THEN 1 ELSE 0 END) AS pcg9, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'TRAUMA' THEN 1 ELSE 0 END) AS pcg10, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'HEART2' THEN 1 ELSE 0 END) AS pcg11, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'RENAL3' THEN 1 ELSE 0 END) AS pcg12, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'ROAMI' THEN 1 ELSE 0 END) AS pcg13, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'MISCL5' THEN 1 ELSE 0 END) AS pcg14, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'ODaBNCA' THEN 1 ELSE 0 END) AS pcg15, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'UTI' THEN 1 ELSE 0 END) AS pcg16, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'COPD' THEN 1 ELSE 0 END) AS pcg17, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'GYNEC1' THEN 1 ELSE 0 END) AS pcg18, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'CANCRB' THEN 1 ELSE 0 END) AS pcg19, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'FXDISLC' THEN 1 ELSE 0 END) AS pcg20, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'AMI' THEN 1 ELSE 0 END) AS pcg21, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'PRGNCY' THEN 1 ELSE 0 END) AS pcg22, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'HEMTOL' THEN 1 ELSE 0 END) AS pcg23, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'HEART4' THEN 1 ELSE 0 END) AS pcg24, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'SEIZURE' THEN 1 ELSE 0 END) AS pcg25, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'APPCHOL' THEN 1 ELSE 0 END) AS pcg26, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'CHF' THEN 1 ELSE 0 END) AS pcg27, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'GYNECA' THEN 1 ELSE 0 END) AS pcg28, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup IS NULL THEN 1 ELSE 0 END) AS pcg29, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'PNEUM' THEN 1 ELSE 0 END) AS pcg30, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'RENAL2' THEN 1 ELSE 0 END) AS pcg31, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'GIOBSENT' THEN 1 ELSE 0 END) AS pcg32, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'STROKE' THEN 1 ELSE 0 END) AS pcg33, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'CANCRA' THEN 1 ELSE 0 END) AS pcg34, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'FLaELEC' THEN 1 ELSE 0 END) AS pcg35, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'MISCL1' THEN 1 ELSE 0 END) AS pcg36, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'HIPFX' THEN 1 ELSE 0 END) AS pcg37, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'METAB1' THEN 1 ELSE 0 END) AS pcg38, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'PERVALV' THEN 1 ELSE 0 END) AS pcg39, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'LIVERDZ' THEN 1 ELSE 0 END) AS pcg40, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'CATAST' THEN 1 ELSE 0 END) AS pcg41, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'CANCRM' THEN 1 ELSE 0 END) AS pcg42, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'PERINTL' THEN 1 ELSE 0 END) AS pcg43, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'PNCRDZ' THEN 1 ELSE 0 END) AS pcg44, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'RENAL1' THEN 1 ELSE 0 END) AS pcg45, \\\n",
    "                 SUM(CASE WHEN PrimaryConditionGroup = 'SEPSIS' THEN 1 ELSE 0 END) AS pcg46, \\\n",
    "                 \\\n",
    "                 SUM(CASE WHEN Specialty = 'Internal' THEN 1 ELSE 0 END) AS sp1, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Laboratory' THEN 1 ELSE 0 END) AS sp2, \\\n",
    "                 SUM(CASE WHEN Specialty = 'General Practice' THEN 1 ELSE 0 END) AS sp3, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Surgery' THEN 1 ELSE 0 END) AS sp4, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Diagnostic Imaging' THEN 1 ELSE 0 END) AS sp5, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Emergency' THEN 1 ELSE 0 END) AS sp6, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Other' THEN 1 ELSE 0 END) AS sp7, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Pediatrics' THEN 1 ELSE 0 END) AS sp8, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Rehabilitation' THEN 1 ELSE 0 END) AS sp9, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Obstetrics and Gynecology' THEN 1 ELSE 0 END) AS sp10, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Anesthesiology' THEN 1 ELSE 0 END) AS sp11, \\\n",
    "                 SUM(CASE WHEN Specialty = 'Pathology' THEN 1 ELSE 0 END) AS sp12, \\\n",
    "                 SUM(CASE WHEN Specialty IS NULL THEN 1 ELSE 0 END) AS sp13, \\\n",
    "                 \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'EM' THEN 1 ELSE 0 END ) AS pg1, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'PL' THEN 1 ELSE 0 END ) AS pg2, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'MED' THEN 1 ELSE 0 END ) AS pg3, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SCS' THEN 1 ELSE 0 END ) AS pg4, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'RAD' THEN 1 ELSE 0 END ) AS pg5, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SDS' THEN 1 ELSE 0 END ) AS pg6, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SIS' THEN 1 ELSE 0 END ) AS pg7, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SMS' THEN 1 ELSE 0 END ) AS pg8, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'ANES' THEN 1 ELSE 0 END ) AS pg9, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SGS' THEN 1 ELSE 0 END ) AS pg10, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SEOA' THEN 1 ELSE 0 END ) AS pg11, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SRS' THEN 1 ELSE 0 END ) AS pg12, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SNS' THEN 1 ELSE 0 END ) AS pg13, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SAS' THEN 1 ELSE 0 END ) AS pg14, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SUS' THEN 1 ELSE 0 END ) AS pg15, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup IS NULL THEN 1 ELSE 0 END ) AS pg16, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SMCD' THEN 1 ELSE 0 END ) AS pg17, \\\n",
    "                 SUM(CASE WHEN ProcedureGroup = 'SO' THEN 1 ELSE 0 END ) AS pg18, \\\n",
    "                 \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Office' THEN 1 ELSE 0 END) AS ps1, \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Independent Lab' THEN 1 ELSE 0 END) AS ps2, \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Urgent Care' THEN 1 ELSE 0 END) AS ps3, \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Outpatient Hospital' THEN 1 ELSE 0 END) AS ps4, \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Inpatient Hospital' THEN 1 ELSE 0 END) AS ps5, \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Ambulance' THEN 1 ELSE 0 END) AS ps6, \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Other' THEN 1 ELSE 0 END) AS ps7, \\\n",
    "                 SUM(CASE WHEN PlaceSvc = 'Home' THEN 1 ELSE 0 END) AS ps8, \\\n",
    "                 SUM(CASE WHEN PlaceSvc IS NULL THEN 1 ELSE 0 END) AS ps9 \\\n",
    "                 \\\n",
    "                 FROM __THIS__ \\\n",
    "                 GROUP BY Year, MemberID\"\n",
    "sql_0002_text = \"SELECT *, \\\n",
    "                 \\\n",
    "                 CASE WHEN max_LOS IS NULL THEN 0 ELSE max_LOS END AS new_max_LOS, \\\n",
    "                 CASE WHEN min_LOS IS NULL THEN 0 ELSE min_LOS END AS new_min_LOS, \\\n",
    "                 CASE WHEN avg_LOS IS NULL THEN 0 ELSE avg_LOS END AS new_avg_LOS, \\\n",
    "                 \\\n",
    "                 CASE WHEN max_dsfs IS NULL THEN 0 ELSE max_dsfs END AS new_max_dsfs, \\\n",
    "                 CASE WHEN min_dsfs IS NULL THEN 0 ELSE min_dsfs END AS new_min_dsfs, \\\n",
    "                 CASE WHEN range_dsfs IS NULL THEN -1 ELSE range_dsfs END AS new_range_dsfs, \\\n",
    "                 CASE WHEN avg_dsfs IS NULL THEN 0 ELSE avg_dsfs END AS new_avg_dsfs, \\\n",
    "                 \\\n",
    "                 CASE WHEN range_CharlsonIndexI IS NULL THEN -1 ELSE range_CharlsonIndexI END AS new_range_CharlsonIndexI \\\n",
    "                 \\\n",
    "                 FROM __THIS__\"\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "sql_0002 = SQLTransformer(statement=(sql_0002_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "aggregate_claims_pipeline = Pipeline(stages=[sql_0001, sql_0002])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "aggregate_claims_pipeline_model = aggregate_claims_pipeline.fit(df_claims_transformed)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_claims_aggregated = aggregate_claims_pipeline_model.transform(df_claims_transformed).cache()\n",
    "\n",
    "# Check the aggregated data\n",
    "print 'df_claims_aggregated has %d rows:' %(df_claims_aggregated.count())\n",
    "# print df_claims_aggregated.printSchema()\n",
    "# print df_claims_aggregated.show(1)\n",
    "\n",
    "# Free up memory\n",
    "df_claims_transformed.unpersist()\n",
    "\n",
    "print 'End time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print 'Elapsed Time: %.2f minutes' %((time.time() - start_time_agg)/60)\n",
    "print 'Elapsed CPU Time: %.2f minutes' %((time.clock() - cpu_start_time_agg)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.A.b  Drug Data <a name=\"4.A.b\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In aggregating the Drug data, we calculate the maximum, minimum, and average of DrugCount, and we also count the number of drug records for each patient in a given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_drug_count_aggregated has 141571 rows:\n",
      "root\n",
      " |-- MemberID_dc: string (nullable = true)\n",
      " |-- Year_dc: string (nullable = true)\n",
      " |-- max_DrugCount: integer (nullable = true)\n",
      " |-- min_DrugCount: integer (nullable = true)\n",
      " |-- avg_DrugCount: decimal(17,5) (nullable = true)\n",
      " |-- months_DrugCount: long (nullable = false)\n",
      "\n",
      "None\n",
      "+-----------+-------+-------------+-------------+-------------+----------------+\n",
      "|MemberID_dc|Year_dc|max_DrugCount|min_DrugCount|avg_DrugCount|months_DrugCount|\n",
      "+-----------+-------+-------------+-------------+-------------+----------------+\n",
      "|   88457086|     Y2|            2|            1|      1.42857|               7|\n",
      "|   88916957|     Y1|            3|            1|      1.33333|               6|\n",
      "+-----------+-------+-------------+-------------+-------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, Year: string, DSFS: string, DrugCount: string, DrugCountI: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Aggregate variables across Year and MemberID\n",
    "sql_0001_text = \"SELECT MemberID AS MemberID_dc, Year AS Year_dc, \\\n",
    "                 \\\n",
    "                 MAX(DrugCountI) AS max_DrugCount, \\\n",
    "                 MIN(DrugCountI) AS min_DrugCount, \\\n",
    "                 AVG(DrugCountI * 1.0) AS avg_DrugCount, \\\n",
    "                 COUNT(*) AS months_DrugCount \\\n",
    "                 \\\n",
    "                 FROM __THIS__ \\\n",
    "                 GROUP BY Year, MemberID\"\n",
    "\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "aggregate_drugs_pipeline = Pipeline(stages=[sql_0001])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "aggregate_drugs_pipeline_model = aggregate_drugs_pipeline.fit(df_drug_count_transformed)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_drug_count_aggregated = aggregate_drugs_pipeline_model.transform(df_drug_count_transformed).cache()\n",
    "\n",
    "# Check the aggregated data\n",
    "print 'df_drug_count_aggregated has %d rows:' %(df_drug_count_aggregated.count())\n",
    "print df_drug_count_aggregated.printSchema()\n",
    "print df_drug_count_aggregated.show(2)\n",
    "\n",
    "# Free up memory\n",
    "df_drug_count_transformed.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.A.c  Lab Data <a name=\"4.A.c\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In aggregating the Lab data, we calculate the maximum, minimum, and average of LabCount, and we also count the number of lab records for each patient in a given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_lab_count_aggregated has 154934 rows:\n",
      "root\n",
      " |-- MemberID_lc: string (nullable = true)\n",
      " |-- Year_lc: string (nullable = true)\n",
      " |-- max_LabCount: integer (nullable = true)\n",
      " |-- min_LabCount: integer (nullable = true)\n",
      " |-- avg_LabCount: decimal(17,5) (nullable = true)\n",
      " |-- months_LabCount: long (nullable = false)\n",
      "\n",
      "None\n",
      "+-----------+-------+------------+------------+------------+---------------+\n",
      "|MemberID_lc|Year_lc|max_LabCount|min_LabCount|avg_LabCount|months_LabCount|\n",
      "+-----------+-------+------------+------------+------------+---------------+\n",
      "|    8236453|     Y3|          10|           3|     6.14286|              7|\n",
      "|   51031139|     Y1|          10|           5|     8.33333|              3|\n",
      "+-----------+-------+------------+------------+------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, Year: string, DSFS: string, LabCount: string, LabCountI: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Aggregate variables across Year and MemberID\n",
    "sql_0001_text = \"SELECT MemberID AS MemberID_lc, Year AS Year_lc, \\\n",
    "                 \\\n",
    "                 MAX(LabCountI) AS max_LabCount, \\\n",
    "                 MIN(LabCountI) AS min_LabCount, \\\n",
    "                 AVG(LabCountI * 1.0) AS avg_LabCount, \\\n",
    "                 COUNT(*) AS months_LabCount \\\n",
    "                 \\\n",
    "                 FROM __THIS__ \\\n",
    "                 GROUP BY Year, MemberID\"\n",
    "\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "aggregate_labs_pipeline = Pipeline(stages=[sql_0001])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "aggregate_labs_pipeline_model = aggregate_labs_pipeline.fit(df_lab_count_transformed)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_lab_count_aggregated = aggregate_labs_pipeline_model.transform(df_lab_count_transformed).cache()\n",
    "\n",
    "# Check the aggregated data\n",
    "print 'df_lab_count_aggregated has %d rows:' %(df_lab_count_aggregated.count())\n",
    "print df_lab_count_aggregated.printSchema()\n",
    "print df_lab_count_aggregated.show(2)\n",
    "\n",
    "# Free up memory\n",
    "df_lab_count_transformed.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B One Hot Encoding: Members Data <a name=\"4.B\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In this section, we one hot encode the age and sex variables in the Members Data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_members_transformed has 113000 rows:\n",
      "root\n",
      " |-- MemberID: string (nullable = true)\n",
      " |-- AgeAtFirstClaim: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- age_05: integer (nullable = false)\n",
      " |-- age_15: integer (nullable = false)\n",
      " |-- age_25: integer (nullable = false)\n",
      " |-- age_35: integer (nullable = false)\n",
      " |-- age_45: integer (nullable = false)\n",
      " |-- age_55: integer (nullable = false)\n",
      " |-- age_65: integer (nullable = false)\n",
      " |-- age_75: integer (nullable = false)\n",
      " |-- age_85: integer (nullable = false)\n",
      " |-- age_MISS: integer (nullable = false)\n",
      " |-- sex_M: integer (nullable = false)\n",
      " |-- sex_F: integer (nullable = false)\n",
      "\n",
      "None\n",
      "+--------+---------------+---+------+------+------+------+------+------+------+------+------+--------+-----+-----+\n",
      "|MemberID|AgeAtFirstClaim|Sex|age_05|age_15|age_25|age_35|age_45|age_55|age_65|age_75|age_85|age_MISS|sex_M|sex_F|\n",
      "+--------+---------------+---+------+------+------+------+------+------+------+------+------+--------+-----+-----+\n",
      "|14723353|          70-79|  M|     0|     0|     0|     0|     0|     0|     0|     1|     0|       0|    1|    0|\n",
      "|75706636|          70-79|  M|     0|     0|     0|     0|     0|     0|     0|     1|     0|       0|    1|    0|\n",
      "+--------+---------------+---+------+------+------+------+------+------+------+------+------+--------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, AgeAtFirstClaim: string, Sex: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Create indicator variables for age and sex\n",
    "sql_0001_text = \"SELECT *, \\\n",
    "                 CASE WHEN AgeAtFirstClaim  = '0-9' THEN 1 ELSE 0 END AS age_05,  \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '10-19' THEN 1 ELSE 0 END AS age_15, \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '20-29' THEN 1 ELSE 0 END AS age_25, \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '30-39' THEN 1 ELSE 0 END AS age_35, \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '40-49' THEN 1 ELSE 0 END AS age_45, \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '50-59' THEN 1 ELSE 0 END AS age_55, \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '60-69' THEN 1 ELSE 0 END AS age_65, \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '70-79' THEN 1 ELSE 0 END AS age_75, \\\n",
    "                 CASE WHEN AgeAtFirstClaim = '80+' THEN 1 ELSE 0 END AS age_85,   \\\n",
    "                 CASE WHEN AgeAtFirstClaim IS NULL THEN 1 ELSE 0 END AS age_MISS  \\\n",
    "                 FROM __THIS__\"\n",
    "sql_0002_text = \"SELECT *, \\\n",
    "                 CASE WHEN Sex = 'M' THEN 1 ELSE 0 END AS sex_M,  \\\n",
    "                 CASE WHEN Sex = 'F' THEN 1 ELSE 0 END AS sex_F   \\\n",
    "                 FROM __THIS__\"\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "sql_0002 = SQLTransformer(statement=(sql_0002_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "transform_members_pipeline = Pipeline(stages=[sql_0001, sql_0002])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "transform_members_pipeline_model = transform_members_pipeline.fit(df_members)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_members_transformed = transform_members_pipeline_model.transform(df_members).cache()\n",
    "\n",
    "# Check the transformed data\n",
    "print 'df_members_transformed has %d rows:' %(df_members_transformed.count())\n",
    "print df_members_transformed.printSchema()\n",
    "print df_members_transformed.show(2)\n",
    "\n",
    "# Free up memory\n",
    "df_members.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.C  Create Final DataFrame For Modeling <a name=\"4.C\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We turn from using SQL to [operating directly on the DataFrames](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) in order to join all of our data and create the final DataFrame that will be used for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.C.a  Merge Data <a name=\"4.C.a\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We merge all of our DataFrames together in the following steps:\n",
    "\n",
    "1.  Union Target DataFrames Across Years  \n",
    "2.  Join/Merge in Members Data  \n",
    "3.  Join/Merge in Aggregated Claims Data  \n",
    "4.  Join/Merge in Aggregated Drug Data  \n",
    "5.  Join/Merge in Aggregated Lab Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples is 147473.\n"
     ]
    }
   ],
   "source": [
    "# 1. Union Target DataFrames Across Years\n",
    "df_merged = df_target_Y2_transformed.unionAll(df_target_Y3_transformed).cache()\n",
    "print 'Number of training samples is %d.' %(df_merged.count())\n",
    "# print 'Schema for df_merged DataFrame:'\n",
    "# print df_merged.printSchema()\n",
    "# print df_merged.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing Members Data: 0\n"
     ]
    }
   ],
   "source": [
    "# 2. Join/Merge in Members Data\n",
    "df_merged = df_merged.join(df_members_transformed, 'MemberID', 'left_outer').cache()\n",
    "# print df_merged.printSchema()\n",
    "# print df_merged.show(1)\n",
    "print 'Number of rows with missing Members Data: %d' %(df_merged.where('AgeAtFirstClaim IS NULL').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing Claims Data: 0\n"
     ]
    }
   ],
   "source": [
    "# 3. Join/Merge in Aggregated Claims Data\n",
    "condition = [df_merged.MemberID == df_claims_aggregated.MemberID_c, df_merged.Year == df_claims_aggregated.Year_c]\n",
    "df_merged = df_merged.join(df_claims_aggregated, condition, 'left_outer') \\\n",
    "                     .drop('MemberID_c') \\\n",
    "                     .drop('Year_c') \\\n",
    "                     .cache()\n",
    "# print df_merged.printSchema()\n",
    "# print df_merged.show(1)\n",
    "print 'Number of rows with missing Claims Data: %d' %(df_merged.where('num_Claims IS NULL').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing Drug Data: 51423\n"
     ]
    }
   ],
   "source": [
    "# 4. Join/Merge in Aggregated Drug Data\n",
    "condition = [df_merged.MemberID == df_drug_count_aggregated.MemberID_dc, df_merged.Year == df_drug_count_aggregated.Year_dc]\n",
    "df_merged = df_merged.join(df_drug_count_aggregated, condition, 'left_outer') \\\n",
    "                     .drop('MemberID_dc') \\\n",
    "                     .drop('Year_dc') \\\n",
    "                     .cache()\n",
    "# print df_merged.printSchema()\n",
    "# print df_merged.show(1)\n",
    "print 'Number of rows with missing Drug Data: %d' %(df_merged.where('max_DrugCount IS NULL').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing Lab Data: 42978\n"
     ]
    }
   ],
   "source": [
    "# 5. Join/Merge in Aggregated Lab Data\n",
    "condition = [df_merged.MemberID == df_lab_count_aggregated.MemberID_lc, df_merged.Year == df_lab_count_aggregated.Year_lc]\n",
    "df_merged = df_merged.join(df_lab_count_aggregated, condition, 'left_outer') \\\n",
    "                     .drop('MemberID_lc') \\\n",
    "                     .drop('Year_lc') \\\n",
    "                     .cache()\n",
    "# print df_merged.printSchema()\n",
    "# print df_merged.show(1)\n",
    "print 'Number of rows with missing Lab Data: %d' %(df_merged.where('max_LabCount IS NULL').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID_lc: string, Year_lc: string, max_LabCount: int, min_LabCount: int, avg_LabCount: decimal(17,5), months_LabCount: bigint]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up memory\n",
    "df_target_Y2_transformed.unpersist()\n",
    "df_target_Y3_transformed.unpersist()\n",
    "df_members_transformed.unpersist()\n",
    "df_claims_aggregated.unpersist()\n",
    "df_drug_count_aggregated.unpersist()\n",
    "df_lab_count_aggregated.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.C.b  Handle Missing Data <a name=\"4.C.b\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We need to replace null values for those members who are missing drug and/or lab data.  We also put in indicator variables to let us know whether a member is missing this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_complete has 147473 rows:\n",
      "Double-Check number of rows with missing Drug Data: 51423\n",
      "Double-Check number of rows with missing Lab Data: 42978\n"
     ]
    }
   ],
   "source": [
    "# Step 1:  Create the SQL statements\n",
    "# Create indicator variables for missing DrugCount and LabCount data, and replace null values with 0's.\n",
    "sql_0001_text = \"SELECT *, \\\n",
    "                 CASE WHEN max_DrugCount IS NULL THEN 1 ELSE 0 END AS null_DrugCount,  \\\n",
    "                 CASE WHEN max_LabCount IS NULL THEN 1 ELSE 0 END AS null_LabCount, \\\n",
    "                 \\\n",
    "                 CASE WHEN max_DrugCount IS NULL THEN 0 ELSE max_DrugCount END AS new_max_DrugCount, \\\n",
    "                 CASE WHEN min_DrugCount IS NULL THEN 0 ELSE min_DrugCount END AS new_min_DrugCount, \\\n",
    "                 CASE WHEN avg_DrugCount IS NULL THEN 0 ELSE avg_DrugCount END AS new_avg_DrugCount, \\\n",
    "                 CASE WHEN months_DrugCount IS NULL THEN 0 ELSE months_DrugCount END AS new_months_DrugCount, \\\n",
    "                 \\\n",
    "                 CASE WHEN max_LabCount IS NULL THEN 0 ELSE max_LabCount END AS new_max_LabCount, \\\n",
    "                 CASE WHEN min_LabCount IS NULL THEN 0 ELSE min_LabCount END AS new_min_LabCount, \\\n",
    "                 CASE WHEN avg_LabCount IS NULL THEN 0 ELSE avg_LabCount END AS new_avg_LabCount, \\\n",
    "                 CASE WHEN months_LabCount IS NULL THEN 0 ELSE months_LabCount END AS new_months_LabCount \\\n",
    "                 \\\n",
    "                 FROM __THIS__\"\n",
    "\n",
    "# Step 2: Create the SQL transformers\n",
    "sql_0001 = SQLTransformer(statement=(sql_0001_text))\n",
    "\n",
    "# Step 3: Define the pipeline\n",
    "missing_data_pipeline = Pipeline(stages=[sql_0001])\n",
    "\n",
    "# Step 4: Fit the pipeline model\n",
    "missing_data_pipeline_model = missing_data_pipeline.fit(df_merged)\n",
    "\n",
    "# Step 5: Run the pipeline model\n",
    "df_complete = missing_data_pipeline_model.transform(df_merged).cache()\n",
    "\n",
    "# Check the complete data\n",
    "print 'df_complete has %d rows:' %(df_complete.count())\n",
    "print 'Double-Check number of rows with missing Drug Data: %d' %(df_complete.where('null_DrugCount = 1').count())\n",
    "print 'Double-Check number of rows with missing Lab Data: %d' %(df_complete.where('null_LabCount = 1').count())\n",
    "# print df_complete.printSchema()\n",
    "# print df_complete.show(1)\n",
    "\n",
    "# Free up memory\n",
    "# df_merged.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.C.c  Drop Unnecessary Variables <a name=\"4.C.c\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We drop all the variables not needed for modeling and drop any rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_final has 147473 rows and 135 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[MemberID: string, ClaimsTruncated: int, DaysInHospital: double, label: double, Year: string, AgeAtFirstClaim: string, Sex: string, age_05: int, age_15: int, age_25: int, age_35: int, age_45: int, age_55: int, age_65: int, age_75: int, age_85: int, age_MISS: int, sex_M: int, sex_F: int, num_Claims: bigint, num_Providers: bigint, num_Vendors: bigint, num_PCPs: bigint, num_PlaceSvcs: bigint, num_Specialities: bigint, num_PrimaryConditionGroups: bigint, num_ProcedureGroups: bigint, max_PayDelay: int, min_PayDelay: int, avg_PayDelay: double, max_LOS: int, min_LOS: int, avg_LOS: double, LOS_TOT_UNKNOWN: bigint, LOS_TOT_SUPRESSED: bigint, LOS_TOT_KNOWN: bigint, max_dsfs: int, min_dsfs: int, range_dsfs: int, avg_dsfs: double, max_CharlsonIndexI: int, min_CharlsonIndexI: int, avg_CharlsonIndexI: double, range_CharlsonIndexI: int, pcg1: bigint, pcg2: bigint, pcg3: bigint, pcg4: bigint, pcg5: bigint, pcg6: bigint, pcg7: bigint, pcg8: bigint, pcg9: bigint, pcg10: bigint, pcg11: bigint, pcg12: bigint, pcg13: bigint, pcg14: bigint, pcg15: bigint, pcg16: bigint, pcg17: bigint, pcg18: bigint, pcg19: bigint, pcg20: bigint, pcg21: bigint, pcg22: bigint, pcg23: bigint, pcg24: bigint, pcg25: bigint, pcg26: bigint, pcg27: bigint, pcg28: bigint, pcg29: bigint, pcg30: bigint, pcg31: bigint, pcg32: bigint, pcg33: bigint, pcg34: bigint, pcg35: bigint, pcg36: bigint, pcg37: bigint, pcg38: bigint, pcg39: bigint, pcg40: bigint, pcg41: bigint, pcg42: bigint, pcg43: bigint, pcg44: bigint, pcg45: bigint, pcg46: bigint, sp1: bigint, sp2: bigint, sp3: bigint, sp4: bigint, sp5: bigint, sp6: bigint, sp7: bigint, sp8: bigint, sp9: bigint, sp10: bigint, sp11: bigint, sp12: bigint, sp13: bigint, pg1: bigint, pg2: bigint, pg3: bigint, pg4: bigint, pg5: bigint, pg6: bigint, pg7: bigint, pg8: bigint, pg9: bigint, pg10: bigint, pg11: bigint, pg12: bigint, pg13: bigint, pg14: bigint, pg15: bigint, pg16: bigint, pg17: bigint, pg18: bigint, ps1: bigint, ps2: bigint, ps3: bigint, ps4: bigint, ps5: bigint, ps6: bigint, ps7: bigint, ps8: bigint, ps9: bigint, new_max_LOS: int, new_min_LOS: int, new_avg_LOS: double, new_max_dsfs: int, new_min_dsfs: int, new_range_dsfs: int, new_avg_dsfs: double, new_range_CharlsonIndexI: int, max_DrugCount: int, min_DrugCount: int, avg_DrugCount: decimal(17,5), months_DrugCount: bigint, max_LabCount: int, min_LabCount: int, avg_LabCount: decimal(17,5), months_LabCount: bigint, null_DrugCount: int, null_LabCount: int, new_max_DrugCount: int, new_min_DrugCount: int, new_avg_DrugCount: decimal(17,5), new_months_DrugCount: bigint, new_max_LabCount: int, new_min_LabCount: int, new_avg_LabCount: decimal(17,5), new_months_LabCount: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = df_complete.drop('MemberID') \\\n",
    "                      .drop('Year') \\\n",
    "                      .drop('DaysInHospital') \\\n",
    "                      .drop('AgeAtFirstClaim') \\\n",
    "                      .drop('Sex') \\\n",
    "                      .drop('max_LOS') \\\n",
    "                      .drop('min_LOS') \\\n",
    "                      .drop('avg_LOS') \\\n",
    "                      .drop('max_dsfs') \\\n",
    "                      .drop('min_dsfs') \\\n",
    "                      .drop('range_dsfs') \\\n",
    "                      .drop('avg_dsfs') \\\n",
    "                      .drop('range_CharlsonIndexI') \\\n",
    "                      .drop('max_DrugCount') \\\n",
    "                      .drop('min_DrugCount') \\\n",
    "                      .drop('avg_DrugCount') \\\n",
    "                      .drop('months_DrugCount') \\\n",
    "                      .drop('max_LabCount') \\\n",
    "                      .drop('min_LabCount') \\\n",
    "                      .drop('avg_LabCount') \\\n",
    "                      .drop('months_LabCount') \\\n",
    "                      .dropna() \\\n",
    "                      .cache()\n",
    "                \n",
    "# Check the final data\n",
    "print 'df_final has %d rows and %d columns.' %(df_final.count(), len(df_final.columns))\n",
    "# print df_final.printSchema()\n",
    "# print df_final.show(1)\n",
    "\n",
    "# Free up memory\n",
    "df_complete.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.C.d   External Storage Read/Write of Final DataFrame <a name=\"4.C.d\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In case your kernel dies, and you do not want to rerun all of the above cells, you may want to store the final DataFrame.  Here is code for writing to and reading from external storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame df_final written to storage.\n"
     ]
    }
   ],
   "source": [
    "# Write to/Read from external storage\n",
    "\n",
    "# Write DataFrame to external storage\n",
    "df_final.write.save('df_final', mode='overwrite')\n",
    "print 'DataFrame df_final written to storage.'\n",
    "\n",
    "# Read DataFrame from external storage\n",
    "# df_final = sql_context.read.load('df_final')\n",
    "# print 'DataFrame df_final read from storage.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.C.e   EDA-2 <a name=\"4.C.e\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "It is a best practice to do more EDA after feature engineering to see how the features relate to the target variable.  See [Section 7: Next Steps](#7.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO: EDA on df_final.  Explore correlations of features and target variable, etc.  \n",
    "#        See if a feature set can be found that beats the performance of PCA in Section 5.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Modeling and Evaluation <a name=\"5\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.A  Create Training, Validation, and Test DataFrames <a name=\"5.A\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We randomly split the data in two steps, with 70% for training, 15% for validation, and 15% for final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training dataset: 103229\n",
      "Number of samples in validation dataset: 21980\n",
      "Number of samples in test dataset: 22264\n"
     ]
    }
   ],
   "source": [
    "df_train, df_other = df_final.randomSplit([0.70, 0.30], seed=24)\n",
    "df_val, df_test = df_other.randomSplit([0.50, 0.50], seed=24)\n",
    "\n",
    "df_train.cache()\n",
    "df_val.cache()\n",
    "df_test.cache()\n",
    "\n",
    "print 'Number of samples in training dataset: %d' %(df_train.count())\n",
    "print 'Number of samples in validation dataset: %d' %(df_val.count())\n",
    "print 'Number of samples in test dataset: %d' %(df_test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a function to get the column names of a given DataFrame, excluding the label (DaysInHospital)\n",
    "\n",
    "def get_column_names(dataframe):\n",
    "    column_names = dataframe.columns\n",
    "    final_columns = []\n",
    "    for column in column_names:\n",
    "        if column != 'label':\n",
    "            final_columns.append(column)\n",
    "    return final_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.B   Define Evaluation Metric <a name=\"5.B\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "The [HHP prediction accuracy measure](https://www.heritagehealthprize.com/c/hhp/details/evaluation) is the log root-mean-squared-error (log RMSE) loss function\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}{[log(p_i + 1) - log(a_i + 1)]\\,^2}}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "1.  $i$ is a member;  \n",
    "2.  $n$ is the total number of members;  \n",
    "3.  $p_i$ is the predicted number of days spent in the hospital for member $i$ in the test period;  \n",
    "4.  $a_i$ is the actual number of days spent in the hospital for member $i$ in the test period.  \n",
    "\n",
    "We will calculate this loss for the models we will train below.  \n",
    "\n",
    "To help in this calculation, we create a function to calculate the squared difference of the predictions and actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_squared_error(line):\n",
    "    actual = line[0]\n",
    "    prediction = line[1]\n",
    "    return (prediction - actual)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.C   Build and Evaluate Baseline Model <a name=\"5.C\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We use pipelines to build a baseline linear regression model with elastic net regularization (a combination of L1 and L2 regularization).  See [here](http://spark.apache.org/docs/latest/mllib-linear-methods.html#regularizers) for more information on regularization options for linear regression in Spark.\n",
    "\n",
    "The baseline model includes all 135 features.  In Section 5.D below, we will do some feature selection to reduce the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark16/python/pyspark/ml/regression.py:123: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log RMSE loss for the baseline model using linear regression is 0.462934\n"
     ]
    }
   ],
   "source": [
    "# Get all of our features together into one array called \"features\".  Do not include the label!\n",
    "feature_assembler = VectorAssembler(inputCols=get_column_names(df_train), outputCol=\"features\")\n",
    "\n",
    "# Define our model\n",
    "lr = LinearRegression(maxIter=100, elasticNetParam=0.80, labelCol=\"label\", featuresCol=\"features\", \n",
    "                      predictionCol = \"prediction\")\n",
    "\n",
    "# Define our pipeline\n",
    "pipeline_baseline = Pipeline(stages=[feature_assembler, lr])\n",
    "\n",
    "# Train our model using the training data\n",
    "model_baseline = pipeline_baseline.fit(df_train)\n",
    "\n",
    "# Use our trained model to make predictions using the validation data\n",
    "output_baseline = model_baseline.transform(df_val)  #.select(\"features\", \"label\", \"prediction\", \"coefficients\")\n",
    "predictions_baseline = output_baseline.select(\"label\", \"prediction\")\n",
    "\n",
    "# Measure the performance of our trained model on the validation data\n",
    "loss_baseline =  math.sqrt(predictions_baseline.map(calc_squared_error) \\\n",
    "                                               .reduce(lambda x,y: x+y)/float(predictions_baseline.count()))\n",
    "print \"The log RMSE loss for the baseline model using linear regression is %.6f\" %(loss_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model intercept:\n",
      "0.291329776374\n",
      "Baseline Model coefficients:\n",
      "[0.144059618566,-0.151405835641,-0.159943410084,-0.0945226337502,-0.126005534019,-0.160065302438,-0.159586868412,-0.123135310773,-0.0646469532678,0.00756374170444,0.0,-0.1446056407,-0.130053570429,0.000955626283541,0.000112108451644,0.0229743737326,-0.0133904405207,-0.00642017373504,-0.0161047437375,0.00552072364695,-0.0114542279277,0.000143528842829,0.00048435234629,-0.000524532175696,0.0,0.0,0.000955626283541,0.00584091589735,0.00608151809875,0.00401875751127,0.000222640563173,-0.00271415204716,0.00270136696357,-0.000327433605455,-0.00366323329242,-0.00353123606677,-0.00726889232987,0.0052131416062,-0.00759023625725,-0.00713348791836,-0.000146878174716,0.000693943085698,0.00414833327466,-0.00366063008065,-0.0100261858525,-0.00201529447708,0.0015321798347,0.0126907275677,-7.5556850662e-05,-0.00551560242023,0.00487105020992,0.0329217174065,0.00755463041043,-0.000701717749044,0.00135268189973,-0.00602011756255,0.00374336350728,-0.00340206937781,0.0,-0.00960676944087,0.0142862335893,-0.0053939182992,-0.00661899509948,-0.00759909505646,-0.00333073141416,-0.00424651453005,-0.00758255406151,0.00105652450896,0.00105113403839,-0.0194599479677,-0.011469687978,-0.0202119505627,-0.0220559817508,0.000237664589094,-0.00553145438334,-0.0254987280881,-0.00158587340432,-0.00585872376363,-0.00144614126789,-0.00356479230055,0.0113652093495,0.0178911420128,-0.0013757283255,-8.35989534602e-05,-0.00483289391516,-0.00337782876762,0.0017090114211,-0.0265620375141,0.0,-0.00367565829002,-0.0110890019572,-0.00984104016063,-0.0164380748832,-0.0196437452816,-0.00198701413809,-0.00806335681383,-0.000666046495395,-0.0445687991431,-0.0209354213545,0.0106584852904,-0.00244349171235,-0.00116165960871,-0.0459086966818,-0.0158174115166,0.0,-0.102591874239,-0.0328893506895,0.00858402937607,0.0195091155142,0.0115047018447,0.00972398756289,0.00584081043158,-0.00363031650623,0.0135579508116,0.0183937271367,0.0,0.00759411071183,0.00463623187629,-0.0119464870523,-0.00468104693436,0.0321030849328,-0.00336917906377,0.0128785048616,0.00524946463575,0.0448174283881,-0.00323389089091,-0.00193443511923,-0.0108308560984,0.0354790377676,-0.0020398384708,0.0038450190057,-0.000656270614148,-0.000961644769769,-0.00688157709814]\n"
     ]
    }
   ],
   "source": [
    "# Get the parameters of the baseline model\n",
    "print 'Baseline Model intercept:'\n",
    "print model_baseline.stages[-1].intercept\n",
    "print 'Baseline Model coefficients:'\n",
    "print model_baseline.stages[-1].coefficients\n",
    "\n",
    "\n",
    "# See Section 7: Next Steps #12\n",
    "# Get linear regression summary, such as coefficient standard errors, R-squared, etc.\n",
    "# This has only become available with Spark 2.0.0.\n",
    "# See the documentation on 'class pyspark.ml.regression.LinearRegressionSummary' at \n",
    "# https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html?highlight=crossvalidator#pyspark.ml.regression.LinearRegressionSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.D   Feature Selection: Principal Component Analysis (PCA) <a name=\"5.D\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Here are some references for doing feature selection:  \n",
    "\n",
    "-  For Categorical Features:  http://spark.apache.org/docs/latest/mllib-feature-extraction.html \n",
    "-  For Singular Value Decomposition and Principal Component Analysis:  http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html   \n",
    "\n",
    "We will use PCA to reduce our 135 variables down to just 10 components, and then fit another linear regression model using the 10 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA training complete.\n",
      "Example PCA components:\n",
      "[Row(pca_features=DenseVector([-101.8709, -7.232, 3.351, 1.5132, -3.182, 7.2285, -4.2851, -2.1116, -7.564, 3.5573]))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Pipeline for generating 10 PCA components\n",
    "\n",
    "# Get all of our features together into one array called \"features\".  Do not include the label!\n",
    "feature_assembler = VectorAssembler(inputCols=get_column_names(df_train), outputCol=\"features\")\n",
    "\n",
    "# Define our model\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Define our pipeline\n",
    "pipeline_pca = Pipeline(stages=[feature_assembler, pca])\n",
    "\n",
    "# Train our model using the training data\n",
    "model_pca = pipeline_pca.fit(df_train)\n",
    "print 'PCA training complete.'\n",
    "\n",
    "# Use our trained model to output new PCA features\n",
    "df_train_pca = model_pca.transform(df_train)\n",
    "df_val_pca = model_pca.transform(df_val)\n",
    "df_test_pca = model_pca.transform(df_test)\n",
    "\n",
    "df_train_pca.cache()\n",
    "df_val_pca.cache()\n",
    "df_test_pca.cache()\n",
    "\n",
    "print 'Example PCA components:'\n",
    "print df_train_pca.select('pca_features').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log RMSE loss for the PCA model using linear regression is 0.475768\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for building a linear regression model with the 10 components calculated above.\n",
    "# Note: We do not need to use the VectorAssembler here because the 10 PCA components are already\n",
    "#       compiled in a vector stored in the variable \"pca_features.\"\n",
    "\n",
    "# Define our model\n",
    "lr_pca = LinearRegression(maxIter=100, elasticNetParam=0.80, labelCol=\"label\", featuresCol=\"pca_features\", \n",
    "                      predictionCol = \"prediction\")\n",
    "\n",
    "# Define our pipeline\n",
    "pipeline_pca = Pipeline(stages=[lr_pca])\n",
    "\n",
    "# Train our model using the training data\n",
    "model_pca = pipeline_pca.fit(df_train_pca)\n",
    "\n",
    "# Use our trained model to make predictions using the validation data\n",
    "output_pca = model_pca.transform(df_val_pca)\n",
    "predictions_pca = output_pca.select(\"label\", \"prediction\")\n",
    "\n",
    "# Measure the performance of our trained model on the validation data\n",
    "loss_pca =  math.sqrt(predictions_pca.map(calc_squared_error) \\\n",
    "                                     .reduce(lambda x,y: x+y)/float(predictions_pca.count()))\n",
    "print \"The log RMSE loss for the PCA model using linear regression is %.6f\" %(loss_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Model Intercept:\n",
      "0.0479622137003\n",
      "PCA Model Coefficients:\n",
      "[-0.000987254186672,0.00211178198125,-0.00443428036693,-0.000759002880767,0.0012825897954,-0.00126693653089,-0.00395773531123,-0.00888382580741,-0.00183520669693,-0.00558761965335]\n"
     ]
    }
   ],
   "source": [
    "# Get the parameters of the PCA model\n",
    "print 'PCA Model Intercept:'\n",
    "print model_pca.stages[-1].intercept\n",
    "print 'PCA Model Coefficients:'\n",
    "print model_pca.stages[-1].coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.E   Hyperparameter Tuning <a name=\"5.E\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Hyperparameter tuning can be done using spark.ml pipelines.  See [here](http://spark.apache.org/docs/latest/ml-guide.html#example-model-selection-via-cross-validation) for examples of using **cross validation** to find the best hyperparameters.  See [here](http://spark.apache.org/docs/latest/ml-guide.html#example-model-selection-via-train-validation-split) for examples of using **train-validation split** to find the best hyperparameters.\n",
    "\n",
    "In this section, we will use **cross validation** to find the best hyperparameters for a **linear regression model** using our **PCA features** from Section [5.D](#5.D).\n",
    "\n",
    "The steps we will follow for using the [CrossValidator](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.tuning.CrossValidator) from [spark.ml.tuning](http://spark.apache.org/docs/latest/ml-tuning.html#ml-tuning-model-selection-and-hyperparameter-tuning) are:\n",
    "\n",
    "1.  Set lists of the parameters to try\n",
    "2.  Build the parameter grid that will be searched\n",
    "3.  Create a CrossValidator instance\n",
    "4.  Run the cross validation to find the best set of parameters\n",
    "5.  Validate the final best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 complete.\n",
      "Step 2 complete.\n",
      "Step 3 complete.\n",
      "Start time: 2016-08-04 19:28:30\n",
      "Step 4 complete.  End Time: 2016-08-04 19:31:20\n",
      "Step 5 complete.\n",
      "The RMSE loss for the best linear regression model found using cross validation is 0.475768\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Step 1: Set lists of the parameters to try\n",
    "elastic_net_params = [0.20, 0.50, 0.80]\n",
    "print 'Step 1 complete.'\n",
    "\n",
    "# Step 2: Build the parameter grid that will be searched\n",
    "param_grid_lr_pca = ParamGridBuilder().addGrid(lr_pca.elasticNetParam, elastic_net_params) \\\n",
    "                                      .build()\n",
    "print 'Step 2 complete.'\n",
    "\n",
    "# Step 3: Create a CrossValidator instance\n",
    "cv_lr_pca = CrossValidator().setEstimator(pipeline_pca) \\\n",
    "                            .setEvaluator(RegressionEvaluator(metricName=\"rmse\")) \\\n",
    "                            .setEstimatorParamMaps(param_grid_lr_pca) \\\n",
    "                            .setNumFolds(2)  # Use 3+ in practice\n",
    "print 'Step 3 complete.'\n",
    "\n",
    "# Step 4: Run the cross validation to find the best set of parameters\n",
    "print 'Start time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "model_cv_lr_pca = cv_lr_pca.fit(df_train_pca)\n",
    "print 'Step 4 complete.  End Time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Step 5: Validate the final best model\n",
    "output_cv_lr_pca = model_cv_lr_pca.transform(df_val_pca).select(\"features\", \"label\", \"prediction\")\n",
    "predictions_cv_lr_pca = output_cv_lr_pca.select(\"label\", \"prediction\")\n",
    "loss_cv_lr_pca =  math.sqrt(predictions_cv_lr_pca.map(calc_squared_error) \\\n",
    "                                                 .reduce(lambda x,y: x+y)/float(predictions_cv_lr_pca.count()))\n",
    "print 'Step 5 complete.'\n",
    "print \"The RMSE loss for the best linear regression model found using cross validation is %.6f\" %(loss_cv_lr_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double-check Log RMSE:\n",
      "0.4757684172\n",
      "Best Model Intercept:\n",
      "0.0479622137003\n",
      "Best Model Coefficients:\n",
      "[-0.000987254186672,0.00211178198125,-0.00443428036693,-0.000759002880767,0.0012825897954,-0.00126693653089,-0.00395773531123,-0.00888382580741,-0.00183520669693,-0.00558761965335]\n"
     ]
    }
   ],
   "source": [
    "# Double-check our calculate of the log RMSE above\n",
    "print 'Double-check Log RMSE:'\n",
    "print RegressionEvaluator(metricName=\"rmse\").evaluate(model_cv_lr_pca.transform(df_val_pca))\n",
    "\n",
    "# Get the parameters of the best model\n",
    "print 'Best Model Intercept:'\n",
    "print model_cv_lr_pca.bestModel.stages[-1].intercept\n",
    "print 'Best Model Coefficients:'\n",
    "print model_cv_lr_pca.bestModel.stages[-1].coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Results <a name=\"6\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Our **best result (log RMSE = 0.462934)** was achieved by the baseline linear regression model run on all 135 features.  The PCA linear regression model using 10 principal components had **log RMSE = 0.475768**, even after hyperparameter tuning.\n",
    "\n",
    "<table align=\"left\">\n",
    "<caption>Model Performance:  Loss</caption>\n",
    "<tr><td align=\"center\">Baseline Linear Regression</td><td align=\"center\">PCA Linear Regression</td><td align=\"center\">Hyperparameter Tuning on PCA</td></tr>\n",
    "<tr><td align=\"center\">0.462934</td><td align=\"center\">0.475768</td><td align=\"center\">0.475768</td></tr>\n",
    "</table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Next Steps <a name=\"7\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "1.  Complete [Section 3.B: EDA-0](#3.B).  Gather information to see what transformations may need to be done on the data.  Answer questions about each raw DataFrame.  In general, is the data in good shape?  For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what values does DaysInHospital take on?  Are they all integers?  What values does ClaimsTruncated take on?  Are they all integers?  In the Claims DataFrame (df_claims), how many different ProviderIDs are there?  How many different PrimaryConditionGroups are there?  What are their values?  What values can the CharlesonIndex take on?  Are they integers?  In the Drug Count DataFrame (df_drug_count), what values can DrugCount take on?  Are they all integers?  Given this information, what transformations are needed?\n",
    "\n",
    "2.  Complete [Section 3.D: EDA-1](#3.D).  Create tables and graphs to display information about the transformed DataFrames.  For inspiration, see the [Titanic notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/kmbgrkhh73931lo/Titanic-EDA-LogisticRegression.ipynb).  Answer questions about each DataFrame.  For example, in each of the Target DataFrames (df_target_Y1, df_target_Y2, df_target_Y3), what is the minimum, maximum, mean, and standard deviation of DaysInHospital?  In the Claims DataFrame, group by MemberID and Year and count the number of records.  What is the minimum, maximum, mean, and standard deviation of the count?  Do the same for the Drug Count and Lab Count DataFrames, etc.\n",
    "\n",
    "3.  In [Section 4.A.a: Claims Data](#4.A.a), where we aggregate by MemberID and Year, automate the creation of the SQL string to one hot encode the categorical variables by selecting distinct values of the variables PrimaryConditionGroup, Specialty, ProcedureGroup, and PlaceSvc.\n",
    "\n",
    "4.  Complete [Section 4.C.e: EDA-2](#4.C.e).  Create tables and graphs to display information relating the different feature variables to the target variable.  Get a feel for which features might be predictive of DaysInHospital.\n",
    "\n",
    "5.  In [Section 5.C: Build and Evaluate Baseline Model](#5.C), we build a linear regression model.  What if the model predicts negative values?  Transform the predictions so that any negative values are recoded to 0 and remeasure model performance.\n",
    "\n",
    "6.  In [Section 5.D: Feature Selection: Principal Component Analysis (PCA)](#5.D), try doing some hyperparameter tuning (i.e. try different numbers of components).  Print the model performance results out in a table.\n",
    "\n",
    "7.  In [Section 5.D: Feature Selection: Principal Component Analysis (PCA)](#5.D), build a model by selecting features by hand based on the findings in [Section 4.C.e: EDA-2](#4.C.e).  Can a model be built that outperforms the PCA model?\n",
    "\n",
    "8.  In Section [5.E: Hyperparameter Tuning](#5.E), try using the [train-validation split](http://spark.apache.org/docs/latest/ml-guide.html#example-model-selection-via-train-validation-split).  Does it outperform cross-validation?  Does it yield different optimal hyperparameters?\n",
    "\n",
    "9.  At the end of [Section 5.E: Hyperparameter Tuning](#5.E), figure out how to get the \"best model\" hyperparameters out of PipelineModels (we could not find how to do this in the Spark documentation).  Start looking [here](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=pipeline#pyspark.ml.PipelineModel).  Although, it may not be possible.  See [here](http://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark).  A user-defined function may have to be created.  See example [here](#9.C).\n",
    "\n",
    "10.  Could particular providers (ProviderID), vendors (Vendor), or primary care physicians (PCP) work with sicker patients who are more likely to go into the hospital?  If so, these variables might be highly correlated with the target variable (DaysInHospital).  There are thousands of different ProviderIDs, Vendors, and PCPs in the Claims data.  If we were to one hot encode them, this would create thousands of new features.  One way to reduce the dimensionality would be to hash the new features.  For an example of hashing, see the [\"3 Idiots' Approach\"](http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf) to the [Kaggle Criteo competition](https://www.kaggle.com/c/criteo-display-ad-challenge).  Also see their code [here](https://github.com/guestwalk/kaggle-2014-criteo).\n",
    "\n",
    "11.  Try additional models.  See possibilities [here](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.regression) (e.g. Decision Tree Regressor, Gradient-Boosted Trees Regressor, Random Forest Regressor).  See an example [here](#9.B).  Tune their hyperparameters.  Try different feature selections.\n",
    "\n",
    "12.  This notebook was built using Spark 1.6.  Try running the notebook on Spark 2.0 ([download here](http://spark.apache.org/downloads.html)).  What happens?  What changes are needed?  See [here](https://spark.apache.org/docs/latest/ml-guide.html) for the Spark documentation on the differences in Version 2.0.  One noteworthy difference is that we can now get a [summary of a linear regression model's statistics](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html?highlight=crossvalidator#pyspark.ml.regression.LinearRegressionSummary), such as the p-values, standard errors, and t-statistics of the coefficients and intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  Resources <a name=\"8\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- CSV Data Source [(link to Databricks' csv file upload module)](https://github.com/databricks/spark-csv)\n",
    "- Data Dictionary [(link to Kaggle competition)](https://www.heritagehealthprize.com/c/hhp/data)\n",
    "- DataFrame Operations [(link to Spark documentation)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "- DataFrames [(link to Spark documentation)](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "- EDA Techniques [(link to website)](http://itl.nist.gov/div898/handbook/eda/eda.htm)\n",
    "- Feature Extraction [(link to Spark documentation)](http://spark.apache.org/docs/latest/ml-features.html)\n",
    "- Feature Selection [(link to Spark documentation)](http://spark.apache.org/docs/latest/mllib-feature-extraction.html), [(link to more Spark documenation)](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)\n",
    "- Heritage Health Prize Summary [(link to Kaggle competition)](https://www.heritagehealthprize.com/c/hhp)\n",
    "- Hyperparameter Tuning: Cross Validation [(link to Spark documentation)](http://spark.apache.org/docs/latest/ml-guide.html#example-model-selection-via-cross-validation)\n",
    "- Hyperparameter Tuning: Train-Validation Split [(link to Spark documentation)](http://spark.apache.org/docs/latest/ml-guide.html#example-model-selection-via-train-validation-split)\n",
    "- Loss Function [(link to Kaggle competition)](https://www.heritagehealthprize.com/c/hhp/details/evaluation)\n",
    "- One Hot Encoding [(link to Quora)](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science), [(link to Wikipedia)](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)\n",
    "- Pipelines [(link to Spark documentation)](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- Regularization for Linear Regression [(link to Spark documentation)](http://spark.apache.org/docs/latest/mllib-linear-methods.html#regularizers)\n",
    "- Spark SQL [(link to Spark documentation)](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- SQL Statements Blog Source [(link to blog)](http://anotherdataminingblog.blogspot.com/2011/10/code-for-respectable-hhp-model.html)\n",
    "- Tuning Models in Spark [(link to Spark documentation)](http://spark.apache.org/docs/latest/ml-tuning.html#ml-tuning-model-selection-and-hyperparameter-tuning)\n",
    "- Titanic Kaggle Competition [(link to notebook)](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/kmbgrkhh73931lo/Titanic-EDA-LogisticRegression.ipynb), [(link to Kaggle competition)](https://www.kaggle.com/c/titanic)\n",
    "- User-Defined Functions in Spark Pipelines [(link to blog post)](http://blog.insightdatalabs.com/spark-pipelines-elegant-yet-powerful/)\n",
    "- Winners of the HHP Challenge [(link to milestone papers)](http://www.heritagehealthprize.com/c/hhp/details/milestone-winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9.  Appendix <a name=\"8\"></a>\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.A  Runtimes <a name=\"9.A\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "We had a single Spark server on a 64GB RAM and 8 core Softlayer Centos VM. We originally set up the ipython environment to be able to run multiple jobs at the same time by splitting RAM and cores using the command below while launching the ipython notebook server:    \n",
    "\n",
    "**IPYTHON_OPTS=\"notebook --ip=myIP --port=80\" $SPARK_HOME/bin/pyspark --master spark://myIP:7077 --executor-memory 30G --conf spark.cores.max=4**  \n",
    "(this would allow running two similtaneous jobs, since total RAM is 64GB and total cores are eight) \n",
    "\n",
    "However, in the end we configured the server to run only one job at a time since some of the code would not have run otherwise. That said, we know that most of the code should run fine on your local setup (we used a Mac with 16GB RAM and 4 cores) except:\n",
    "\n",
    "1. Feature Selection Using Principal Component Analysis (PCA)\n",
    "2. Hyperparameter Tuning\n",
    "\n",
    "One possible strategy is to run most of the code on your local set-up, save all of the output, then switch to the cloud when the code starts to fail (e.g. getting outOfMemory errors or code running forever) to save on cloud costs.\n",
    "\n",
    "Given this set-up, how long did it take to prepare the data, train models, and tune them?  We ran this entire notebook from beginning to end, and it took **17 minutes 20 seconds** to run in clock time.  It took **1 minute 32 seconds** of CPU time:\n",
    "\n",
    "<table align=\"left\">\n",
    "<caption>Runtimes</caption>\n",
    "<tr><td>Start time:</td><td>2016-08-03 03:01:07</td></tr>\n",
    "<tr><td>End time:</td><td>2016-08-03 03:18:27</td></tr>\n",
    "<tr><td>Elapsed time:</td><td>17.32 minutes</td></tr>\n",
    "<tr><td>Elapsed CPU time:</td><td>1.53 minutes</td></tr>\n",
    "</table>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Move this cell to the top of the notebook to measure runtimes and choose 'Run All' in the Cell menu\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "\n",
    "# start_time = time.time()\n",
    "# cpu_start_time = time.clock()\n",
    "# print 'Start time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Move this cell to the bottom of the notebook to measure runtimes and choose 'Run All' in the Cell menu\n",
    "# print 'End time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "# print 'Elapsed Time: %.2f minutes' %((time.time() - start_time)/60)\n",
    "# print 'Elapsed CPU Time: %.2f minutes' %((time.clock() - cpu_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.B  Example of Training a Non-Linear Model Using Spark Pipelines <a name=\"9.B\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "See [Section 7: Next Steps](#7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# feature_assembler = VectorAssembler(inputCols=get_column_names(df_train), outputCol=\"features\")\n",
    "\n",
    "# gbt = GBTRegressor(featuresCol=\"features\")\n",
    "\n",
    "# pipeline_baseline_gbt = Pipeline(stages=[feature_assembler, gbt])\n",
    "# model_baseline_gbt = pipeline_baseline_gbt.fit(df_train)\n",
    "# output_baseline_gbt = model_baseline_gbt.transform(df_val).select(\"features\", \"label\", \"prediction\")\n",
    "# predictions_baseline_gbt = output_baseline_gbt.select(\"label\", \"prediction\")\n",
    "\n",
    "# logloss_baseline_gbt =  math.sqrt(predictions_baseline_gbt.map(calc_squared_diff) \\\n",
    "#                                         .reduce(lambda x,y: x+y)/float(predictions_baseline_gbt.count()))\n",
    "# print \"The log loss for the baseline model using a gradient-boosted tree regressor is %.6f\" %(logloss_baseline_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.C  User-Defined Functions in Spark Pipelines <a name=\"9.C\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Spark.ml has many built-in functions for feature engineering that can be put into a Pipeline.  See [here](http://spark.apache.org/docs/latest/ml-features.html).  But users can also define their own functions.  See below for an example of how to implement your own function ([source](http://blog.insightdatalabs.com/spark-pipelines-elegant-yet-powerful/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's an example of how to create a user-defined function to put into a spark.ml Pipeline.\n",
    "\n",
    "# from pyspark.ml.util import keyword_only  \n",
    "# from pyspark.ml.pipeline import Transformer  \n",
    "# from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "\n",
    "# # Create a custom word count transformer class\n",
    "# class MyWordCounter(Transformer, HasInputCol, HasOutputCol):  \n",
    "#     @keyword_only\n",
    "#     def __init__(self, inputCol=None, outputCol=None):\n",
    "#         super(WordCounter, self).__init__()\n",
    "#         kwargs = self.__init__._input_kwargs\n",
    "#         self.setParams(**kwargs)\n",
    "\n",
    "#     @keyword_only\n",
    "#     def setParams(self, inputCol=None, outputCol=None):\n",
    "#         kwargs = self.setParams._input_kwargs\n",
    "#         return self._set(**kwargs)\n",
    "\n",
    "#     def _transform(self, dataset):\n",
    "#         out_col = self.getOutputCol()\n",
    "#         in_col = dataset[self.getInputCol()]\n",
    "\n",
    "#         # Define transformer logic\n",
    "#         def f(s):\n",
    "#             return len(s.split(' '))\n",
    "#         t = LongType()\n",
    "\n",
    "#         return dataset.withColumn(out_col, udf(f, t)(in_col))\n",
    "\n",
    "# # Instantiate the new word count transformer\n",
    "# wc = MyWordCounter(inputCol=\"review\", outputCol=\"wc\")  \n",
    "# example_pipeline = Pipeline(stages=[wc])\n",
    "# example_pipeline_model = example_pipeline.fit(example_data)\n",
    "# example_df = example_pipeline_model.transform(example_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
