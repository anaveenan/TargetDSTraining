{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees for Classification (over discrete input variables)\n",
    "\n",
    "- Loosely based on  from Chapter 17 of [Data Science From Scratch](https://www.safaribooksonline.com/library/view/data-science-from/9781491901410/) by Joel Grus   \n",
    "- Code Source adopted from: https://github.com/joelgrus/data-science-from-scratch/blob/master/code/decision_trees.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [Introduction](#1)\n",
    "2.  [Entropy](#2)\n",
    "3.  [The Entropy of a Partition](#3)\n",
    "4.  [Creating a Decision Tree](#4)   \n",
    "5.  [Putting It All Together](#5)\n",
    "6.  [Random Forests](#6)\n",
    "7.  [For Further Exploration](#7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is 56A0-1A81\n",
      "\n",
      " Directory of C:\\Users\\z001c9v\\Documents\\Target DS Training 2016\\TargetDSTraining\\HW4\n",
      "\n",
      "08/12/2016  10:05 PM    <DIR>          .\n",
      "08/12/2016  10:05 PM    <DIR>          ..\n",
      "08/07/2016  09:25 PM    <DIR>          .ipynb_checkpoints\n",
      "08/12/2016  10:13 AM               470 dataset.txt\n",
      "08/12/2016  10:05 PM           141,810 Decision_Trees.ipynb\n",
      "08/10/2016  11:33 PM            28,629 titanic_test.csv\n",
      "08/10/2016  11:33 PM            61,194 titanic_train.csv\n",
      "               4 File(s)        232,103 bytes\n",
      "               3 Dir(s)  355,592,691,712 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Pre - Processing\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!dir\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.11 :: Anaconda 4.0.0 (64-bit)\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('COMPUTERNAME', '9XK2R72')\n",
      "('PROCESSOR_LEVEL', '6')\n",
      "('LIB', ';C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('USERDOMAIN', 'DHC')\n",
      "('SPARK_HOME', 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\')\n",
      "('PSMODULEPATH', 'C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\')\n",
      "('COMMONPROGRAMFILES', 'C:\\\\Program Files\\\\Common Files')\n",
      "('PROCESSOR_IDENTIFIER', 'Intel64 Family 6 Model 94 Stepping 3, GenuineIntel')\n",
      "('VBOX_MSI_INSTALL_PATH', 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\')\n",
      "('PROGRAMFILES', 'C:\\\\Program Files')\n",
      "('PROCESSOR_REVISION', '5e03')\n",
      "('DATACONNECTORLIBPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('SYSTEMROOT', 'C:\\\\WINDOWS')\n",
      "('CLICOLOR', '1')\n",
      "('PROGRAMFILES(X86)', 'C:\\\\Program Files (x86)')\n",
      "('COMSPEC', 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe')\n",
      "('TERM', 'xterm-color')\n",
      "('DOCKER_TOOLBOX_INSTALL_PATH', 'C:\\\\Apps\\\\Docker Toolbox')\n",
      "('WINDOWS_TRACING_LOGFILE', 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log')\n",
      "('DB2INSTANCE', 'DB2')\n",
      "('PROCESSOR_ARCHITECTURE', 'AMD64')\n",
      "('ALLUSERSPROFILE', 'C:\\\\ProgramData')\n",
      "('SCCMNORDCDOWNLOAD', '1')\n",
      "('LOCALAPPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local')\n",
      "('HOMEPATH', '\\\\')\n",
      "('USERDOMAIN_ROAMINGPROFILE', 'DHC')\n",
      "('JAVA_HOME', 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\')\n",
      "('JPY_INTERRUPT_EVENT', '896')\n",
      "('PROGRAMW6432', 'C:\\\\Program Files')\n",
      "('USERNAME', 'Z001C9V')\n",
      "('COPLIB', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('LOGONSERVER', '\\\\\\\\TETTSDCC10P')\n",
      "('PROMPT', '$P$G')\n",
      "('WINDOWS_TRACING_FLAGS', '3')\n",
      "('JPY_PARENT_PID', '912')\n",
      "('PROGRAMDATA', 'C:\\\\ProgramData')\n",
      "('PYTHONPATH', 'C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;')\n",
      "('CLASSPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\tdgeospatial.jar;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\jmsam.jar;.;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2java.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\sqlj.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc_license_cu.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\bin;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\common.jar')\n",
      "('PATH', 'C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\Apps\\\\R\\\\Rtools\\\\bin;C:\\\\Apps\\\\R\\\\Rtools\\\\gcc-4.6.3\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Teradata Performance Monitor Object 15.00\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\msg;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\ODBC Driver for Teradata\\\\Lib;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files\\\\SAS94\\\\x86\\\\Secure\\\\ccme4;C:\\\\Program Files\\\\SAS94\\\\Secure\\\\ccme4;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\tdwallet\\\\nt-i386\\\\;C:\\\\Program Files (x86)\\\\Microsoft Application Virtualization Client;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin\\\\;C:\\\\WINDOWS\\\\SysWOW64\\\\;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\SysWOW64\\\\Wbem;C:\\\\WINDOWS\\\\SysWOW64\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program Files (x86)\\\\AdminStudio\\\\9.5\\\\Common;C:\\\\PROGRA~1\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Program Files\\\\ibm\\\\gsk8\\\\lib64;C:\\\\Program Files (x86)\\\\ibm\\\\gsk8\\\\lib;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Apps\\\\Anaconda2;C:\\\\Apps\\\\Anaconda2\\\\Scripts;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda3;C:\\\\Apps\\\\Anaconda3\\\\Scripts;C:\\\\Apps\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;;C:\\\\Apps\\\\Docker Toolbox;C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\;C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2;')\n",
      "('HOMESHARE', '\\\\\\\\corp.target.com\\\\dfsroot\\\\HomeDirs\\\\HQ02\\\\59808964')\n",
      "('GIT_PAGER', 'cat')\n",
      "('UATDATA', 'C:\\\\WINDOWS\\\\CCM\\\\UATData\\\\D9F8C395-CAB8-491d-B8AC-179A1FE1BE77')\n",
      "('PATHEXT', '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC')\n",
      "('INCLUDE', 'C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\INCLUDE;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('SESSIONNAME', 'Console')\n",
      "('FP_NO_HOST_CHECK', 'NO')\n",
      "('WINDIR', 'C:\\\\WINDOWS')\n",
      "('TEMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('HOMEDRIVE', 'H:')\n",
      "('OS', 'Windows_NT')\n",
      "('SYSTEMDRIVE', 'C:')\n",
      "('PUBLIC', 'C:\\\\Users\\\\Public')\n",
      "('NUMBER_OF_PROCESSORS', '8')\n",
      "('APPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Roaming')\n",
      "('USERDNSDOMAIN', 'CORP.TARGET.COM')\n",
      "('IBMDB2', 'C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN')\n",
      "('PAGER', 'cat')\n",
      "('COMMONPROGRAMW6432', 'C:\\\\Program Files\\\\Common Files')\n",
      "('HADOOP_HOME', 'C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2')\n",
      "('COMMONPROGRAMFILES(X86)', 'C:\\\\Program Files (x86)\\\\Common Files')\n",
      "('IPY_INTERRUPT_EVENT', '896')\n",
      "('USERPROFILE', 'C:\\\\Users\\\\z001c9v')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for key, value in os.environ.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyspark",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-27bcafd806d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Spark Initialization: DOCKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyspark"
     ]
    }
   ],
   "source": [
    "# Spark Initialization: DOCKER \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda2\\;C:\\Apps\\Anaconda2\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;\n",
      "<pyspark.context.SparkContext object at 0x0000000003FD50B8>\n",
      "<pyspark.sql.context.SQLContext object at 0x0000000003F2A7F0>\n"
     ]
    }
   ],
   "source": [
    "# Initiate Spark on Local:  ** Do not run this on Docker **\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python\\\\lib\\\\py4j-0.10.1-src.zip')) #Note, this needs to be in PYTHONPATH env variable.\n",
    "\n",
    "\n",
    "print(os.environ['PYTHONPATH'])\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"MyApp\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Introduction <a name=\"1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\"A decision tree uses a tree structure to represent a number of possible *decision paths* and an outcome for each path.\n",
    "\n",
    "If you have ever played the game Twenty Questions, then it turns out you are familiar with decision trees.\n",
    "\n",
    "<img src=\"https://dl.dropbox.com/s/huzi4ffo0ooxu32/DecisionTreeAnimal.png\" width=\"600\" height=\"600\" />\n",
    "\n",
    "Finding an 'optimal' decision tree for a set of training data is computationally a very hard problem. (We will get around this by trying to build a good-enough tree rather than an optimal one, although for large data sets this can still be alot of work.) More important, it is very easy (and very bad) to build decision trees that are *overfitted* to the training data, and that don’t generalize well to unseen data.\n",
    "\n",
    "Most people divide decision trees into *classification trees* (which produce categorical outputs) and *regression trees* (which produce numeric outputs).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Entropy <a name=\"2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\"In order to build a decision tree, we will need to decide what questions to ask and in what order. At each stage of the tree there are some possibilities we’ve eliminated and some that we haven’t. After learning that an animal doesn’t have more than five legs, we’ve eliminated the possibility that it’s a grasshopper. We haven’t eliminated the possibility that it’s a duck. Every possible question partitions the remaining possibilities according to their answers.  \n",
    "\n",
    "Ideally, we’d like to choose questions whose answers give a lot of information about what our tree should predict. If there’s a single yes/no question for which 'yes' answers always correspond to True outputs and 'no' answers to False outputs (or vice versa), this would be an awesome question to pick. Conversely, a yes/no question for which neither answer gives you much new information about what the prediction should be is probably not a good choice.\n",
    "\n",
    "We capture this notion of 'how much information' with *entropy*. You have probably heard this used to mean disorder. We use it to represent the uncertainty associated with data.\n",
    "\n",
    "Imagine that we have a set $S$ of data, each member of which is labeled as belonging to one of a finite number of classes $C_1, \\dots , C_n$. If all the data points belong to a single class, then there is no real uncertainty, which means we’d like there to be low entropy.  If the data points are evenly spread across the classes, there is a lot of uncertainty and we’d like there to be high entropy.  \n",
    "\n",
    "In math terms, if $p_i$ is the proportion of data labeled as class $c_i$, we define the entropy as: \n",
    "\n",
    "\\begin{equation}\n",
    "H(S)=  −p_1 log_2(p_1) − \\dots − p_n log_2(p_n)\n",
    "\\end{equation}\n",
    "\n",
    "with the (standard) convention that $0 log 0 = 0$.\n",
    "\n",
    "Without worrying too much about the grisly details, each term $−p_i log_2(p_i)$ is non-negative and is close to zero precisely when $p_i$ is either close to zero or close to one (Figure 17-2).\n",
    "\n",
    "<img src=\"https://dl.dropbox.com/s/txnatys7w6u5mor/plogp.png\" width=\"600\" height=\"600\" />\n",
    "\n",
    "This means the entropy will be small when every $p_i$ is close to 0 or 1 (i.e., when most of the data is in a single class), and it will be larger when many of the $p_i$’s are not close to 0 (i.e., when the data is spread across multiple classes). This is exactly the behavior we desire.  It is easy enough to roll all of this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Our data will consist of pairs (input, label), which means that we’ll need to compute the class probabilities ourselves. Observe that we don’t actually care which label is associated with each probability, only what the probabilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count/total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):        \n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  The Entropy of a Partition <a name=\"3\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\"What we’ve done so far is compute the entropy (think 'uncertainty') of a single set of labeled data. Now, each stage of a decision tree involves asking a question whose answer partitions data into one or (hopefully) more subsets. For instance, our 'does it have more than five legs?' question partitions animals into those who have more than five legs (e.g., spiders) and those that don’t (e.g., echidnas).\n",
    "\n",
    "Correspondingly, we’d like some notion of the entropy that results from partitioning a set of data in a certain way. We want a partition to have low entropy if it splits the data into subsets that themselves have low entropy (i.e., are highly certain), and high entropy if it contains subsets that (are large and) have high entropy (i.e., are highly uncertain).\n",
    "\n",
    "For example, my 'Australian five-cent coin' question was pretty dumb (albeit pretty lucky!), as it partitioned the remaining animals at that point into $S_1$ = {echidna} and $S_2$ = {everything else}, where $S_2$ is both large and high-entropy. ($S_1$ has no entropy but it represents a small fraction of the remaining 'classes.')  \n",
    "\n",
    "Mathematically, if we partition our data $S$ into subsets $S_1, \\dots , S_m$ containing proportions $q_1, \\dots , q_m$ of the data, then we compute the entropy of the partition as a weighted sum: \n",
    "\n",
    "\\begin{equation}\n",
    "H = q_1H(S_1) + \\dots + q_mH(S_m)\n",
    "\\end{equation}\n",
    "\n",
    "which we can implement as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)    \n",
    "    return sum(data_entropy(subset)*len(subset)/total_count for subset in subsets )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One problem with this approach is that partitioning by an attribute with many different values will result in a very low entropy due to overfitting. For example, imagine you work for a bank and are trying to build a decision tree to predict which of your customers are likely to default on their mortgages, using some historical data as your training set. Imagine further that the data set contains each customer’s Social Security number. Partitioning on SSN will produce one-person subsets, each of which necessarily has zero entropy. But a model that relies on SSN is certain not to generalize beyond the training set. For this reason, you should probably try to avoid (or bucket, if appropriate) attributes with large numbers of possible values when creating decision trees.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Creating a Decision Tree <a name=\"4\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\"The VP provides you with the interviewee data, consisting of (per your specification) pairs (input, label), where each input is a *dict* of candidate attributes, and each label is either True (the candidate interviewed well) or False (the candidate inter‐viewed poorly). In particular, you are provided with each candidate’s level, her preferred language, whether she is active on Twitter, and whether she has a PhD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "          ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "          ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "          ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "          ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "          ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "          ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "          ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "          ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "          ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "          ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "          ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "          ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "          ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "          ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Our tree will consist of *decision nodes* (which ask a question and direct us differently depending on the answer) and *leaf nodes* (which give us a prediction). We will build it using the relatively simple ID3 algorithm, which operates in the following manner.  Let’s say we’re given some labeled data, and a list of attributes to consider branching on.\n",
    "\n",
    "- If the data all have the same label, then create a leaf node that predicts that label and then stop.\n",
    "- If the list of attributes is empty (i.e., there are no more possible questions to ask), then create a leaf node that predicts the most common label and then stop.\n",
    "- Otherwise, try partitioning the data by each of the attributes\n",
    "- Choose the partition with the lowest partition entropy\n",
    "- Add a decision node based on the chosen attribute\n",
    "- Recur on each partitioned subset using the remaining attributes\n",
    "\n",
    "This is what’s known as a 'greedy' algorithm because, at each step, it chooses the most immediately best option. Given a data set, there may be a better tree with a worse-looking first move. If so, this algorithm won’t find it. Nonetheless, it is relatively easy to understand and implement, which makes it a good place to begin exploring decision trees.\n",
    "\n",
    "Let’s manually go through these steps on the interviewee data set. The data set has both `True` and `False` labels, and we have four attributes we can split on. So our first step will be to find the partition with the least entropy. We’ll start by writing a function that does the partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item \n",
    "    is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups\n",
    "    \n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
    "    each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"and one that uses it to compute entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"        \n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Then we just need to find the minimum-entropy partition for the whole data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.693536138896\n",
      "lang 0.860131712855\n",
      "tweets 0.788450457308\n",
      "phd 0.892158928262\n"
     ]
    }
   ],
   "source": [
    "for key in['level','lang','tweets','phd']:\n",
    "    print key, partition_entropy_by(inputs, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> level 0.693536138896  \n",
    "> lang 0.860131712855  \n",
    "> tweets 0.788450457308  \n",
    "> phd 0.892158928262  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The lowest entropy comes from splitting on level, so we’ll need to make a subtree for each possible level value. Every Mid candidate is labeled True, which means that the Mid subtree is simply a leaf node predicting True. For Senior candidates, we have a mix of Trues and Falses, so we need to split again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.950977500433\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [(input, label) for input, label in inputs if input[\"level\"] == \"Senior\"]\n",
    "\n",
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print key, partition_entropy_by(senior_inputs, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lang 0.4  \n",
    "> tweets 0.0  \n",
    "> phd 0.950977500433 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"This shows us that our next split should be on tweets, which results in a zero-entropy partition. For these Senior-level candidates, 'yes' tweets always result in True while 'no' tweets always result in False.\n",
    "\n",
    "Finally, if we do the same thing for the Junior candidates, we end up splitting on phd, after which we find that no PhD always results in True and PhD always results in False.\"\n",
    "\n",
    "<img src=\"https://dl.dropbox.com/s/sglzar089hb5rfg/DecisionTreeHiring.png\" width=\"600\" height=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Putting It All Together <a name=\"5\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\"Now that we’ve seen how the algorithm works, we would like to implement it more generally. This means we need to decide how we want to represent trees. We’ll use pretty much the most lightweight representation possible. We define a *tree* to be one of the following:\n",
    "\n",
    "- `True`\n",
    "- `False`\n",
    "- `a tuple (attribute, subtree_dict)`\n",
    "\n",
    "Here `True` represents a leaf node that returns `True` for any input, `False` represents a leaf node that returns False for any input, and a tuple represents a decision node that, for any input, finds its `attribute` value, and classifies the input using the corresponding subtree.  With this representation, our hiring tree would look like:\n",
    "\n",
    "`('level',  \n",
    "  {'Junior':('phd',{'no':True,'yes':False}),  \n",
    "   'Mid': True,  \n",
    "   'Senior':('tweets',{'no':False,'yes':True})})`  \n",
    "\n",
    "There’s still the question of what to do if we encounter an unexpected (or missing) attribute value. What should our hiring tree do if it encounters a candidate whose `level` is “Intern”? We’ll handle this case by adding a `None` key that just predicts the most common label. (Although this would be a bad idea if `None` is actually a value that appears in the data.)\n",
    "\n",
    "Given such a representation, we can classify an input with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "    \n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "   \n",
    "    # otherwise find the correct subtree\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute)  # None if input is missing attribute\n",
    "\n",
    "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
    "        subtree_key = None              # we'll use the None subtree\n",
    "    \n",
    "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
    "    return classify(subtree, input)     # and use it to classify the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"All that’s left is to build the tree representation from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the tree\n",
      "('level', {None: True, 'Senior': ('tweets', {'yes': True, None: False, 'no': False}), 'Mid': True, 'Junior': ('phd', {'yes': False, None: True, 'no': True})})\n"
     ]
    }
   ],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "\n",
    "    # if this is our first pass, \n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0:                  # if only Falses are left\n",
    "        return False                    # return a \"False\" leaf\n",
    "        \n",
    "    if num_falses == 0:                 # if only Trues are left\n",
    "        return True                     # return a \"True\" leaf\n",
    "\n",
    "    if not split_candidates:            # if no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "                            \n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "        key=partial(partition_entropy_by, inputs))\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates \n",
    "                      if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.iteritems() }\n",
    "\n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "\n",
    "    return (best_attribute, subtrees)\n",
    "\n",
    "print \"building the tree\"\n",
    "tree = build_tree_id3(inputs)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the tree we built, every leaf consisted entirely of `True` inputs or entirely of `False` inputs. This means that the tree predicts perfectly on the training data set. But we can also apply it to new data that wasn’t in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junior / Java / tweets / no phd:  True\n",
      "Junior / Java / tweets / phd:  False\n"
     ]
    }
   ],
   "source": [
    "print ( \"Junior / Java / tweets / no phd: \" ) , classify(tree, \n",
    "     {\"level\" : \"Junior\", \n",
    "      \"lang\" : \"Java\", \n",
    "      \"tweets\" : \"yes\", \n",
    "      \"phd\" : \"no\"})  \n",
    "\n",
    "print ( \"Junior / Java / tweets / phd: \" ) , classify(tree, \n",
    "    {\"level\" : \"Junior\", \n",
    "     \"lang\" : \"Java\", \n",
    "     \"tweets\" : \"yes\", \n",
    "     \"phd\" : \"yes\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"And also to data with missing or unexpected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intern:  True\n",
      "Senior:  False\n"
     ]
    }
   ],
   "source": [
    "print \"Intern: \", classify(tree, { \"level\" : \"Intern\" } )\n",
    "print \"Senior: \", classify(tree, { \"level\" : \"Senior\" } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Random Forests <a name=\"6\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\"Given how closely decision trees can fit themselves to their training data, it’s not surprising that they have a tendency to overfit. One way of avoiding this is a technique called *random forests*, in which we build multiple decision trees and let them vote on how to classify inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Our tree-building process was deterministic, so how do we get random trees?  One piece involves bootstrapping data (recall “Digression: The Bootstrap” on page183).  Rather than training each tree on all the inputs in the training set, we train each tree on the result of `bootstrap_sample(inputs)`. Since each tree is built using different data, each tree will be different from every other tree. (A side benefit is that it’s totally fair to use the nonsampled data to test each tree, which means you can get away with using all of your data as the training set if you are clever in how you measure performance.) This technique is known as *bootstrap aggregating* or *bagging*.  \n",
    "\n",
    "A second source of randomness involves changing the way we chose the `best_attribute` to split on. Rather than looking at all the remaining attributes, we first choose a random subset of them and then split on whichever of those is best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # if there's already few enough split candidates, look at all of them\n",
    "# if len(split_candidates) <= self.num_split_candidates:\n",
    "#     sampled_split_candidates = split_candidates    \n",
    "# # otherwise pick a random sample\n",
    "# else:\n",
    "#     sampled_split_candidates = random.sample(split_candidates, self.num_split_candidates)\n",
    "    \n",
    "# # now choose the best attribute only from those candidates\n",
    "# best_attribute = min(sampled_split_candidates, key = partial(partition_entropy_by, inputs))\n",
    "\n",
    "# partitions=partition_by(inputs, best_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"This is an example of a broader technique called *ensemble learning* in which we combine several weak learners (typically high-bias, low-variance models) in order to produce an overall strong model.\n",
    "\n",
    "Random forests are one of the most popular and versatile models around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  For Further Exploration <a name=\"7\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- scikit-learn has many [`Decision Tree`](http://bit.ly/1ycPmuq) models. It also has an [`ensemble`](http://bit.ly/1ycPom1) module that includes a `RandomForestClassifier` as well as other ensemble methods.\n",
    "- We barely scratched the surface of decision trees and their algorithms. [Wikipedia](http://bit.ly/1ycPn1j) is a good starting point for broader exploration.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  HW4 \n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__HW4.1 Build a decision to predict whether you can play tennis or not <a name=\"8\"></a>__\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Decision Trees\n",
    "\n",
    "Write a program in Python (or in Spark; this part is optional) to implement the ID3 decision tree algorithm. You should build a tree to predict PlayTennis, based on the other attributes (but, do not use the Day attribute in your tree.). You should read in a space delimited dataset in a file called dataset.txt and output to the screen your decision tree and the training set accuracy in some readable format. For example, here is the tennis dataset. The first line will contain the names of the fields:\n",
    "\n",
    "<PRE>\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no\n",
    "</PRE>\n",
    "\n",
    "The last column is the classification attribute, and will always contain contain the values yes or no.\n",
    "\n",
    "For output, you can choose how to draw the tree so long as it is clear what the tree is. You might find it easier if you turn the decision tree on its side, and use indentation to show levels of the tree as it grows from the left. For example:\n",
    "\n",
    "<PRE>\n",
    "outlook = sunny\n",
    "|  humidity = high: no\n",
    "|  humidity = normal: yes\n",
    "outlook = overcast: yes\n",
    "outlook = rainy\n",
    "|  windy = TRUE: no\n",
    "|  windy = FALSE: yes\n",
    "\n",
    "</PRE>\n",
    "\n",
    "You don't need to make your tree output look exactly like above: feel free to print out something similarly readable if you think it is easier to code.\n",
    "\n",
    "You may find Python dictionaries especially useful here, as they will give you a quick an easy way to help manage counting the number of times you see a particular attribute.\n",
    "\n",
    "Here are some FAQs that I've gotten in the past regarding this assignment, and some I might get if I don't answer them now.\n",
    "\n",
    "__Should my code work for other datasets besides the tennis dataset?__ \n",
    "Yes. We will give your program a different dataset to try it out with. You may assume that our dataset is correct and well-formatted, but you should not make assumptions regrading number of rows, number of columns, or values that will appear within. The last column will also be the classification, and will always contain yes or no values.\n",
    "\n",
    "__Is it possible that some value, like \"normal,\" could appear in more than one column?__\n",
    "Yes. In addition to the column \"humidity\", we might have had another column called \"skycolor\" which could have values \"normal,\" \"weird,\" and \"bizarre.\"\n",
    "\n",
    "__Could \"yes\" and \"no\" appear as possible values in columns other than the classification column?__\n",
    "Yes. In addition to the classification column \"playtennis,\" we might have had another column called \"seasonalweather\" which would contain \"yes\" and \"no.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.txt\n",
    "\n",
    "Day outlook temperature humidity wind playtennis\n",
    "d1 sunny hot high FALSE no\n",
    "d2 sunny hot high TRUE no\n",
    "d3 overcast hot high FALSE yes\n",
    "d4 rainy mild high FALSE yes\n",
    "d5 rainy cool normal FALSE yes\n",
    "d6 rainy cool normal TRUE no\n",
    "d6 overcast cool normal TRUE yes\n",
    "d7 sunny mild high FALSE no\n",
    "d8 sunny cool normal FALSE yes\n",
    "d9 rainy mild normal FALSE yes\n",
    "d10 sunny mild normal TRUE yes\n",
    "d11 overcast mild high TRUE yes\n",
    "d12 overcast hot normal FALSE yes\n",
    "d12 rainy mild high TRUE no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Day outlook temperature humidity wind playtennis\r\n",
      "d1 sunny hot high FALSE no\r\n",
      "d2 sunny hot high TRUE no\r\n",
      "d3 overcast hot high FALSE yes\r\n",
      "d4 rainy mild high FALSE yes\r\n",
      "d5 rainy cool normal FALSE yes\r\n",
      "d6 rainy cool normal TRUE no\r\n",
      "d6 overcast cool normal TRUE yes\r\n",
      "d7 sunny mild high FALSE no\r\n",
      "d8 sunny cool normal FALSE yes\r\n",
      "d9 rainy mild normal FALSE yes\r\n",
      "d10 sunny mild normal TRUE yes\r\n",
      "d11 overcast mild high TRUE yes\r\n",
      "d12 overcast hot normal FALSE yes\r\n",
      "d12 rainy mild high TRUE no"
     ]
    }
   ],
   "source": [
    "!cat dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item \n",
    "    is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups\n",
    "    \n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
    "    each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW4.1.1 What is the classification accuracy of the tree on the training data?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.2  Is it possible to produce some set of correct training examples that will get the algorihtm\n",
    "to include the attribute Temperature in the learned tree, even though the true target concept is\n",
    "independent of Temperature? if no, explain. If yes, give such a set. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.3  Now, build a tree using only examples D1–D7. What is the classification accuracy for the\n",
    "training set? what is the accuracy for the test set (examples D8–D14)? explain why you think these\n",
    "are the results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW4.1.4 In this case, and others, there are only a few labelled examples available for training (that\n",
    "is, no additional data is available for testing or validation). Suggest a concrete pruning strategy, that\n",
    "can be readily embedded in the algorithm, to avoid over fitting. Explain why you think this strategy\n",
    "should work.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.2 Regression Tree (OPTIONAL Homework) \n",
    "[Back to Table of Contents](#TOC)\n",
    "Implement a decision tree algorithm for regression for two input continous variables and one categorical input variable on a single core computer using Python. \n",
    "\n",
    "- Use the IRIS dataset to evaluate your code, where the input variables are: Petal.Length Petal.Width  Species  and the target or output variable is  Sepal.Length. \n",
    "- Use the same dataset to train and test your implementation. \n",
    "- Stop expanding nodes once you have less than ten (10) examples (along with the usual stopping criteria). \n",
    "- Report the mean squared error for your implementation and contrast that with the MSE from scikit-learn's implementation on this dataset (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## HW4.3 Predict survival on the Titanic using Python (Logistic regression, SVMs, Random Forests)\n",
    "# https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<type 'str'>\n",
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "<type 'list'>\n",
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "('t_train:', <type 'list'>)\n",
      "('t_train:', ['1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S', '2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C', '3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S', '4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S', '5,0,3,\"Allen, Mr. William Henry\",male,35,0,0,373450,8.05,,S'])\n",
      "('t_train_rdd:', <class 'pyspark.rdd.RDD'>)\n",
      "('t_train_2:', <class 'pyspark.rdd.PipelinedRDD'>)\n",
      "('t_train_3:', <class 'pyspark.rdd.PipelinedRDD'>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input training data\n",
    "#train_data_csv = '/home/jovyan/work/root/Documents/Target DS Training 2016/TargetDSTraining/HW4/titanic_train.csv'\n",
    "t_train = sc.textFile(\"titanic_train.csv\")\n",
    "print (type(t_train))\n",
    "\n",
    "t_train_utf = t_train.map(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "col_names = t_train_utf.collect()[0]\n",
    "print (type(col_names))\n",
    "print (col_names)\n",
    "col_names_dt = col_names.split(',')\n",
    "print (type(col_names_dt))\n",
    "col_names_dt.remove('PassengerId')\n",
    "col_names_dt.remove('Name')\n",
    "print (col_names_dt)\n",
    "                                  \n",
    "\n",
    "\n",
    "t_train = t_train_utf.cache()\n",
    "#del t_train[0]\n",
    "t_data = t_train.collect()[1:]      # indexing the RDD removes Rows!!!!\n",
    "print ('t_train:' , type(t_train.collect()[1:]))\n",
    "print ('t_train:' , t_data[:5])\n",
    "t_train_rdd = sc.parallelize(t_data)\n",
    "print ('t_train_rdd:' , type(t_train_rdd))\n",
    "t_train_rdd.take(5)\n",
    "\n",
    "t_train_2 = t_train_rdd.map(lambda line: line.split(','))\n",
    "print ('t_train_2:' ,type(t_train_2))\n",
    "t_train_2.take(5)\n",
    "\n",
    "t_train_3 = t_train_2.map(lambda line: line.pop(0))\n",
    "print ('t_train_3:' ,type(t_train_3))\n",
    "t_train_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <type 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-32b35ebc4de3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#df = sqlContext.createDataFrame(rdd, ['name', 'age'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"PassengerId\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Survived\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Pclass\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Sex\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Age\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"SibSp\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Parch\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Ticket\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Fare\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Cabin\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Embarked\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#for i in people:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \"\"\"\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\sql\\session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\sql\\session.pyc\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\sql\\session.pyc\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m             \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\sql\\types.pyc\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <type 'str'>"
     ]
    }
   ],
   "source": [
    "# Start by parsing the training data\n",
    "t_train_rdd_1 = t_train.map(lambda x: Row(x.split(',')))\n",
    "t_train_rdd_1.first()\n",
    "#print t_train_rdd_1.collect()\n",
    "                            \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "parts = t_train.map(lambda l: l.split(\",\"))\n",
    "print (type(people))\n",
    "#people = t_train.map(lambda p: Row() for c in parts).collect() #, age=int(p[1])))\n",
    "print (type(people))\n",
    "\n",
    "#df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
    "df = sqlContext.createDataFrame(t_train, [\"PassengerId\",\"Survived\",\"Pclass\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\"])\n",
    "df.collect()\n",
    "#for i in people:\n",
    " #   print i\n",
    "\n",
    "#sqlContext.createDataFrame(t_train , ('PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "#sqlContext.createDataFrame()\n",
    "#.printSchema\n",
    "Person = Row(\"name\", \"age\")\n",
    "print type(Person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the data should be RDD of LabeledPoint",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-7515436f79a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m model = DecisionTree.trainClassifier(t_train_rdd_1, numClasses=2, categoricalFeaturesInfo={},\n\u001b[1;32m----> 6\u001b[1;33m                                      impurity='gini', maxDepth=5, maxBins=32)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36mtrainClassifier\u001b[1;34m(cls, data, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \"\"\"\n\u001b[0;32m    205\u001b[0m         return cls._train(data, \"classification\", numClasses, categoricalFeaturesInfo,\n\u001b[1;32m--> 206\u001b[1;33m                           impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(cls, data, type, numClasses, features, impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\u001b[0m\n\u001b[0;32m    142\u001b[0m                minInstancesPerNode=1, minInfoGain=0.0):\n\u001b[0;32m    143\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"the data should be RDD of LabeledPoint\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         model = callMLlibFunc(\"trainDecisionTreeModel\", data, type, numClasses, features,\n\u001b[0;32m    146\u001b[0m                               impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\n",
      "\u001b[1;31mAssertionError\u001b[0m: the data should be RDD of LabeledPoint"
     ]
    }
   ],
   "source": [
    "# Using mllib :\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from time import time\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "model = DecisionTree.trainClassifier(t_train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "t0 = time()\n",
    "model = DecisionTree.trainClassifier(t_train_rdd_1, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "t1 = time()\n",
    "\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.4 Heritage Healthcare Prize\n",
    "\n",
    "Please the following notebooks\n",
    "- Spark SQL + MLLib solution (with optional \n",
    "- Spark Map-Reduce + MMLlib solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
