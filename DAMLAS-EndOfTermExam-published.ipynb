{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS MidTerm Exam  \n",
    "\n",
    "11:00AM - 1:00PM(CT)\n",
    "August 22, 2016   \n",
    "End of Term Exam\n",
    "\n",
    "\n",
    "Data Analytics and Machine Learning at Scale\n",
    "\n",
    "Target, Minneapolis\n",
    "\n",
    "### Please insert your contact information here\n",
    "__Alfredo Marquez__    \n",
    "alfredo.marquez2@target.com\n",
    "> submission link: https://docs.google.com/forms/d/e/1FAIpQLScUwl33oUccLOhDZtRKpcy6YDqiLU4BDGUL7MDo4CHCIIsjag/viewform?c=0&w=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for exam \n",
    "\n",
    "0. Please submit your solutions and notebook via the following form:\n",
    "\n",
    "     [Exam Submission form](http://goo.gl/forms/iN4XIXQyiH1iAdyD3) \n",
    "\n",
    "            \n",
    "1. __Please acknowledge receipt of exam by sending a quick reply to the Jimi__ \n",
    "2. Review the submission form first to scope it out (it will take a 5-10 minutes to input your \n",
    "   answers and other information into this form)\n",
    "3. Please keep all your work and responses in ONE (1) notebook only (and submit via the submission form)\n",
    "4. Please make sure that the NBViewer link for your Submission notebook works\n",
    "5. Please do NOT discuss this exam with anyone (including your class mates) __until after Monday, September 5. __\n",
    "6. This is an open book exam meaning you can consult webpages and textbooks, class notes, slides etc. but you can not consult each other or any other person/group. Please complete this exam by yourself within the time limit. \n",
    "7. For markdown help in iPython Notebooks please see:\n",
    "https://sourceforge.net/p/ipython/discussion/markdown_syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam questions begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('COMPUTERNAME', '9XK2R72')\n",
      "('PROCESSOR_LEVEL', '6')\n",
      "('LIB', ';C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('USERDOMAIN', 'DHC')\n",
      "('SPARK_HOME', 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\')\n",
      "('PSMODULEPATH', 'C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\PowerShell\\\\Modules\\\\')\n",
      "('COMMONPROGRAMFILES', 'C:\\\\Program Files\\\\Common Files')\n",
      "('PROCESSOR_IDENTIFIER', 'Intel64 Family 6 Model 94 Stepping 3, GenuineIntel')\n",
      "('VBOX_MSI_INSTALL_PATH', 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\')\n",
      "('PROGRAMFILES', 'C:\\\\Program Files')\n",
      "('PROCESSOR_REVISION', '5e03')\n",
      "('DATACONNECTORLIBPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('SYSTEMROOT', 'C:\\\\WINDOWS')\n",
      "('CLICOLOR', '1')\n",
      "('PROGRAMFILES(X86)', 'C:\\\\Program Files (x86)')\n",
      "('COMSPEC', 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe')\n",
      "('TERM', 'xterm-color')\n",
      "('DOCKER_TOOLBOX_INSTALL_PATH', 'C:\\\\Apps\\\\Docker Toolbox')\n",
      "('WINDOWS_TRACING_LOGFILE', 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log')\n",
      "('DB2INSTANCE', 'DB2')\n",
      "('PROCESSOR_ARCHITECTURE', 'AMD64')\n",
      "('ALLUSERSPROFILE', 'C:\\\\ProgramData')\n",
      "('SCCMNORDCDOWNLOAD', '1')\n",
      "('LOCALAPPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local')\n",
      "('HOMEPATH', '\\\\')\n",
      "('USERDOMAIN_ROAMINGPROFILE', 'DHC')\n",
      "('JAVA_HOME', 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\')\n",
      "('JPY_INTERRUPT_EVENT', '1224')\n",
      "('PROGRAMW6432', 'C:\\\\Program Files')\n",
      "('USERNAME', 'z001c9v')\n",
      "('COPLIB', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('LOGONSERVER', '\\\\\\\\TCTTSDCC10P')\n",
      "('PROMPT', '$P$G')\n",
      "('WINDOWS_TRACING_FLAGS', '3')\n",
      "('JPY_PARENT_PID', '1144')\n",
      "('PROGRAMDATA', 'C:\\\\ProgramData')\n",
      "('PYTHONPATH', 'C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;')\n",
      "('CLASSPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\tdgeospatial.jar;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\jmsam.jar;.;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2java.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\sqlj.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc_license_cu.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\bin;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\common.jar')\n",
      "('PATH', 'C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\Apps\\\\R\\\\Rtools\\\\bin;C:\\\\Apps\\\\R\\\\Rtools\\\\gcc-4.6.3\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Teradata Performance Monitor Object 15.00\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\msg;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\ODBC Driver for Teradata\\\\Lib;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files\\\\SAS94\\\\x86\\\\Secure\\\\ccme4;C:\\\\Program Files\\\\SAS94\\\\Secure\\\\ccme4;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\tdwallet\\\\nt-i386\\\\;C:\\\\Program Files (x86)\\\\Microsoft Application Virtualization Client;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin\\\\;C:\\\\WINDOWS\\\\SysWOW64\\\\;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\SysWOW64\\\\Wbem;C:\\\\WINDOWS\\\\SysWOW64\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program Files (x86)\\\\AdminStudio\\\\9.5\\\\Common;C:\\\\PROGRA~1\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Program Files\\\\ibm\\\\gsk8\\\\lib64;C:\\\\Program Files (x86)\\\\ibm\\\\gsk8\\\\lib;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\DTS\\\\Binn\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\ManagementStudio\\\\;C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 10.0\\\\Common7\\\\IDE\\\\PrivateAssemblies\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\DTS\\\\Binn\\\\;C:\\\\Apps\\\\Anaconda2;C:\\\\Apps\\\\Anaconda2\\\\Scripts;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda3;C:\\\\Apps\\\\Anaconda3\\\\Scripts;C:\\\\Apps\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;;C:\\\\Apps\\\\Docker Toolbox;C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\;C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2;')\n",
      "('HOMESHARE', '\\\\\\\\corp.target.com\\\\dfsroot\\\\HomeDirs\\\\HQ02\\\\59808964')\n",
      "('GIT_PAGER', 'cat')\n",
      "('UATDATA', 'C:\\\\WINDOWS\\\\CCM\\\\UATData\\\\D9F8C395-CAB8-491d-B8AC-179A1FE1BE77')\n",
      "('PATHEXT', '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC')\n",
      "('INCLUDE', 'C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\INCLUDE;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('SESSIONNAME', 'Console')\n",
      "('FP_NO_HOST_CHECK', 'NO')\n",
      "('WINDIR', 'C:\\\\WINDOWS')\n",
      "('TEMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('HOMEDRIVE', 'H:')\n",
      "('OS', 'Windows_NT')\n",
      "('SYSTEMDRIVE', 'C:')\n",
      "('PUBLIC', 'C:\\\\Users\\\\Public')\n",
      "('NUMBER_OF_PROCESSORS', '8')\n",
      "('APPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Roaming')\n",
      "('USERDNSDOMAIN', 'CORP.TARGET.COM')\n",
      "('IBMDB2', 'C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN')\n",
      "('PAGER', 'cat')\n",
      "('COMMONPROGRAMW6432', 'C:\\\\Program Files\\\\Common Files')\n",
      "('HADOOP_HOME', 'C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2')\n",
      "('COMMONPROGRAMFILES(X86)', 'C:\\\\Program Files (x86)\\\\Common Files')\n",
      "('IPY_INTERRUPT_EVENT', '1224')\n",
      "('USERPROFILE', 'C:\\\\Users\\\\z001c9v')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.11 :: Anaconda 4.0.0 (64-bit)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!python --version\n",
    "import os\n",
    "\n",
    "for key, value in os.environ.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda2\\;C:\\Apps\\Anaconda2\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;\n",
      "<pyspark.context.SparkContext object at 0x0000000003EAB6D8>\n",
      "<pyspark.sql.context.SQLContext object at 0x000000000640BDA0>\n"
     ]
    }
   ],
   "source": [
    "# Initiate Spark on Local:  ** Do not run this on Docker **\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python\\\\lib\\\\py4j-0.10.1-src.zip')) #Note, this needs to be in PYTHONPATH env variable.\n",
    "\n",
    "\n",
    "print(os.environ['PYTHONPATH'])\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"MyApp\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:1\n",
    "Assume your tasked with modeling a REGRESSION problem. How do you determine which variables may be important?\n",
    "\n",
    "\n",
    "1. If your data has unknown structure, start with:  Tree-based methods\n",
    "+ If statistical measures of importance are needed, start with: linear models (think Generalized linear models (GLMs)\n",
    "+ If statistical measures of importance are not needed, start with: Regression with shrinkage (e.g., LASSO, Elastic net)\n",
    "+ If statistical measures of importance are not needed, use : Stepwise regression\n",
    "\n",
    "Select the single most correct response from the following: \n",
    "\n",
    "+ (a) 1 \n",
    "+ (b) 2\n",
    "+ (c) 3\n",
    "+ (d) 1, 2, 3, 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  **is D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:2\n",
    "Using one-hot-encoding, a categorical feature with four distinct values would be represented by how many features?\n",
    "\n",
    "(a) 1 feature  \n",
    "(b) 2 features  \n",
    "(c) 4 features  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** **is C** \n",
    ">> Even though there are 4 features created, you would only us 3 unless you are building the model through the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:3\n",
    "When it comes to decision tree prediction (in the case of either classification or regression problems) which of the the following steps are perfermed:\n",
    "\n",
    "1. A decision tree takes as input an object or situation described by a set of attributes and returns a decision the predicted out value for the input.\n",
    "2. A decision tree reaches its decision by performing a sequence of tests.\n",
    "3. Each internal node in the tree corresponds to a test of the value of one of the properties, and the branches from the node are labeled with the possible values of the test.\n",
    "4. Each leaf node in the tree specifies the value to be returned if that leaf is reached.\n",
    "\n",
    "+ (a) 1 \n",
    "+ (b) 1, 2, 3, 4\n",
    "+ (c) 3\n",
    "+ (d) 1, 2, 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:4\n",
    "\n",
    "Using Claude Shannon’s model of Entropy, compute the: \n",
    "\n",
    "+ Entropy of a fair coin\n",
    "+ Entropy of an unfair coint that comes up heads 80% of the time\n",
    "\n",
    "Select the correct answer from the following:\n",
    "\n",
    "+ (a) 1 bit (entropy of a fair coin) and 0.8 bits (the biased coin)\n",
    "+ (b) 1 bit (entropy of a fair coin) and 1 bit (the biased coin)\n",
    "+ (c) 1 bit (entropy of a fair coin) and 0.2068 bits (the biased coin)\n",
    "+ (d) 1 bit (entropy of a fair coin) and 0.2 bits (the biased coin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:5\n",
    "Given the following gradient descent scenario indicate which of the statements below are correct: \n",
    "\n",
    " + Given an objective function (also know as a cost fucntion) that we wish to minimize. This function has one decision variable w1.\n",
    " + Suppose we reach a local minimum, i.e.,  w1 is already at a local minimum, what do you think one step of gradient descent will do? \n",
    " \n",
    " \n",
    " __STATEMENTS__\n",
    "\n",
    "1. It turns out at the local optimum, your derivative will be equal to zero. \n",
    "2. So  the slope of the line  tangent to the objective function at this point w1  will be equal to zero and thus this derivative term is equal to zero. \n",
    "3. The  gradient descent update at this local optimum is, w1 = w1 -  alpha * 0; where alpha is the learning rate \n",
    "4. If you're already at the local optimum it leaves  w1 unchanged cause its updates as w1 equals w1. So if your parameters are already at a local minimum one step with gradient descent does absolutely nothing. It keeps your solution at the local optimum. \n",
    "\n",
    "\n",
    "Select the single most correct answer from the following:\n",
    "\n",
    "+ (a) 1, 2, 3, 4\n",
    "+ (b) 2\n",
    "+ (c) 3,4\n",
    "+ (d) 2,3,4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:6  \n",
    "\n",
    "Given the following gradient descent scenario indicate which of the statements below are correct: \n",
    "\n",
    " + Given a convex objective function (also known as a cost fucntion) that we wish to minimize. This function has one decision variable w1.\n",
    " + Suppose we reach a local minimum, i.e.,  w1 is already at a local minimum, what do you think one step of gradient descent will do? \n",
    " + use a fixed learning rate alpha; assume alpha is 1. \n",
    " \n",
    "__STATEMENTS__  \n",
    "a. When the current estimate of the minimum of the objective function is further away from the mimimum the gradient is much bigger   \n",
    "b. When the current estimate of the minimum of the objective function is close to the mimimum, my derivative term is even smaller and so the magnitude of the update to w1 is even smaller.  \n",
    "c. As gradient descent runs, you will automatically take smaller and smaller steps. Until eventually you're taking very small steps, and you finally converge to the  global minimum (it is a convex optimization after all).     \n",
    "d. Gradient descent will never converge will a fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: ** ideally you want alpha to be less than 1, and never more than 1.**\n",
    ">> Assuming for a second that alpha = 1 converges nicely, then the **Answer is A,B,C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:7\n",
    "When dealing with numercial data which of the following are ways to deal with missing data:\n",
    "\n",
    "(a) Delete records that have missing input values  \n",
    "(b) Standardize the data and set all missing values to 1 (one)  \n",
    "(c) Use K-nearest neighbours based on the test set to fill in missing values in the training set  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **possible answer are A,C**\n",
    ">> B doesn't work because you can't standardize NULL values. D is false because A is true, or possible strategy. And imputing missing data from available data using averages, or other statistical methods is accpetable, including KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:8\n",
    "In a digital advertising problem where we are modeling the Pr(Click|context) and have no downstream knowledge of what purchases resulted from a click on the ad shown, which of the following approaches would lead to potentially more clicks on the ads shown?\n",
    "\n",
    "(a) Model Click vs not click event using linear regression   \n",
    "(b) Model Click vs not click event using logistic regression  \n",
    "(c) Model probability of a purchase  \n",
    "(d) none of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is B, because the response variable is a binary outcome.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:9\n",
    "Which of the following are true about the purpose of a loss function?\n",
    "\n",
    "(a) It’s a way to penalize a model for incorrect predictions  \n",
    "(b) It precisely defines the optimization problem to be solved for a particular learning model  \n",
    "(c) Loss functions can be used for modeling both classification and regression problems  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is B & C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:10\n",
    "When implementing Logistic (or linear) Regression with Regularization in Spark which of the following apply when using the following cost function: \n",
    "\n",
    "\\begin{equation*}\n",
    "minimize   \\left(f(w):= λR(w) + \\sum_{k=1}^n \\left(w;w_i,y_i \\right) \\right) \n",
    "\\end{equation*}\n",
    "\n",
    "(a) When lambda equals one, it provides the same result as standard logistic (linear) regression   \n",
    "(b) One only needs to modify the standard logistic (linear) regression (i.e., with no regularization term) by adding some code after the map-reduce (loss) gradient steps    \n",
    "(c) When lambda equals zero, it provides the same result as standard logistic (linear) regression  \n",
    "(d) One only needs to modify the standard logistic (linear) regression by modifying the mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:11 Given the following paired RDDs \n",
    "RDD1 = {(1, 2), (3, 4), (3, 6)}\n",
    "RDD2 = {(3, 9) (3, 6)}\n",
    "\n",
    "Using PySpark, write code to perform an inner join of these paired RDDs. What is the resulting RDD? Make your Spark available in your notebook:\n",
    "\n",
    "a: [(3, (4, 9)), (3, (6, 9))]    \n",
    "b: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]   \n",
    "c: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 9))]    \n",
    "d: None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is A, don't know why the code below is not working.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (3, 4), (3, 6)]\n",
      "[(3, 9), (3, 6)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'JoinRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1200d3b2b392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mJoinRRD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRDD1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRDD2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mJoinRDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'JoinRDD' is not defined"
     ]
    }
   ],
   "source": [
    "# code for Problem# 11\n",
    "import pyspark\n",
    "#data1 = [(1, 2), (3, 4), (3, 6)] \n",
    "#data2 = [(3, 9) (3, 6)]\n",
    "RDD1 = sc.parallelize({(1, 2), (3, 4), (3, 6)})\n",
    "RDD2 = sc.parallelize({(3, 9) , (3, 6)})\n",
    "\n",
    "print RDD1.collect()\n",
    "print RDD2.collect()\n",
    "\n",
    "JoinRRD = RDD1.join(RDD2).collect()\n",
    "print JoinRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:12  You have been tasked to build a predictive model to forecast beer sales for a chain of stores.\n",
    "\n",
    "\n",
    "After doing basic exploratory analysis on the data, what is the first thing you do regarding modeling?\n",
    "\n",
    "\n",
    "(a) Construct a baseline model  \n",
    "(b) Determine a metric to evaluate your machine learnt models  \n",
    "(c) Split your data into training, validation and test subsets (or split using cross fold validatation)  \n",
    "(d) All of the  of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: ** is C, but the others are good to.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:13\n",
    "\n",
    "The beer sales data consists of 52 weeks of cases-sold and price-per-case data for 3 carton sizes of beer (12-packs, 18-packs, 30-packs) at a small chain of supermarkets.\n",
    "\n",
    "Three additional rows of hypothetical price data for 12-,18-, 30-packs have been entered for purposes of forecasting from the models.  \n",
    "\n",
    "\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction accuracy of a model for say a forecasting method in statistics, \n",
    "for example in trend estimation. It usually expresses accuracy as a percentage, and is defined by the formula:\n",
    "\n",
    "MAPE = average over all examples (100*Abs(Actual - Predicted) / Actual)) \n",
    "\n",
    "Note when Actual is zero that test row is dropped from the evaluation.\n",
    "\n",
    "Construct a mean model for target variable `CASES18PK` (for the purposes of this question, take the mean of the CASES18PK column as your model prediction). Calculate the MAPE for the mean model over the training set. Select the closest answer to your calculated MAPE.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 250%  \n",
    "(c) 20%  \n",
    "(d) 180%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:14\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The target variable `CASES18P`K is skewed, so take the log of it (and make it more normally distributed) and compute the MAPE of the mean model for `LN_CASES18PK`. Select the closest answer to your calculated MAPE.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 30%  \n",
    "(c) 20%  \n",
    "(d) 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:15\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "\n",
    "Build a linear regression model using the following variables:\n",
    "\n",
    "Log(CASES18PK)  ~  log(PRICE12PK) + \tlog(PRICE18PK) +\tlog(PRICE30PK)\n",
    "\n",
    "Calculate MAPE over the test set and select the closest answer.\n",
    "\n",
    "(a) 4.3%   \n",
    "(b) 4.6%   \n",
    "(c) 3.5%  \n",
    "(d) 3.9%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET:16\n",
    "Recall that Spark automatically sends all variables referenced in your closures to the\n",
    "worker nodes. While this is convenient, it can also be inefficient because (1) the\n",
    "default task launching mechanism is optimized for small task sizes, and (2) you\n",
    "might, in fact, use the same variable in multiple parallel operations, but Spark will\n",
    "send it separately for each operation. As an example, say that we wanted to write a\n",
    "Spark program that looks up countries by their call signs (e.g., the call sign for Ireland is EJZ) by prefix matching in an\n",
    "table. In the following the \"signPrefixes\" variable is essentially a table with two columns \"Sign\" and \"Country Name\". The goal is \n",
    "to join the following tables:\n",
    "\n",
    "`signPrefixes` table with columns \"Sign\" and \"Country Name\"  \n",
    "`contactCounts` table with columns \"Sign\" and \"count\"\n",
    "\n",
    "to yield  a new table:\n",
    "\n",
    "`countryContactCounts` with the following columns \"Country Name\" and \"count\"\n",
    "\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#..... Other code...\n",
    "#Country lookup code\n",
    "\n",
    "## Helper functions for looking up the call signs\n",
    "\n",
    " def lookupCountry(sign, prefixes):\n",
    "    pos = bisect.bisect_left(prefixes, sign)\n",
    "    return prefixes[pos].split(\",\")[1]\n",
    "\n",
    "\n",
    "def loadCallSignTable():\n",
    "    f = open(\"callsign_tbl_sorted.txt\", \"r\")\n",
    "    return f.readlines()\n",
    "\n",
    "## Lookup the locations of the call signs on the\n",
    "## RDD contactCounts. We load a list of call sign\n",
    "## prefixes to country code to support this lookup.\n",
    "signPrefixes = loadCallSignTable()\n",
    "\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "    country = lookupCountry(sign_count[0], signPrefixes)\n",
    "    count = sign_count[1]\n",
    "    return (country, count)\n",
    "\n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(lambda signCount: processSignCount(signCount, signPrefixes))\n",
    "                        .reduceByKey((lambda x, y: x + y)))\n",
    "\n",
    "countryContactCounts.saveAsTextFile(outputDir + \"/countries.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we modfify this code to make it more efficient? Choose one response only\n",
    "\n",
    "(a) modify line 18 with `sc.broadcast(loadCallSignTable())`\n",
    "\n",
    "(b) Use accumulators to store the counts for each country  \n",
    "(c) The code is already optimal  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: **is A** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End of Exam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
